<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>A law of data separation in deep learning</title>
    <url>/2024/11/07/CS/data_law/data_law/</url>
    <content><![CDATA[<p>这篇文章研究机器学习过程中，隐藏层对数据的处理方式，发现数据在等几何定律的分离，并且可以观察到类别的出现，因此总结归纳了一个可以量化的规律。</p>
<p>Reference: * <a
href="https://www.pnas.org/doi/10.1073/pnas.2221704120">A law of data
separation in deep learning</a> * <a
href="https://github.com/HornHehhf/Equi-Separation">github地址</a></p>
<span id="more"></span>
<p>其针对监督学习任务，接下来讨论的参数均是在同一层中，<span
class="math inline"><em>x</em><sub><em>k</em><em>i</em></sub></span>表示第<span
class="math inline"><em>k</em></span>个类别第<span
class="math inline"><em>i</em></span>个输出值，<span
class="math inline"><em>x̄</em><sub><em>k</em></sub></span>表示第<span
class="math inline"><em>k</em></span>个类别的均值，<span
class="math inline"><em>x̄</em></span>表示<span
class="math inline"><em>k</em></span>个类别的均值。</p>
<p>定义两个距离矩阵，首先是类别间(between)的距离<span
class="math inline"><em>S</em><em>S</em><sub><em>b</em></sub></span>：
<span class="math display">$$
SS_b = \frac{1}{n}\sum_{k=1}^K n_k(\bar{x}_k - \bar{x})(\bar{x}_k -
\bar{x})^T,
$$</span> 其中<span
class="math inline"><em>n</em></span>是总样本量，通过求<span
class="math inline"><em>S</em><em>S</em><sub><em>b</em></sub></span>的逆（伪逆）可以定义类别间的相似性<span
class="math inline"><em>S</em><em>S</em><sub><em>b</em></sub><sup>†</sup></span>。同时可以定义同一类(within)中的距离<span
class="math inline"><em>S</em><em>S</em><sub><em>w</em></sub></span>：
<span class="math display">$$
SS_w = \frac{1}{n}
\sum_{k-1}^K\sum_{i=1}^{n_k}(x_{ki}-\bar{x}_k)(x_{ki}-\bar{x}_k)^T.
$$</span></p>
<p>基于以上内容，定义距离上的“逆信噪比”： <span
class="math display"><em>D</em> = Tr(<em>S</em><em>S</em><sub><em>w</em></sub><em>S</em><em>S</em><sub><em>b</em></sub><sup>†</sup>).</span></p>
<p>将不同类别间的相似性作为信号，而统一类别间的距离作为噪音，这个量的设计十分自然。并且当<span
class="math inline"><em>D</em></span>越小，代表分割越好。</p>
<p>同时在这里我们需要提出一个问题，对于分类任务，类别在高维空间中如何分布是一个很好的构型，等价于我们该从哪几个指标评判在高维空间类别分布。这里引入概念<strong>Equiangular
Tight Frame</strong>。</p>
<blockquote>
<p><a
href="https://www.pnas.org/doi/epdf/10.1073/pnas.2015509117">Prevalence
of Neural Collapse during the terminal phase of deep learning
training</a></p>
</blockquote>
<p>他要求向量具有如下的性质：</p>
<ul>
<li>等角性：任意两个不同的框架向量之间的内积绝对值相同。这样保证了最大的分离。</li>
<li>紧框架：ETF
满足能量在向量组中的“紧”分布，这意味着向量组能够均匀地表示信号，满足帧不等式的等式情况。该要求使得模型的鲁棒性质提升。</li>
</ul>
<p>在实验中发现：</p>
<figure>
<img src="./1.png" alt="SGD" />
<figcaption aria-hidden="true">SGD</figcaption>
</figure>
<p>其中每幅图的纵坐标表示<span
class="math inline"><em>D</em></span>，随着网络深度的加深，网络的分割能力越强。</p>
<figure>
<img src="./2.png" alt="SGD" />
<figcaption aria-hidden="true">SGD</figcaption>
</figure>
<p>通过降维，可以发现网络分类能力越来越强。</p>
<p>基于此文章总结了下降规律为：</p>
<p><span
class="math display"><em>D</em><sub><em>l</em></sub> = <em>ρ</em><sup><em>l</em></sup><em>D</em><sub>0</sub></span></p>
<p>其中<span
class="math inline"><em>ρ</em> ∈ [0, 1]</span>是一个由网络结构与训练方式决定的量，<span
class="math inline"><em>D</em><sub>0</sub></span>是初始的差别。</p>
<h1 id="append">Append</h1>
<ol type="1">
<li><p><strong>定义与特性</strong>：Equiangular Tight Frame (ETF)
是一种具有等角性（Equiangularity）和紧帧性（Tightness）特征的向量集合。</p>
<ul>
<li><strong>等角性</strong>：ETF
中任意两个向量之间的内积绝对值相同，这种等角特性使得向量之间的夹角是均匀的。</li>
<li><strong>紧帧性</strong>：ETF
满足紧帧条件，即每个向量的投影平方和在整个帧上均匀分布，保证了信号的能量在所有帧向量中保持一致。</li>
</ul></li>
<li><p><strong>Simplex ETF</strong>：Simplex ETF 是一种特殊的
ETF，它的向量构成了一个正交单纯形的顶点。这种向量集合具有最均匀的分布，可以看作是理想的分散性参考，用于分析类别或信号分布的均匀性。</p></li>
<li><p><strong>用途和优势</strong>：</p>
<ul>
<li><strong>信号处理与压缩感知</strong>：ETF
的等角和紧帧特性使其非常适合用于稀疏信号的重构和能量均匀分布的信号表示。</li>
<li><strong>数据分析与分类</strong>：ETF
提供了类别分散度的理想参考，通过比较类别特征向量与 Simplex ETF
结构的相似性，可以评估类别在特征空间中的分布均匀性。</li>
<li><strong>鲁棒性和冗余表示</strong>：ETF
的冗余和紧帧特性使得即使部分帧向量丢失，信号依然可以较好地重建，因此在存在噪声或缺失信息时表现出较强的鲁棒性。</li>
</ul></li>
<li><p><strong>几何结构和构造</strong>：在实际应用中，ETF
向量集通常构成一个等距或等角的几何结构（如正交单纯形的顶点），提供了一种理想的框架来衡量和评估向量之间的角度和距离分布。</p></li>
</ol>
<p>ETF
是一种在信号处理、压缩感知和数据分析中广泛应用的框架结构，凭借其均匀的能量分布和等角特性，在高维空间中提供了对称性、均匀性和冗余性。ETF
尤其适用于需要均匀分布和高鲁棒性的场景，是一种用于信号重构、类别分布分析和特征提取的理想框架。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Fermi Bose Machine</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 使用指南</title>
    <url>/2024/06/25/CS/hexo/hexo/</url>
    <content><![CDATA[<p>主要用于实现我对 Hexo 各种奇奇怪怪的需求。</p>
<span id="more"></span>
<h1 id="视频嵌入">视频嵌入</h1>
<h2 id="hexo-中嵌入-bilibili-视频">Hexo 中嵌入 Bilibili 视频</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;iframe src=&quot;//player.bilibili.com/player.html?aid=你的视频AID&amp;bvid=你的视频BVID&amp;cid=你的视频CID&amp;page=1&quot; scrolling=&quot;no&quot; border=&quot;0&quot; frameborder=&quot;no&quot; framespacing=&quot;0&quot; allowfullscreen=&quot;true&quot; width=&quot;640&quot; height=&quot;480&quot;&gt; &lt;/iframe&gt;</span><br></pre></td></tr></table></figure>
<h2 id="hexo-中嵌入-toutube-视频">Hexo 中嵌入 Toutube 视频</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/你的视频ID&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>ZheJiang Lab</title>
    <url>/2025/04/27/LLM/2025ZJLab/note/</url>
    <content><![CDATA[<p>参加杨东平老师在之江实验室举办的大语言模型研讨会议，从物理、神经计算、生物等方面理解和看待语言模型以及神经网络的发展。</p>
<span id="more"></span>
<h1 id="introduction">Introduction</h1>
<figure>
<img src="./invite.jpeg" alt="会议日程" />
<figcaption aria-hidden="true">会议日程</figcaption>
</figure>
<h1 id="侧向预测编码">侧向预测编码</h1>
<p>参考文献： * <a href="https://arxiv.org/abs/2501.12139">Discontinuous
phase transition of feature detection in lateral predictive coding</a> *
<a href="https://doi.org/10.1007/s11433-024-2341-2">Energy-information
trade-oﬀ induces continuous and discontinuous phase transitions in
lateral predictive coding</a></p>
<h2 id="什么是lateral-predictive-coding">什么是lateral predictive
coding？</h2>
<p>通过观察同一层网络相互之间的关系，从而对自身进行预测，将预测结果与自身对照，产生误差，对误差进行回传。对于同一层的不同变量记为<span
class="math inline"><em>x</em><sub><em>i</em></sub><sup><em>l</em></sup></span>，误差<span
class="math inline"><em>ϵ</em><sub><em>i</em></sub><sup><em>l</em></sup> = <em>x</em><sub><em>i</em></sub><sup><em>l</em></sup> − <em>f</em>(<em>x</em><sub><em>i</em></sub><sup><em>l</em></sup>)</span>，通过Bottom-up（类似于反向传播）的方式优化。引入lateral
predictive
coding的目的在于可以减少冗余，如果一个网络大部分内容可以预测（<span
class="math inline"><em>ϵ</em> = 0</span>），那么很多的信息就是无用信息。</p>
<p>这是一个用lateral 方案对MNIST数据集进行预测的例子： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ==== LPC 模拟模型 ====</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LPC_MNIST</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">256</span>)</span><br><span class="line">        <span class="variable language_">self</span>.lateral = nn.Linear(<span class="number">256</span>, <span class="number">256</span>)  <span class="comment"># lateral prediction within same layer</span></span><br><span class="line">        <span class="variable language_">self</span>.classifier = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)  <span class="comment"># 展平</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 主激活</span></span><br><span class="line">        h = torch.relu(<span class="variable language_">self</span>.linear(x))  <span class="comment"># shape: (B, 256)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 侧向预测（当前层内部）</span></span><br><span class="line">        h_detached = h.detach()  <span class="comment"># 假设 lateral 连接不能访问当前梯度</span></span><br><span class="line">        h_pred = <span class="variable language_">self</span>.lateral(h_detached)  <span class="comment"># h_i ≈ f(h_j≠i)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 预测误差</span></span><br><span class="line">        error = h - h_pred</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用预测误差作为表征送入分类器</span></span><br><span class="line">        out = <span class="variable language_">self</span>.classifier(error)</span><br><span class="line">        <span class="keyword">return</span> out, error</span><br></pre></td></tr></table></figure>
注意到，最后进行分类所使用的特征数据是误差<span
class="math inline"><em>ϵ</em></span>。</p>
<h2 id="理论框架">理论框架</h2>
<p>将LPC写为动力学过程： <span class="math display">$$
\begin{align}
\frac{d \bf x}{d t} = \bf s -\bf x - \bf W \bf x
\end{align}
$$</span> 其中<span class="math inline">$\bf s$</span>是输入向量，<span
class="math inline">$\bf x$</span>可以认为是预测误差。稳定解为：<span
class="math inline">$\bf x=(\bf I+\bf W)^{-1}\bf s$</span>。</p>
<p>将预测误差和定义为能量： <span class="math display">$$\begin{align}
E\,\equiv\,\sum_{i=1}^{N}\Bigl\langle\bigl|x_{i}\bigr|\Bigr\rangle\,=\,\sum_{i=1}^{N}\Bigl\langle\Bigl|\sum_{j=1}^{N}\bigl(\frac{I}{I+W}\bigr)_{i
j}s_{j}\Bigr|\Bigr\rangle
\end{align}$$</span></p>
<p>熵为（文章的补充材料中有证明）： <span
class="math display">$$\begin{align}
S=-\log\bigl[\operatorname*{det}(I+W)\bigr]
\end{align}$$</span></p>
<p>以熵作为输出信息的复杂度，从自由能和能量竞争的理论出发，发现lateral编码会出现非连续相变，或许进一步揭示了lateral
predictive coding的潜在运行机制。</p>
<p>核心思路是，定义能量，并且通过随机矩阵计算熵的表达式，通过自由能最小化的方式，用温度调控两者的分布</p>
<h1
id="大模型机理分析数据合成与慢思考机制">大模型机理分析——数据合成与慢思考机制</h1>
<p>刘勇老师研究LLM如何进行思考，从泛化上界的视角研究MCTS提升模型能力的原理。</p>
<p>我的思考是，如何把多智能体的想法和泛化上界相结合，通过证明把人类的经验总结进prompt会提升模型的表达能力。</p>
<h1
id="通过凝聚现象理解语言模型的推理与记忆">通过凝聚现象理解语言模型的推理与记忆</h1>
<p>参考文献： - <a
href="https://neurips.cc/virtual/2024/poster/94702">Initialization is
Critical to Whether Transformers Fit Composite Functions by Reasoning or
Memorizing</a> - <a href="https://arxiv.org/abs/2401.08309">ANCHOR
FUNCTION: A TYPE OF BENCHMARK FUNCTIONS FOR STUDYING LANGUAGE
MODELS</a></p>
<p>通过设计精巧实验(称为 anchor
function)，分析不同初始化方法下，模型产生的记忆与推理两种模式。</p>
<h2 id="anchor-function">Anchor function</h2>
<h3 id="one-anchor-function">One-anchor function</h3>
<p>考虑函数 <span
class="math inline"><em>f</em>(<em>X</em>) : ℝ<sup><em>n</em> × <em>d</em></sup> → ℝ<sup><em>d</em></sup></span>
，<span class="math inline"><em>n</em></span>是输入信息的长度，<span
class="math inline"><em>d</em></span>是tokens的长度，<span
class="math inline"><em>X</em> = (<strong>x</strong><sub><strong>1</strong></sub>, <strong>x</strong><sub><strong>2</strong></sub>, <strong>x</strong><sub><strong>3</strong></sub>⋯<strong>x</strong><sub><strong>n</strong></sub>)</span>并且<span
class="math inline"><strong>x</strong><sub><strong>i</strong></sub> ∈ ℝ<sup><em>d</em></sup></span>。有如下的映射关系：
<span class="math display">$$\begin{align}
f(\mathbf{x_1}, \mathbf{x_2}, \mathbf{x_3}\cdots\mathbf{x_n}) =
\mathbf{x_{i+1}}\quad \text{where}\quad x_i =a
\end{align}$$</span></p>
<p>例如对于<span class="math inline"><em>d</em> = 1</span>维、<span
class="math inline"><em>a</em> = 3</span>的情况下<span
class="math inline"><em>f</em>(12, 33, 14, 3, 42, 54, 34, 20, 28) = 42</span>。其中<span
class="math inline"><em>a</em> = 3</span>是一个钩子，找到序列中对应的位置，然后给出后一个位置的结果。可以得到一个更加普适的函数：
<span class="math display">$$\begin{align}
f(\mathbf{x_1}, \mathbf{x_2}, \mathbf{x_3}\cdots\mathbf{x_n}) =
g(\mathbf{x_{i+1}})\quad \text{where}\quad x_i =a
\end{align}$$</span> 其中<span
class="math inline">𝕩<sub>𝕚</sub></span>为钩子(anchor)项，对应的<span
class="math inline">𝕩<sub>𝕚 + 𝟙</sub></span>是钥匙(key)项。</p>
<h3 id="two-anchor-composite-function">Two-anchor composite
function</h3>
<p>Anchor 集为一系列不同的token，例如<span
class="math inline"><em>A</em> = {<em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, ⋯, <em>a</em><sub><em>J</em></sub>}</span>，输出函数为<span
class="math inline"><em>g</em>(𝕩; <em>a</em><sub><em>k</em></sub>)</span>。在每一个输入的序列中，有且仅有一对连续元素属于<span
class="math inline"><em>A</em></span>： <span
class="math display">$$\begin{align}
f(\mathbf{x_1}, \mathbf{x_2}, \mathbf{x_3}\cdots\mathbf{x_n}) =
g(g(\mathbf{x_{i-1}};\mathbf{x_{i}});\mathbf{x_{i+1}})\quad
\text{where}\quad x_i,x_{i+1} \in A
\end{align}$$</span></p>
<p>可以得到以下集中任务分类： <img src="./anchor.png"
alt="anchor" /></p>
<h2 id="experiment">Experiment</h2>
<figure>
<img src="./expriment.png" alt="figure" />
<figcaption aria-hidden="true">figure</figcaption>
</figure>
<p>上图是实验示意图，输入包含三个部分：anchor项（黄色）、key项（橙色）、噪声项（灰色），输出为目标结果。anchor项是与众不同的，这个实验中为<span
class="math inline">1, 2, 3, 4</span>，并且每一种anchor表示对key项的一个操作。一共有用16种组合，其中15种项是在训练过程中作为输入集，并且这里面有14个是可以通过推理得出的，<span
class="math inline">(3, 4)</span>这个组合只能通过记忆得出，一组<span
class="math inline">(4, 3)</span>用于测试。这组测试anchor会给出三种可能的结果，一种是学习到对称性给出和<span
class="math inline">(3, 4)</span>一样的解，一种是学习到推理解，以及其它非推理解。</p>
<p>通过实验发现，解的类型和初始化参数、网络深度有关，通过设置超参<span
class="math inline"><em>γ</em></span>调控不同的学习模式，<span
class="math inline">1/<em>d</em><sub>in</sub><sup><em>γ</em></sup></span>是每层参数的标准差，其中<span
class="math inline"><em>d</em><sub>in</sub></span>是输入神经元数目。</p>
<p><img src="./anchor-1.png" /></p>
<p>从实验中可以发现，存在一个明显的分界线，在不同的超参下识别的结果存在明显的分别。可能的解释是，在方差较小的时候，参数变化小，因此需要学习推理的内容；当方差变大模型可以学习更多的内容，因此参数倾向于记忆内容；方差进一步增大，模型会记忆更多内容，正确率下降。</p>
<p>从以下几个方面分析模型学习到不同策略的原因： * 信息流 *
信息在向量空间的表示 * 不同观点下的模型复杂度</p>
<figure>
<img src="./information-flow.png" alt="information-flow" />
<figcaption aria-hidden="true">information-flow</figcaption>
</figure>
<p>上面是从信息流动的角度分析，浅层的网络对于信息处理起到了重要的作用。(a)(b)图分析了对称解的情况，浅层网络将anchor映射到一组位置，然后将key表示在这个结果上；在图(b)中通过t-SNE降维的方法，将不同anchor得到的结果降维，发现能够很好分类。图(c)是推理的结果，显示会先把key映射到对应的anchor上然后进行处理。</p>
<figure>
<img src="./information-cosine.png" alt="information-cosine" />
<figcaption aria-hidden="true">information-cosine</figcaption>
</figure>
<p>上图是度量cosine相似度的热力图。图(a)是推理的结果，红框表示是相同的输出结果，可以发现，其具有很高的相似度；图(b)是对称的结果，基本没有相似性。这样显然的区别，表明模型推理能力和结构有关。为了进一步验证这个猜想，设计one-anchor的实验（图c），发现模型确实在推理的结果上表现相似。</p>
<figure>
<img src="./anchor-para.png" alt="information-para" />
<figcaption aria-hidden="true">information-para</figcaption>
</figure>
<p>上图(a)(b)是计算参数的权重<span
class="math inline"><em>W</em></span>cosin距离的热力图，其中图(a)表示推理的结果，可以看到出现明显的方向性，图(b)是对称解，并没有明显的方向性，复杂度柔和进模型中。图(c)(d)为t-SNE之后的embedding结果，图(c)为推理解有明显的方向，相比之下对称解图(d)显得杂乱。</p>
<h1 id="深度学习机理的动力学分析">深度学习机理的动力学分析</h1>
<p>赵鸿老师通过自己的方式，从数据和参数层面，分析神经网络的相关性质，很物理也很有趣。</p>
<h1 id="储备池计算">储备池计算</h1>
<p>参考文献： * <a
href="https://doi.org/10.1016/j.neunet.2019.03.005">Recent advances in
physical reservoir computing: A review</a></p>
<p>储备池计算（Reservoir
Computing），就是通过非线性的动力学过程将数据特征升至高维时间空间中，从而完成特征提取。</p>
<p><img src="./RC.png" /> 上图是储备池计算的示意图，在a图中输入权重<span
class="math inline"><em>W</em><sup><em>i</em><em>n</em></sup></span>与动力学参数<span
class="math inline"><em>W</em></span>是固定参数，不参与训练。只训练读出头部分<span
class="math inline"><em>W</em><sup><em>o</em><em>u</em><em>t</em></sup></span>。</p>
<h1 id="大语言模型的训练">大语言模型的训练</h1>
<p>陈星强和吴生礅在进行大模型的训练，通过unsloth这个包。重复这个工作，找他俩商量聊天，完成这个任务。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Multi-Agent</tag>
        <tag>Prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown 文章中的交叉引用（利用锚点）</title>
    <url>/2024/04/15/CS/anchor_point/anchor_point/</url>
    <content><![CDATA[<p>参考文章<a
href="https://retzzz.github.io/55e045cb/#HTML-anchor">在带有数学公式的markdown文档里的交叉引用</a>实现。</p>
<p><span id="anchor_name"/>这篇文章作为案例，实现公式的交叉引用。</p>
<p>更多阅读： * <a
href="https://retzzz.github.io/55e045cb/#HTML-anchor">在带有数学公式的markdown文档里的交叉引用</a>
* <a
href="https://onemathematicalcat.org//MathJaxDocumentation/TeXSyntax.htm"><span
class="math inline">$\LaTeX$</span>在MathJax中的命令</a> * <a
href="https://blog.csdn.net/weixin_40301746/article/details/123967807">MathJax
与 Katex 在公式对齐、编号、交叉引用方面的不同</a> * <a
href="https://zhuanlan.zhihu.com/p/381263375">Markdown杂记</a></p>
<span id="more"></span>
<p>注意：这里渲染<span
class="math inline">$\LaTeX$</span>的引擎需要为MathJax，之前还真没注意过这个区别。MathJax支持公式引用，但必须自己手动标号。我觉得应该是因为交叉引用会使实时渲染产生问题。但是可以静态渲染，然后再检查，麻烦了一点。希望这个<a
href="https://github.com/shd101wyy/vscode-markdown-preview-enhanced/issues/67">问题</a>能够解决吧。</p>
<h1 id="使用cassid方案">使用<code>cassId</code>方案</h1>
<h2 id="添加锚点">添加锚点</h2>
<h3 id="行间公式">行间公式</h3>
<p>编号为<span class="math inline">1</span> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$\cssId&#123;1&#125;&#123;\overline&#123;v&#125;&#125;:\overline&#123;S&#125;\to\&#123;F,T\&#125;$$</span><br></pre></td></tr></table></figure> 渲染为： <span
class="math display">$$
\cssId{1}{\overline{v}}:\overline{S}\to\{F,T\}
$$</span></p>
<p>接下来加入环境，编号为<span class="math inline">3</span>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$\begin&#123;align&#125;</span><br><span class="line">\cssId&#123;3&#125;&#123;\overline&#123;v&#125;&#125;:\overline&#123;S&#125;\to\&#123;F,T\&#125;</span><br><span class="line">\end&#123;align&#125;$$</span><br></pre></td></tr></table></figure> 渲染为：</p>
<p><span class="math display">$$\begin{align}
\cssId{3}{\overline{v}}:\overline{S}\to\{F,T\}
\end{align}$$</span></p>
<h3 id="行内公式">行内公式</h3>
<p>编号为<span class="math inline">3</span>： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">这是一段没有意义的废话$\cssId&#123;2&#125;&#123;\overline&#123;v&#125;&#125;:\overline&#123;S&#125;\to\&#123;F,T\&#125;$</span><br></pre></td></tr></table></figure> 渲染为：</p>
<p>例如这样的行内公式<span
class="math inline">$\cssId{2}{\overline{v}}:\overline{S}\to\{F,T\}$</span>。</p>
<h2 id="引用锚点">引用锚点</h2>
<p>公式中的引用： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\href&#123; #1&#125;&#123;\overline&#123;v&#125;&#125;=F(x)</span><br></pre></td></tr></table></figure>
通过替换<code>#</code>之后的标签引用，而且前面要有空格。</p>
<p>以下为渲染之后的例子： <span class="math display">$$\href{
#1}{\overline{v}}=F(x)$$</span> <span class="math display">$$\href{
#2}{\overline{v}}=F(x)$$</span> <span class="math display">$$\href{
#3}{\overline{v}}=F(x)$$</span></p>
<h1 id="使用mathjax方案">使用<code>mathjax</code>方案</h1>
<p>编号为<code>test</code>： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$\begin&#123;align&#125;</span><br><span class="line">a\neq&amp; b \label&#123;test&#125; \\</span><br><span class="line">c\neq&amp; b \label&#123;test2&#125; \\</span><br><span class="line">\end&#123;align&#125;$$</span><br></pre></td></tr></table></figure> 渲染为： <span
class="math display">$$\begin{align}
a\neq&amp; b \label{test} \\
c\neq&amp; b \label{test2} \\
\end{align}$$</span></p>
<p>通过<code>$\eqref&#123;test&#125;$</code>引用<span
class="math inline">$\eqref{test}$</span>，或者通过<code>$\ref&#123;test2&#125;$</code>引用公式<span
class="math inline">$\ref{test2}$</span>。</p>
<h1 id="用html插入方案">用HTML插入方案</h1>
<p>使用<code>&lt;span id="anchor_name"/&gt;</code>来建立锚点，例如：</p>
<p><code>&lt;span id="anchor_name"/&gt;这篇文章作为案例，实现公式的交叉引用。</code></p>
<p>使用相对地址引用锚点： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[点击这里](#anchor_name)</span><br></pre></td></tr></table></figure></p>
<p><a href="#anchor_name">点击这里</a></p>
<h1 id="hexo官方方案">Hexo官方方案</h1>
<p>目前跑通一个，给出下属链接：</p>
<ul>
<li><a
href="https://hexo.io/zh-cn/docs/tag-plugins#%E5%BC%95%E7%94%A8%E6%96%87%E7%AB%A0">官方</a></li>
<li><a
href="https://blog.jijian.link/2020-01-08/hexo-anchor-link/#hexo-%E6%A0%87%E9%A2%98%E4%B8%8E-id-%E5%85%B3%E7%B3%BB">博客</a></li>
</ul>
<p>连接到之前的一个文章 Git: <a href="/2022/09/16/CS/git/git/" title="Git Note">Git Note</a> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% post_link CS/git/git %&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>RECONCILE and ReAgent</title>
    <url>/2025/04/13/LLM/multiAgent/multiAgent/</url>
    <content><![CDATA[<p>提出两种多智能体合作方案。</p>
<p>文献： * <a href="https://arxiv.org/abs/2309.13007">RECONCILE:
Round-Table Conference Improves Reasoning via Consensus among Diverse
LLMs</a> * <a href="https://github.com/dinobby/ReConcile">ReConcile
code</a> * <a href="https://arxiv.org/abs/2503.06951">ReAgent:
Reversible Multi-Agent Reasoning for Knowledge-Enhanced Multi-Hop QA</a>
* []</p>
<span id="more"></span>
<h1 id="reconcile">RECONCILE</h1>
<p>RECONCILE(Round-Table Conference Improves Reasoning via Consensus
among Diverse
LLMs)，通过不同预训练模型相互合作，通过一致性完成推理任务。</p>
<p>多智能体合作的有效性，来源三个方面： 1. 解决方案多样性 2.
估计每一个智能体的一致性 3. 利用一致性使得不同智能完成合作</p>
<p><img src="./1.png" /></p>
<p>左边是传统的思路，通过单个预训练模型，利用不同的prompt搭建智能体，通过不同的结构设计达成目标。右边这篇论文提出的新方案，采用多个预训练模型通过多轮圆桌讨论对话完成任务目标。</p>
<p><img src="./2.png" /></p>
<p>左侧是一轮的流程，右侧展示了三轮对话之后的情况。</p>
<p><img src="./3.png" /></p>
<p>算法流程图。</p>
<h1 id="reagent">ReAgent</h1>
<p>ReAgent(a Reversible
multi-Agent)是一种新的多智能体架构，通过显式的回溯历史记录增强多跳步(multi-hop)的推理。</p>
<p><img src="./4.png" /></p>
<p>解决multi-hop推理问题，需要的信息碎片多、逻辑链条长，对于LLM来说具有挑战性。已经有的解决方案是通过模型的推理，这个依靠模型训练阶段的表现，另一种方案是MAS。MAS通过前向的逐步推理，能够有效提升模型在multi-hop问题上的表现能力，但是其有一些问题，如果模型在最开始就出现错误，之后很难对这个结果进行纠正。一些MAS框架开始着手于解决这个问题，包括ReAgent。</p>
<p><img src="./5.png" /></p>
<p>上图是ReAgent的体系架构，分为推理与回溯两部分。左边是推理部分，分为执行层、监督层、交互层：在执行层中将复杂的任务分解为子问题，给出推断然后检查验证，最后整合答案输出；交互层负责提供信息的读取的接口；监督层负责处理冲突，并且检查是否符合规则。右边的回溯机制，展示了执行与监督层是工作交互的逻辑。</p>
<p><img src="./6.png" /> <img src="./7.png" /></p>
]]></content>
      <categories>
        <category>Largent language Model</category>
      </categories>
      <tags>
        <tag>Multi-Agent</tag>
        <tag>Prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>Why Do Multi-Agent LLM Systems Fail?</title>
    <url>/2025/04/07/LLM/MultiAgentFail/MultiAgentFail/</url>
    <content><![CDATA[<p>多智能体（Multi-Agent System,
MAS）合作处理问题的思路十分流行，但是在一些热门的batchmark上并没有明显的表现差距。本文针对MAS没有性能提升的问题进行探究，总结出以下三个方面：</p>
<ol type="1">
<li>specification and system design failures</li>
<li>inter-agent misalignment</li>
<li>task verification and termination.</li>
</ol>
<p>文献： * <a href="https://arxiv.org/abs/2503.13657">Why Do
Multi-Agent LLM Systems Fail?</a> * <a
href="https://github.com/multi-agent-systems-failuretaxonomy/MASFT">源代码GitHub</a></p>
<span id="more"></span>
<h1 id="introduction">Introduction</h1>
<p>单智能体，是通过精心设计prompt，用于处理特定问题的语言模型。多智能体（Multi-Agent
System,
MAS）是通过合理设计流程，使智能体与环境、相互交流，并且可以读取文档、历史记录等信息，用于处理复杂、多步的真实世界问题，目前已经在多个方面进行探索——软件工程、药物发现、科学模拟以及通用目标智能体。</p>
<p>MAS相对于单智能体，有效原因普遍认为来自于协同努力、任务目标的分解、并行化、文本独立、集成特定模型以及逻辑性讨论。但是在实际情况中，针对一些问题构建的MAS在泛化的问题上没有普遍提升，目前依旧没有恰当方式构建鲁棒、稳定的MAS。因此针对更加核心、基本的问题“为什么MAS会失败呢？”进行讨论。</p>
<p>基于社会学的方法（Grounded
Theory），提出了多智能体失败分类(Multi-Agent System Failure Taxonomy,
MASFT)。为了使判别自动化，同时提出了LLM作为评判的管线，通过Cohen’s
Kappa一致性指标表明和人类评价具有较高的一致性(0.77)。</p>
<h1 id="masft">MASFT</h1>
<figure>
<img src="./fig2.png" alt="MASFT" />
<figcaption aria-hidden="true">MASFT</figcaption>
</figure>
<p>这个表格中囊括了三个阶段：对话前(Pre
Execution)、对话中(Execution)、对话后(Post
Execution)；这三个阶段分别表示了问题出现的位置。同时也存在三种类型：系统与角色错误(Specification
and System Design Failures)、智能体对话失调(Inter-Agent
Misalignment)、任务验证与终止(Task Verification and
Termination)。问题的跨度越长，代表这个问题在MAS中越容易发生错误。</p>
<h1 id="function">Function</h1>
<figure>
<img src="./fig3.png" alt="Fail" />
<figcaption aria-hidden="true">Fail</figcaption>
</figure>
<p>展示如何通过Grounded Theory中的方法，界定不同类别。</p>
<h1 id="better-mas">Better MAS</h1>
<p>针对MAS失败的原因，针对策略与结构两方面提出可能的解决思路。在策略上针对难以解决的问题，设定专门的prompt；在结构上，大多数的系统失败因素是由于，较弱以及非独立的验证方式，可以针对这点进行改进。</p>
]]></content>
      <categories>
        <category>Largent language Model</category>
      </categories>
      <tags>
        <tag>Multi-Agent</tag>
        <tag>Prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>Active Learning Literature Survey</title>
    <url>/2024/10/16/DL/ActiveLearning/ActiveLearning/</url>
    <content><![CDATA[<p>介绍Active
Learning的基本概念与算法，以及相关python库——ALiPy的使用。</p>
<blockquote>
<p>The key idea behind active learning is that a machine learning
algorithm can achieve greater accuracy with fewer labeled training
instances if it is allowed to choose the data from which is learns.</p>
</blockquote>
<p>Reference: * <a
href="https://www.semanticscholar.org/paper/Active-Learning-Literature-Survey-Settles/818826f356444f3daa3447755bf63f171f39ec47">Active
Learning Literature Survey</a> * <a
href="https://arxiv.org/pdf/1901.03802">ALiPy: Active Learning in
Python</a> * <a
href="https://github.com/NUAA-AL/alipy">GitHub:ALiPy</a></p>
<span id="more"></span>
<h1 id="introduction">Introduction</h1>
<figure>
<img src="./fig1.png" alt="active learning" />
<figcaption aria-hidden="true">active learning</figcaption>
</figure>
<p>active
learning的任务是利用可选择标注的样本的优势，在使用较少的情况下以高正确率完成训练。上图中说明，active
learning从没有标签的<span
class="math inline"><em>u</em></span>中选择数据，让人工对其进行标注。</p>
<figure>
<img src="./fig2.png" alt="example" />
<figcaption aria-hidden="true">example</figcaption>
</figure>
<p>上图很好的说明这个问题，(a)是需要分类的两个数据，(b)为随机选取30个点进行分类的结果，(c)是通过active
learning 选择的30个样本，其结果与正确分类十分一致。</p>
<h1 id="scenarios">Scenarios</h1>
<p>其中包含三个主要方案： * membership query synthesis * stream-based
selective sampling * pool-based active learning</p>
<figure>
<img src="./fig4.png" alt="Scenarios" />
<figcaption aria-hidden="true">Scenarios</figcaption>
</figure>
<h2 id="membership-query-synthesis">Membership Query Synthesis</h2>
<p>这种方案不依赖于事先的分布，可以从相空间中随意选取。优势是非常直接且直观，但是问题是该方案很容易生成一些没有特征指标的内容，在人类标注的过程中会困惑，例如手写数字体识别给出四不像的内容。</p>
<h2 id="stream-based-selective-sampling">Stream-Based Selective
Sampling</h2>
<p>首先从真实的分布中进行选取，然后由learner从中决定是否打标签，这一过程称之为（stream-based
or sequential active learning）</p>
<h2 id="pool-based-active-learning">Pool-Based Active Learning</h2>
<p>真实世界中，很多样本可以单次批量化标注，因此一些方案是将大的未标注区域分为小的未标注区域，然后集中进行标注。</p>
<h1 id="query-strategy-frameworks">Query Strategy Frameworks</h1>
<p>active
learning需要计算未标注数据的信息，该章节总结了通常的计算框架。定义<span
class="math inline"><em>x</em><sub><em>A</em></sub><sup>*</sup></span>为通过算法A最富有信息的实例。</p>
<h2 id="uncertainty-sampling">Uncertainty Sampling</h2>
<p>通常uncertainty
sampling策略使用熵（entropy）作为不确定性的测量，在分类任务中熵写为：</p>
<p><span class="math display">$$
x_{E N T}^*=\underset{x}{\operatorname{argmax}}-\sum_i P\left(y_i \mid x
; \theta\right) \log P\left(y_i \mid x ; \theta\right)
$$</span></p>
<p>其中<span
class="math inline"><em>y</em><sub><em>i</em></sub></span>遍历所有已经标注的数据点。</p>
<p>另一判据是最小置信构型（least confident）:</p>
<p><span class="math display">$$
x_{L C}^*=\underset{x}{\operatorname{argmin}} P\left(y^* \mid x ;
\theta\right)
$$</span></p>
<p>其中 <span
class="math inline"><em>y</em><sup>*</sup> = argmax<sub><em>y</em></sub><em>P</em>(<em>y</em> ∣ <em>x</em>; <em>θ</em>)</span>
是概率最高的标签。对于二分类任务，这个方法等价于熵判据。</p>
<h2 id="query-by-committee">Query-By-Committee</h2>
<p>通过多个模型的评判选择，当多数模型反对一个样本分类的时候，说明这个样本有最大的信息。这个方法背后的逻辑是通过最小化相空间</p>
<figure>
<img src="./fig5.png" alt="最小化相空间" />
<figcaption aria-hidden="true">最小化相空间</figcaption>
</figure>
<p>如果机器学习的任务是在相空间中寻找最小化构型，那么active
learning的目标就是在小样本的情况下尽可能的限制相空间的采样范围。为了度量反对的指标，设计了一些度量函数，其中一个是投票熵：</p>
<p><span class="math display">$$
x_{V E}^*=\underset{x}{\operatorname{argmax}}-\sum_i
\frac{V\left(y_i\right)}{C} \log \frac{V\left(y_i\right)}{C}
$$</span></p>
<p>其中<span
class="math inline"><em>y</em><sub><em>i</em></sub></span>重复所有可能的标签，<span
class="math inline"><em>V</em>(<em>y</em><sub><em>i</em></sub>)</span>是从多个模型处所收到的反对票。另一种衡量距离的方法是通过KL散度：</p>
<p><span class="math display">$$
\begin{gathered}x_{K L}^*=\underset{x}{\operatorname{argmax}}
\frac{1}{C} \sum_{c=1}^C D\left(P_{\theta^{(c)}} \|
P_{\mathcal{C}}\right) \\
D\left(P_{\theta^{(c)}} \| P_{\mathcal{C}}\right)=\sum_i P\left(y_i \mid
x ; \theta^{(c)}\right) \log \frac{P\left(y_i \mid x ;
\theta^{(c)}\right)}{P\left(y_i \mid x ;
\mathcal{C}\right)}\end{gathered}$$</span></p>
<p>其中<span class="math inline">$P\left(y_i \mid x ;
\mathcal{C}\right)=\frac{1}{C} \sum_{c=1}^C P\left(y_i \mid x ;
\theta^{(c)}\right)$</span>，这个度量就是希望选取最偏离平均值的点，减少不同模型之间的差异，希望最后的结果与平均值接近。</p>
<h2 id="expected-model-change">Expected Model Change</h2>
<p>该方案的出发点为，寻找哪些如果打标签将会极大改变当前模型的样本。令<span
class="math inline">∇<em>ℓ</em>(ℒ; <em>θ</em>)</span>为目标函数<span
class="math inline"><em>ℓ</em></span>的梯度，并且<span
class="math inline">∇<em>ℓ</em>(ℒ ∪ ⟨<em>x</em>, <em>y</em>⟩; <em>θ</em>)</span>为在添加数据<span
class="math inline">⟨<em>x</em>, <em>y</em>⟩</span>之后的新梯度。由于不知道现在样本的真实标签，因此通过概率计算：</p>
<p><span class="math display">$$x_{E G
L}^*=\underset{x}{\operatorname{argmax}} \sum_i P\left(y_i \mid x ;
\theta\right)\left\|\nabla \ell\left(\mathcal{L} \cup\left\langle x,
y_i\right\rangle ; \theta\right)\right\|$$</span></p>
<p>其中<span class="math inline">∥⋅∥</span>是欧式范数，并且<span
class="math inline">∇<em>ℓ</em>(ℒ; <em>θ</em>)</span>在之前的训练之后，往往会趋近于<span
class="math inline">0</span>，因此使用近似<span
class="math inline">∇<em>ℓ</em>(ℒ ∪ ⟨<em>x</em>, <em>y</em><sub><em>i</em></sub>⟩; <em>θ</em>) ≈ ∇<em>ℓ</em>(⟨<em>x</em>, <em>y</em><sub><em>i</em></sub>⟩; <em>θ</em>)</span>加速计算。</p>
<h2 id="variance-reduction-and-fisher-information-ratio">Variance
Reduction and Fisher Information Ratio</h2>
<p>这个方法可以用于回归任务。模型生成误差可以描述为：</p>
<p><span class="math display">$$\begin{aligned} E_T\left[(o-y)^2 \mid
x\right]= &amp; E\left[(y-E[y \mid x])^2\right] \\
&amp; +\left(E_{\mathcal{L}}[o]-E[y \mid x]\right)^2 \\
&amp;
+E_{\mathcal{L}}\left[\left(o-E_{\mathcal{L}}[o]\right)^2\right]\end{aligned}$$</span></p>
<p>其中<span
class="math inline"><em>E</em><sub>ℒ</sub>[⋅]</span>是给定标签集<span
class="math inline">ℒ</span>的期望，<span
class="math inline"><em>E</em>[⋅]</span>是候选密度<span
class="math inline"><em>P</em>(<em>y</em>|<em>x</em>)</span>的期望，<span
class="math inline"><em>E</em><sub><em>T</em></sub></span>是两者的期望。<span
class="math inline"><em>o</em> = <em>g</em>(<em>x</em>; <em>θ</em>)</span>是模型预测的记号，<span
class="math inline"><em>g</em></span>是学习的函数。<span
class="math inline"><em>E</em>[(<em>y</em> − <em>E</em>[<em>y</em> ∣ <em>x</em>])<sup>2</sup>]</span>是噪声，表示真实值
<span class="math inline"><em>y</em></span>
与其条件期望值之间的偏差平方的期望，这项源于数据本身的性质，不受模型的影响，也称为噪声。$(E_{}[o]-E[y
x])^2 <span
class="math inline"><em>是</em><em>表</em><em>示</em><em>模</em><em>型</em><em>的</em><em>偏</em><em>差</em>（<em>B</em><em>i</em><em>a</em><em>s</em>），<em>即</em><em>模</em><em>型</em><em>预</em><em>测</em><em>的</em><em>期</em><em>望</em><em>值</em><em>与</em><em>真</em><em>实</em><em>期</em><em>望</em><em>值</em><em>之</em><em>间</em><em>的</em><em>差</em><em>异</em><em>平</em><em>方</em>。</span>E_{})^2]$
表示模型的方差（Variance），即模型预测的变化性或不确定性，反映了模型的Epistemic
Uncertainty（认知不确定性），可以通过获取更多数据或改进模型来减少。</p>
<h2 id="estimated-error-reduction">Estimated Error Reduction</h2>
<h2 id="density-weighted-methods">Density-Weighted Methods</h2>
<p>从未标注的池子里面寻找有争议的点进行标注，例如SVM算法要区分间隔。</p>
<figure>
<img src="./fig6.png" alt="Density-Weighted Methods" />
<figcaption aria-hidden="true">Density-Weighted Methods</figcaption>
</figure>
<h1 id="analysis-of-active-learning">Analysis of Active Learning</h1>
<p>这部分讨论了为什么active learning是有效的。</p>
<h1 id="problem-setting-variants">Problem Setting Variants</h1>
<p>这部分讨论如何把active learning问题进行推广。</p>
<h1 id="related-research-areas">Related Research Areas</h1>
<p>Active learning 有两个要素： * 可以主动选择学习的目标 *
未标注的数据是可以获取的</p>
<h2 id="semi-supervised-learning">Semi-Supervised Learning</h2>
<h2 id="reinforcement-learning">Reinforcement Learning</h2>
<p>强化学习需要在exploration-exploitation之间进行取舍。</p>
<h2 id="equivalence-query-learning">Equivalence Query Learning</h2>
<h2 id="active-class-selection">Active Class Selection</h2>
<p><font color='red'>end</font></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Activate Learning</tag>
        <tag>ALiPy</tag>
      </tags>
  </entry>
  <entry>
    <title>Deconstructing Denoising Diffusion Models for Self-Supervised Learning</title>
    <url>/2024/03/15/DL/Deconstructing_Denoising_Diffusion_Models_for_Self_Supervised_Learning/Deconstructing_Denoising_Diffusion_Models_for_Self_Supervised_Learning/</url>
    <content><![CDATA[<h1 id="abstract">Abstract</h1>
<p>这篇文章分析了 Denoising Diffusion Models(DDM)
在图像领域的表示能力。通过不断解构DDM，从而分析Transformer的性能。最终得出结论：仅仅有很少的几个参数是有用的，对最后的图像生成起到关键作用。</p>
<p>这点和全连接神经网络十分相似，当层数过多的时候，只有输入层附近的几层与输出层附近的几层是关键的，中间几层处于液化状态（可以随意选取，不会影响最终的结果）。</p>
<span id="more"></span>
<p>现有的预训练DDM在生成任务上表现十分优异，然而留下一个开放问题：这些用于生成任务的预训练模型，其是否同样获得表征能力。</p>
<h1 id="deconstructing-denoising-diffusion-models">Deconstructing
Denoising Diffusion Models</h1>
<p>作者发现主要的关键因素是tokenizer，其表现了低维的潜空间。但是不难发现，潜变量并不是tokenizer独有的，standard
VAE, a patch-wise VAE, a patch-wise AE, and a patch-wise PCA encoder
均有潜变量。</p>
<figure>
<img src="./fig1.png" alt="fig1" />
<figcaption aria-hidden="true">fig1</figcaption>
</figure>
<p>解构的路线分为三个步骤： * 首先将Diffusion
Transformer(DiT)架构，变为自监督模型 * 逐步拆解toenizer *
尽可能尝试还原模型，将其变为一个经典的Denoising Autoencoder(DAE)</p>
<h2 id="reorienting-ddm-for-self-supervised-learning">Reorienting DDM
for Self-supervised Learning</h2>
<p>DDM的概念本质上是从DAE的概念上生成的，依旧是用于图像生成，一些设计并不适合自监督学习，一些是并不必要的（例如提升图像生成质量）。</p>
<figure>
<img src="./fig2.png" alt="fig2" />
<figcaption aria-hidden="true">fig2</figcaption>
</figure>
<ol type="1">
<li><p>Remove class-conditioning
标签在图像生成上很重要，但是不符合自监督学习的要求，因此移除标签。此举使得准确性提升，但是图像质量（FID）下降。</p></li>
<li><p>Deconstruct <font color='red'>VQGAN</font></p></li>
<li><p>Replace noise schedule
不需要逐步添加噪声，可以一步直接加入噪声</p></li>
</ol>
<figure>
<img src="./fig3.png" alt="fig3" />
<figcaption aria-hidden="true">fig3</figcaption>
</figure>
<h2 id="deconstructing-the-tokenizer">Deconstructing the Tokenizer</h2>
<p>接下来解构tokenizer，将会用到下面四种自编码器，每一种自编码器都是前面一种的简化。</p>
<ul>
<li><p>Convolutional VAE<br />
损失函数如下： <span
class="math display">∥<em>x</em> − <em>g</em>(<em>f</em>(<em>x</em>))∥<sup>2</sup> + 𝕂𝕃[<em>f</em>(<em>x</em>) ∣ 𝒩]</span>
其中<span class="math inline"><em>f</em>(⋅)</span>是编码函数<span
class="math inline"><em>g</em>(⋅)</span>是解码函数，<span
class="math inline"><em>x</em></span>是输入值，VAE是深度卷积神经网络。</p></li>
<li><p>Patch-wise VAE<br />
选用线性的函数作为编码解码器（全连接网络），损失函数设置如下： <span
class="math display">∥<em>x</em> − <em>U</em><sup><em>T</em></sup><em>V</em><em>x</em>∥<sup>2</sup> + 𝕂𝕃[<em>V</em><em>x</em> ∣ 𝒩]</span>
<span class="math inline"><em>U</em>, <em>V</em></span>是矩阵。</p></li>
<li><p>Patch-wise AE<br />
进一步简化，删除正则化项： <span
class="math display">∥<em>x</em> − <em>U</em><sup><em>T</em></sup><em>V</em><em>x</em>∥<sup>2</sup></span></p></li>
<li><p>Patch-wise PCA<br />
PCA 可以看成是一种特殊的 AE，损失函数为： <span
class="math display">∥<em>x</em> − <em>V</em><sup><em>T</em></sup><em>V</em><em>x</em>∥<sup>2</sup></span></p></li>
</ul>
<figure>
<img src="./fig4.png" alt="fig4" />
<figcaption aria-hidden="true">fig4</figcaption>
</figure>
<p>可以从上表中看出，表现性能都差不多，并且计算KL散度也不是很重要。甚至PCA工作的更好</p>
<h2 id="toward-classical-denoising-autoencoders">Toward Classical
Denoising Autoencoders</h2>
<p>逐步减去 PCA-based DDM 与经典DAE之间的差距。 <img src="./fig5.png"
alt="fig5" /></p>
<h1 id="conclusion">Conclusion</h1>
<figure>
<img src="./fig6.png" alt="fig6" />
<figcaption aria-hidden="true">fig6</figcaption>
</figure>
<p>从最后的实验结果看出，对潜变量空间加入噪音对结果的影响更大。因此说明对模型训练起关键作用的是少数几个参数。</p>
<p><font color='red'>应该是我没有读懂该文章的实验思路，在我的理解下，潜变量应该是浓缩了更多信息的空间，对其加噪音本来就会比对原像素空间其更大的影响。另一方面，这种论证方法，我并不认为十分研究，应该有更多的理论分析保证。</font></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Diffusion Models</tag>
        <tag>Denoising Diffusion Models</tag>
        <tag>Self-Supervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Research Reviews of Combinatorial Optimization Methods Based on Deep Reinforcement Learning</title>
    <url>/2024/06/01/DL/DRL_COM/DRL_COM/</url>
    <content><![CDATA[<p>The application of deep reinforcement learning to combinatorial
optimization methods.</p>
<p>Reference: - <a href="https://arxiv.org/abs/1811.06128">Machine
Learning for Combinatorial Optimization: a Methodological Tour
d’Horizon</a></p>
<p>Link:<br />
- <a href="/2024/06/03/DL/DRL_COM/NeuRewriter/" title="Learning to Perform Local Rewriting for Combinatorial Optimization">Learning to Perform Local Rewriting for Combinatorial Optimization</a><br />
- <a href="/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/" title="Machine Learning with Graphs">graph networks</a><br />
- <a href="/2024/06/05/DL/DRL_COM/PointerNetwork/" title="Pointer Networks">pointer networks</a><br />
- <a href="/2024/06/11/DL/DRL_COM/NCORL/" title="Neural Combiantorial Optimization with Reinforcement Learning">Neural Combiantorial Optimization with Reinforcement Learning</a></p>
<span id="more"></span>
<h1 id="combinatorial-optimization-problem">Combinatorial Optimization
Problem</h1>
<p>Combinatorial optimization problem(COP) usually described as:</p>
<p><span class="math display">$$\begin{align}
\begin{aligned}
&amp; \min F(x) \\
&amp; \text { s.t. } G(x) \geq 0 \\
&amp; x \in D
\end{aligned}
\end{align}$$</span></p>
<p>Solving combinatorial optimization problems involves two types of
methods: exact and approximate approaches.</p>
<p>For exact approaches, there are the ‘Branch and Bound’ and ‘Dynamic
Programming’ algorithms.</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Branch_and_bound">Branch and
Bound algorithm</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/521325108">Lecture
009-Branch-and-Bound</a></li>
</ul>
<p>The Branch and Bound algorithm depends on efficient estimation of the
lower and upper bounds of regions/branches of the search space. If no
bounds are available, the algorithm degenerates to an exhaustive
search.</p>
<p><font color='red'>Can the Branch and Bound algorithm be used for
continuous problems?</font></p>
<p>For approximate approaches, there are the “Approximate” and
“Heuristic” algorithm.</p>
<h1 id="introduction">Introduction</h1>
<p>Hopefield addresses the first thought about using Neural Networks to
solve optimization problems.</p>
<ul>
<li><a href="https://link.springer.com/article/10.1007/BF00339943">“
Neural ”  computation  of  decisions  in optimization  problems</a></li>
<li><a
href="https://www.researchgate.net/publication/220669035_Neural_Networks_for_Combinatorial_Optimization_A_Review_of_More_Than_a_Decade_of_Research">Neural
Networks for Combinatorial Optimization: A Review of More Than a Decade
of Research</a></li>
</ul>
<p>In 2015, Vinyals initiated the application of neural networks in
optimization by comparing combinatorial optimization to machine
translation and introducing the Pointer Network (Ptr-Net). This
comparison highlighted their common ground in sequence-to-sequence
(Seq2Seq) modeling.</p>
<ul>
<li><a href="/2024/06/05/DL/DRL_COM/PointerNetwork/" title="Pointer Networks">pointer networks</a></li>
</ul>
<p>After introducting Ptr-Net, many advanced algorithms have been
developed that combine graph networks or transforms.
<font color='red'>Combining the Pointer Network (Ptr-Net) with graph
networks is futile. The essence of graph networks lies in message
passing, which relies on mean-field theory. Therefore, enhancing these
algorithms amounts to an approximation. The method of choosing the mean
field determines whether those functions can succeed.</font></p>
<p>The above algorithm employs an end-to-end approach, which has the
advantage of providing answers directly and more quickly compared to
traditional algorithms. However, the drawback is that it does not
guarantee the optimality of the solutions and may fail when dealing with
larger problems.</p>
<p>Another type is to base deep reinforcement learning on boosting
traditional algorithms, with the dual directions of improving the branch
and bound method and the iterative searching approach.</p>
<ul>
<li><a href="https://arxiv.org/abs/1811.06128">Machine Learning for
Combinatorial Optimization: a Methodological Tour d’Horizon</a></li>
</ul>
<p>Before continuing our discussion, let’s address why reinforcement
learning is needed to solve combinatorial optimization problems. While
supervised learning networks perform well, their capability is
constrained by the training data. As a result, the quality of the
solution cannot exceed that of the input data. Hence, it is essential to
employ reinforcement learning to enhance the solution quality.</p>
<ul>
<li>The performance of the model is tied to the quality of the
supervised labels.</li>
<li>Getting high-quality labeled data is expensive and may be infeasible
for new problem statements. <font color='red'> How were they sure they
got the best answer?</font></li>
<li>One cares more about finding a competitive solution more than
replicating the results of another algorithm.</li>
</ul>
<h1 id="end-to-end-approach">End-to-End Approach</h1>
<p>Using <a href="/2024/06/05/DL/DRL_COM/PointerNetwork/" title="Pointer Networks">pointer networks</a> and <a href="/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/" title="Machine Learning with Graphs">graph networks</a> is a common approach to solving
combinatorial optimization problems with reinforcement learning.</p>
<figure>
<img src="./table1.png" alt="surbey of point to point model" />
<figcaption aria-hidden="true">surbey of point to point
model</figcaption>
</figure>
<p>Two directions for improving the end-to-end approach are changing the
model and using different training patterns. In my view, the network
model determines the peak performance level, while the training patterns
influence whether this peak can be reached and factors like time and
data requirements.</p>
<p>However, sometimes network models incorporate training patterns,
necessitating an in-depth exploration of specific algorithms. This study
originated from <a href="/2024/06/11/DL/DRL_COM/NCORL/" title="Neural Combiantorial Optimization with Reinforcement Learning">Neural Combiantorial Optimization with Reinforcement Learning</a>, which was later modified by
<a href="/2024/06/11/DL/DRL_COM/RLSVRP/" title="Reinforcement Learning for Solving the Vehicle Routing Problem">Reinforcement Learning for Solving the Vehicle Routing Problem</a>, followed by numerous improved versions.</p>
<h1 id="local-search-methods-based-drl">Local Search Methods based
DRL</h1>
<p>Previously, local search algorithm rules were designed manually,
which is a form of heuristic search. However, we now aim to use
reinforcement learning to automatically generate these search rules to
get better performance.</p>
<p>Chen addressed a combination optimaztion search model “<a href="/2024/06/03/DL/DRL_COM/NeuRewriter/" title="Learning to Perform Local Rewriting for Combinatorial Optimization">Learning to Perform Local Rewriting for Combinatorial Optimization</a>”
based on DRL. <a
href="https://dl.acm.org/doi/10.5555/3454287.3455005">Yolcu</a> modify
the local search methods in satisfiability problem, though need less
step find the ground-state, it will use more time. <a
href="https://arxiv.org/abs/2002.08539">Gao</a> based on Large
Neighborhood Search optimize Destory abd Repair operator. <a
href="https://openreview.net/forum?id=BJe1334YDH">Lu</a> created Learn
to improve(LSI).</p>
<figure>
<img src="./table3.png" alt="Local Search Methods based DRL" />
<figcaption aria-hidden="true">Local Search Methods based
DRL</figcaption>
</figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Combinatorial Optimization Methods</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Combiantorial Optimization with Reinforcement Learning</title>
    <url>/2024/06/11/DL/DRL_COM/NCORL/</url>
    <content><![CDATA[<p>Utilizing neural networks and reinforcement learning to tackle the
Traveling Salesman Problem, where the neural network model is a
Recurrent Neural Network (RNN), and the policy for reinforcement
learning employs policy gradients.</p>
<p>Reference: * <a href="https://arxiv.org/abs/1611.09940">Neural
Combinatorial Optimization with Reinforcement Learning</a> * <a
href="https://github.com/pemami4911/neural-combinatorial-rl-pytorch">code</a></p>
<span id="more"></span>
<p>On the 2D Euclidean TSP, given an input graph, represented as a
sequence of <span class="math inline"><em>n</em></span> cities in a two
dimensional space <span
class="math inline"><em>s</em> = {<strong>x</strong><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>n</em></sup></span>
where each <span
class="math inline"><strong>x</strong><sub><em>i</em></sub> ∈ ℝ<sup>2</sup></span>,
we are concerned with finding a permutation of the points <span
class="math inline"><em>π</em></span>, termed a tour, that visits each
city once and has the minimum total length. We define the length of a
tour defined by a permutation <span
class="math inline"><em>π</em></span> as <span
class="math display">$$\begin{align}
L(\pi \mid
s)=\left\|\mathbf{x}_{\pi(n)}-\mathbf{x}_{\pi(1)}\right\|_2+\sum_{i=1}^{n-1}\left\|\mathbf{x}_{\pi(i)}-\mathbf{x}_{\pi(i+1)}\right\|_2,
\end{align}$$</span> where <span
class="math inline">∥ ⋅ ∥<sub>2</sub></span> denotes <span
class="math inline"><em>ℓ</em><sub>2</sub></span> norm. The aim</p>
<p>Neural network architecture uses the chain relue to factorize the
probability of a tour as: <span class="math display">$$\begin{align}
p(\pi \mid s)=\prod_{i=1}^n p(\pi(i) \mid \pi(&lt;i), s)
\end{align}$$</span> where <span
class="math inline"><em>p</em>(<em>π</em> ∣ <em>s</em>)</span> is
stochastic policy, and then uses individual softmax modules to represent
each term</p>
<p>The architecture is pointer network. <img src="./architecture.png"
alt="architecture" /></p>
<p>Propose to use model-free policy-based Reinforcement Learning to
optimize the parameters of a pointer network denoted <span
class="math inline"><strong>θ</strong></span>. Our training objective is
the expected tour length which, given an input graph <span
class="math inline"><em>s</em></span>, is defined as</p>
<p><span class="math display">$$\begin{align}
J(\boldsymbol{\theta} \mid s)&amp;=\mathbb{E}_{\pi \sim p_\theta(. \mid
s)} L(\pi \mid s) \\
\nabla_\theta J(\theta \mid s)&amp;=\mathbb{E}_{\pi \sim p_\theta(. \mid
s)}\left[(L(\pi \mid s)-b(s)) \nabla_\theta \log p_\theta(\pi \mid
s)\right]\\
\nabla_\theta J(\theta) &amp;\approx \frac{1}{B}
\sum_{i=1}^B\left(L\left(\pi_i \mid s_i\right)-b\left(s_i\right)\right)
\nabla_\theta \log p_\theta\left(\pi_i \mid s_i\right)
\end{align}$$</span> where <span
class="math inline"><em>b</em>(<em>s</em>)</span> denotes a baseline
function that does not depend on <span
class="math inline"><em>π</em></span> and estimates the expected tour
length to reduce the variance of the gradients.</p>
<p>From the above discussion, it is clear that <span
class="math inline"><em>b</em>(<em>s</em>)</span>， which provides a
method to measure path length, is crucial for achieving an optimal
network. The network will deliver an exact policy if <span
class="math inline"><em>b</em>(<em>s</em>)</span> offers a precise
value.</p>
<p>We introduce an auxiliary network, called a critic and parameterized
by <span class="math inline"><em>θ</em><sub><em>v</em></sub></span>, to
learn the expected tour length found by our current policy <span
class="math inline"><em>p</em><sub><em>θ</em></sub></span> given an
input sequence <span class="math inline"><em>s</em></span>.</p>
<p><span class="math display">$$\begin{align}
\mathcal{L}\left(\theta_v\right)=\frac{1}{B}
\sum_{i=1}^B\left\|b_{\theta_v}\left(s_i\right)-L\left(\pi_i \mid
s_i\right)\right\|_2^2
\end{align}$$</span></p>
<p><font color='red'>I believe <span
class="math inline"><em>b</em>(<em>s</em>)</span> is more than just an
auxiliary network. As crucial as an engine in chess, <span
class="math inline"><em>b</em>(<em>s</em>)</span> provides a criterion
for key optimization problems, which can be seamlessly integrated into
search algorithms. However, to utilize it effectively for the TSP, we
need to devise a functional approach to incorporate actions into <span
class="math inline"><em>s</em></span>.</font></p>
<figure>
<img src="./actor-critic.png" alt="architecture" />
<figcaption aria-hidden="true">architecture</figcaption>
</figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Combinatorial Optimization Methods</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning to Perform Local Rewriting for Combinatorial Optimization</title>
    <url>/2024/06/03/DL/DRL_COM/NeuRewriter/</url>
    <content><![CDATA[<p>这篇文章提出了NeuRewriter的方法，使用强化学习中 actor-critic
的方式训练。</p>
<p>感觉类似于 “A Monte Carlo Policy Gradient Method with Local Search
for Binary Optimization”
中的操作。<font color='yellow'>当然了，NeuRewriter是最先提出来的，时间跨度基本都有4年了。</font></p>
<p>Reference: * <a href="https://arxiv.org/abs/1810.00337">Learning to
Perform Local Rewriting for Combinatorial Optimization</a> * <a
href="https://github.com/facebookresearch/neural-rewriter">github地址</a></p>
<span id="more"></span>
<h1 id="problem-setup">Problem Setup</h1>
<p>Let <span class="math inline">𝒮</span> be the solution’s space of
problem domain, and <span class="math inline"><em>c</em> : 𝒮 → ℝ</span>
be the cost function. The goal of optimization is to find <span
class="math inline">arg min<sub><em>s</em> ∈ 𝒮</sub><em>c</em>(<em>s</em>)</span>.
Formally, each solutionis a state, and each local region and the
associated rule is an action.</p>
<p>Optimization as a rewriting problem. Let <span
class="math inline">𝒰</span> be the rewriting ruleset. Suppose <span
class="math inline"><em>s</em><sub><em>t</em></sub></span> is the
current solution (or state) at iteration <span
class="math inline"><em>t</em></span>. We first compute a
state-dependent region set <span
class="math inline"><em>Ω</em>(<em>s</em><sub><em>t</em></sub>)</span>,
then pick a region <span
class="math inline"><em>ω</em><sub><em>t</em></sub> ∈ <em>Ω</em>(<em>s</em><sub><em>t</em></sub>)</span>
using the region-picking policy <span
class="math inline"><em>π</em><sub><em>ω</em></sub>(<em>ω</em><sub><em>t</em></sub> ∣ <em>s</em><sub><em>t</em></sub>)</span>.
We then pick a rewriting rule <span
class="math inline"><em>u</em><sub><em>t</em></sub></span> applicable to
that region <span
class="math inline"><em>ω</em><sub><em>t</em></sub></span> using the
rule-picking policy <span
class="math inline"><em>π</em><sub><em>u</em></sub>(<em>u</em><sub><em>t</em></sub> ∣ <em>s</em><sub><em>t</em></sub>[<em>ω</em><sub><em>t</em></sub>])</span>,
where <span
class="math inline"><em>s</em><sub><em>t</em></sub>[<em>ω</em><sub><em>t</em></sub>]</span>
is a subset of state <span
class="math inline"><em>s</em><sub><em>t</em></sub></span>.</p>
<p><span
class="math inline"><em>Ω</em>(<em>s</em><sub><em>t</em></sub>)</span>
is a problem-dependent region set. For expression simplification, <span
class="math inline"><em>Ω</em>(<em>s</em><sub><em>t</em></sub>)</span>
includes all sub-trees of the expression parse trees; for job
scheduling, <span
class="math inline"><em>Ω</em>(<em>s</em><sub><em>t</em></sub>)</span>
covers all job nodes for scheduling; and for vehicle routing, it
includes all nodes in the route.</p>
<p>We then apply this rewriting rule <span
class="math inline"><em>u</em><sub><em>t</em></sub> ∈ 𝒰</span> to <span
class="math inline"><em>s</em><sub><em>t</em></sub>[<em>ω</em><sub><em>t</em></sub>]</span>,
and obtain the next state <span
class="math inline"><em>s</em><sub><em>t</em> + 1</sub>=</span> <span
class="math inline"><em>f</em>(<em>s</em><sub><em>t</em></sub>, <em>ω</em><sub><em>t</em></sub>, <em>u</em><sub><em>t</em></sub>)</span>.
Given an initial solution (or state) <span
class="math inline"><em>s</em><sub>0</sub></span>, our goal is to find a
sequence of rewriting steps <span
class="math inline">(<em>s</em><sub>0</sub>, (<em>ω</em><sub>0</sub>, <em>u</em><sub>0</sub>)), (<em>s</em><sub>1</sub>, (<em>ω</em><sub>1</sub>, <em>u</em><sub>1</sub>)), …, (<em>s</em><sub><em>T</em> − 1</sub>, (<em>ω</em><sub><em>T</em> − 1</sub>, <em>u</em><sub><em>T</em> − 1</sub>)), <em>s</em><sub><em>T</em></sub></span>
so that the final cost <span
class="math inline"><em>c</em>(<em>s</em><sub><em>T</em></sub>)</span>
is minimized.</p>
<p><font color='blue'>In this part mention two new functions:
region-picking and rule-picking, what’s mean of them? At the end of the
following is rewriting rule. I need the accurate meaning of
rewriting.</font> Before this paper, same idea had been proposed by <a
href="https://github.com/halide/Halide">Halide</a>.</p>
<p>Instead of searching from scratch, this work searches solution by
iteratively applying local rewriting rules to the existing until
convergence. Thus, rewriting formulation is suitable for such
problem:</p>
<ul>
<li>Easily find feasible solution.</li>
<li>Well-behaved local structures, which could be utilized to
incrementally improve the solution. <font color='red'>It’s a hard to
satisfy feature in spin glass.</font></li>
</ul>
<p><font color='red'></p>
<p>Three words means nedd to know: * region-picking * rule-picking *
rewriting rule</p>
<p></font></p>
<h1 id="neural-rewriter-model">Neural Rewriter Model</h1>
<figure>
<img src="./1.png" alt="Model Overview" />
<figcaption aria-hidden="true">Model Overview</figcaption>
</figure>
<p>Above is the entire model framework. Show the pseudo-code below.</p>
<figure>
<img src="./ForwardPass.png" alt="ForwardPass" />
<figcaption aria-hidden="true">ForwardPass</figcaption>
</figure>
<p>Score predictor. Given the state <span
class="math inline"><em>s</em><sub><em>t</em></sub></span>, the score
predictor computes a score <span
class="math inline"><em>Q</em>(<em>s</em><sub><em>t</em></sub>, <em>ω</em><sub><em>t</em></sub>)</span>
for every <span
class="math inline"><em>ω</em><sub><em>t</em></sub> ∈ <em>Ω</em>(<em>s</em><sub><em>t</em></sub>)</span>,
which measures the benefit of rewriting <span
class="math inline"><em>s</em><sub><em>t</em></sub>[<em>ω</em><sub><em>t</em></sub>]</span>.
A high score indicates that rewriting <span
class="math inline"><em>s</em><sub><em>t</em></sub>[<em>ω</em><sub><em>t</em></sub>]</span>
could be desirable.</p>
<p>In lines 2-10, I believe this algorithm resembles the Monte Carlo
method; it initially establishes a probability distribution by <span
class="math inline"><em>Ω</em><sub><em>ω</em></sub></span> and gets each
score; then selects one from it. However, the choice is not entirely
random—an acceptance probability is set. Through this process,
high-quality data for learning are generated. Thus, region-picking
function serves as a judgment function to assess whether the situation
is favorable or not. The loss function of this par write as:</p>
<p><span class="math display">$$\begin{align}
L_\omega(\theta)=\frac{1}{T}
\sum_{t=0}^{T-1}\left(\sum_{t^{\prime}=t}^{T-1} \gamma^{t^{\prime}-t}
r\left(s_t^{\prime},\left(\omega_t^{\prime},
u_t^{\prime}\right)\right)-Q\left(s_t, \omega_t ; \theta\right)\right)^2
\end{align}$$</span></p>
<p>Rule selector. Given <span
class="math inline"><em>s</em><sub><em>t</em></sub>[<em>ω</em><sub><em>t</em></sub>]</span>
to be rewritten, the rule-picking policy predicts a probability
distribution <span
class="math inline"><em>π</em><sub><em>u</em></sub>(<em>s</em><sub><em>t</em></sub>[<em>ω</em><sub><em>t</em></sub>])</span>
over the entire ruleset <span class="math inline">𝒰</span>, and selects
a rule <span
class="math inline"><em>u</em><sub><em>t</em></sub> ∈ 𝒰</span> to apply
accordingly.</p>
<p>In lines 11-16, we will employ the Advantage Actor-Critic algorithm
to train both the Rule-picking and Score-picking models. The primary
network we aim to obtain is the Rule-picking model, which will assist us
in addressing subsequent problems.</p>
<p><span class="math display">$$\begin{align}
\Delta\left(s_t,\left(\omega_t, u_t\right)\right) &amp;\equiv
\sum_{t^{\prime}=t}^{T-1} \gamma^{t^{\prime}-t}
r\left(s_t^{\prime},\left(\omega_t^{\prime},
u_t^{\prime}\right)\right)-Q\left(s_t, \omega_t ; \theta\right) \\
L_u(\phi)&amp;=-\sum_{t=0}^{T-1} \Delta\left(s_t,\left(\omega_t,
u_t\right)\right) \log \pi_u\left(u_t \mid s_t\left[\omega_t\right] ;
\phi\right) \\
L(\theta, \phi)&amp;=L_u(\phi)+\alpha L_\omega(\theta)
\end{align}$$</span></p>
<p>The rewriting rule merely changes the description method; in fact, it
addresses the same issue, similar to the gauge theory in the SK
model.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Combinatorial Optimization Methods</tag>
      </tags>
  </entry>
  <entry>
    <title>Pointer Networks</title>
    <url>/2024/06/05/DL/DRL_COM/PointerNetwork/</url>
    <content><![CDATA[<p>Pointer Network(Ptr-Net) uses attention as a pointer to select a
member of the input sequence as the output. The author shows that
Ptr-Net could solve three challenging geometric problems - finding
planner convex hulls, computing Delunary triangulations, and the planner
Travelling Salesman Problem.</p>
<p>Reference: * <a href="https://arxiv.org/abs/1506.03134">Pointer
Networks</a> * <a href="https://zhuanlan.zhihu.com/p/48959800">Pointer
Networks简介及其应用</a> * <a
href="https://myencyclopedia.github.io/zh/2020/tsp-3-pointer-net/">TSP问题从DP算法到深度学习3：Pointer
Network</a> * <a
href="https://www.bilibili.com/video/BV1m3411p7wD/?p=36&amp;vd_source=f9eb99d14a0acbcfa188c1e70864412e">2022最新版-李宏毅机器学习深度学习课程</a></p>
<span id="more"></span>
<p>Intuitively, using a sequence-to-sequence model, such as an RNN, to
solve combination optimization problems, we must address two issues: the
input size, which has been solved by straightforward technology, and the
solution size, which motivates the development of Ptr-Net.</p>
<figure>
<img src="./1.png" alt="RNN vs Ptr-Net" />
<figcaption aria-hidden="true">RNN vs Ptr-Net</figcaption>
</figure>
<p>We must set a fixed output dimension(Fig 1.a) to address the problem
of finding planar convex hulls in an RNN. It means that if the size of
the solution exceeds this fixed dimension, we cannot obtain the correct
answer theoretically.</p>
<p>To solve the problem of solution dimensions, create a new network
structure (Ptr-Net) based on an attention-based model. Firstly, input
the problem into Ptr-Net to obtain the starting point of the solution.
Then, use that starting point along with the problem as the new input
for Ptr-Net to generate a new answer, which serves as a new key. Repeat
this process until we receive an end signal where the site has
appeared(Fig 1.b).</p>
<p>In each step, using a parametric model to estimate the terms of the
probability chain rule, i.e.</p>
<p><span class="math display">$$\begin{equation}
p\left(\mathcal{C}^{\mathcal{P}} \mid \mathcal{P} ;
\theta\right)=\prod_{i=1}^{m(\mathcal{P})} p_\theta\left(C_i \mid C_1,
\ldots, C_{i-1}, \mathcal{P} ; \theta\right)
\end{equation}$$</span></p>
<p>Here <span class="math inline">(𝒫, 𝒞<sup>𝒫</sup>)</span> is a
training pair, <span
class="math inline">𝒫 = {<em>P</em><sub>1</sub>, …, <em>P</em><sub><em>n</em></sub>}</span>
is a sequence of <span class="math inline"><em>n</em></span> vectors and
<span
class="math inline">𝒞<sup>𝒫</sup> = {<em>C</em><sub>1</sub>, …, <em>C</em><sub><em>m</em>(𝒫)</sub>}</span>
is a sequence of <span class="math inline"><em>m</em>(𝒫)</span> indices,
each between 1 and <span class="math inline"><em>n</em></span>.</p>
<p>The parameters of the model are learnt by maximizing the conditional
probabilities for the training set, i.e.</p>
<p><span class="math display">$$\begin{equation}
\theta^*=\underset{\theta}{\arg \max } \sum_{\mathcal{P},
\mathcal{C}^{\mathcal{P}}} \log p\left(\mathcal{C}^{\mathcal{P}} \mid
\mathcal{P} ; \theta\right),
\end{equation}$$</span></p>
<p>where the sum is over training examples.</p>
<p>Compute the attention vector at each output time i as follows:</p>
<p><span class="math display">$$\begin{align}
u_j^i&amp;=v^T \tanh \left(W_1 e_j+W_2 d_i\right) \quad j \in(1, \ldots,
n) \\
p\left(C_i \mid C_1, \ldots, C_{i-1},
\mathcal{P}\right)&amp;=\operatorname{softmax}\left(u^i\right)
\end{align}$$</span></p>
<p>where <span
class="math inline"><em>e</em><sub><em>j</em></sub></span> is input
data(encoder state), <span
class="math inline"><em>d</em><sub><em>i</em></sub></span> is output
data(decoder state), <span class="math inline"><em>u</em></span> is
softmax normalized output distribution, and <span
class="math inline"><em>W</em><sub>1</sub>, <em>W</em><sub>2</sub></span>
are learnable parameters of model.</p>
<figure>
<img src="./2.png" alt="step1" />
<figcaption aria-hidden="true">step1</figcaption>
</figure>
<p>Then, select the site with the highest probability, 1 in the above
figure.</p>
<figure>
<img src="./3.png" alt="step2" />
<figcaption aria-hidden="true">step2</figcaption>
</figure>
<p>Next, choose site 1 data <span
class="math inline">(<em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>)</span>
as the key and input it into the network to obtain the distribution.
Continue until we receive the end signal, identified as <span
class="math inline">(<em>x</em><sub>1</sub>, <em>y</em><sub>0</sub>)</span>
in this example.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Combinatorial Optimization Methods</tag>
      </tags>
  </entry>
  <entry>
    <title>Git Note</title>
    <url>/2022/09/16/CS/git/git/</url>
    <content><![CDATA[<h1 id="git简单教程">Git简单教程</h1>
<p>工作逻辑图： <img src="./workflow.png" alt="逻辑图" /></p>
<p>每次更改完文件，需要将文件先增加进版本库，然后再提交改变确认本次所有修改完成，并备注本次内容。</p>
<p>reference: * <a
href="https://mofanpy.com/tutorials/others/git/">莫凡PYTHON</a></p>
<span id="more"></span>
<h1 id="基础操作">基础操作</h1>
<p>创建一个新的仓库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure>
<p>查看当前版本库状态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure>
<p>增加进版本库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git add .       #添加所有更改进版本库</span><br><span class="line">git add 文件     #添加具体文件进版本库</span><br></pre></td></tr></table></figure>
<p>提交改变</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git commit -m &quot;这次提交的备注&quot;</span><br></pre></td></tr></table></figure>
<h1 id="回到从前">回到从前</h1>
<h2 id="查看历史版本">查看历史版本</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git lod --oneline</span><br></pre></td></tr></table></figure>
<h2 id="回到过去的版本">回到过去的版本</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 不管我们之前有没有做了一些 add 工作, 这一步让我们回到 上一次的 commit</span><br><span class="line">$ git reset --hard HEAD    </span><br><span class="line"># 输出</span><br><span class="line">HEAD is now at 904e1ba change 2</span><br><span class="line">-----------------------</span><br><span class="line"># 看看所有的log</span><br><span class="line">$ git log --oneline</span><br><span class="line"># 输出</span><br><span class="line">904e1ba change 2</span><br><span class="line">c6762a1 change 1</span><br><span class="line">13be9a7 create 1.py</span><br><span class="line">-----------------------</span><br><span class="line"># 回到 c6762a1 change 1</span><br><span class="line"># 方式1: &quot;HEAD^&quot;</span><br><span class="line">$ git reset --hard HEAD^  </span><br><span class="line"></span><br><span class="line"># 方式2: &quot;commit id&quot;</span><br><span class="line">$ git reset --hard c6762a1</span><br><span class="line">-----------------------</span><br><span class="line"># 看看现在的 log</span><br><span class="line">$ git log --oneline</span><br><span class="line"># 输出</span><br><span class="line">c6762a1 change 1</span><br><span class="line">13be9a7 create 1.py</span><br></pre></td></tr></table></figure>
<h2 id="在回到历史版本之后再回来">在回到历史版本之后再回来</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#第一步</span><br><span class="line">$ git reflog</span><br><span class="line"># 输出</span><br><span class="line">c6762a1 HEAD@&#123;0&#125;: reset: moving to c6762a1</span><br><span class="line">904e1ba HEAD@&#123;1&#125;: commit (amend): change 2</span><br><span class="line">0107760 HEAD@&#123;2&#125;: commit: change 2</span><br><span class="line">c6762a1 HEAD@&#123;3&#125;: commit: change 1</span><br><span class="line">13be9a7 HEAD@&#123;4&#125;: commit (initial): create 1.py</span><br><span class="line"></span><br><span class="line">#第二步</span><br><span class="line">$ git reset --hard 904e1ba</span><br><span class="line">$ git log --oneline</span><br><span class="line"># 输出</span><br><span class="line">904e1ba change 2</span><br><span class="line">c6762a1 change 1</span><br><span class="line">13be9a7 create 1.py</span><br></pre></td></tr></table></figure>
<h2 id="单个文件回到过去">单个文件回到过去</h2>
<p>利用 <code>checkout</code>
将过去版本的文件覆盖现在同名文件，然后作为当前版本的一个新的更新，更新进版本库。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git log --oneline</span><br><span class="line"># 输出</span><br><span class="line">904e1ba change 2</span><br><span class="line">c6762a1 change 1</span><br><span class="line">13be9a7 create 1.py</span><br><span class="line">---------------------</span><br><span class="line">$ git checkout c6762a1 -- 1.py  #checkout + id + -- + 文件名字</span><br></pre></td></tr></table></figure>
<p>然后增加进版本库并提交改变</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add 1.py</span><br><span class="line">$ git commit -m &quot;back to change 1 and add comment for 1.py&quot;</span><br><span class="line">$ git log --oneline</span><br><span class="line"></span><br><span class="line"># 输出</span><br><span class="line">47f167e back to change 1 and add comment for 1.py</span><br><span class="line">904e1ba change 2</span><br><span class="line">c6762a1 change 1</span><br><span class="line">13be9a7 create 1.py</span><br></pre></td></tr></table></figure>
<h1 id="推送到远程仓库">推送到远程仓库</h1>
<p>创建远程仓库、设置本地文件帐号密码。首先在 github
创建新的仓库（不要添加README文件，以及创建主分支），之后按照 github
上提示，将本地文件推送到新的仓库。</p>
<p>将主分支推送到远程仓库（老项目为master, 新项目为main）
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure></p>
<p>如果远程库和本地库不相同，想要强行覆盖远程库 ““” git push -u origin
master -f ““”</p>
<h1 id="拉取远程仓库内容">拉取远程仓库内容</h1>
<p>本地文件在拉取远程仓库前没有修改：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git pull origin master</span><br></pre></td></tr></table></figure>
<h2 id="本地文件发生改动">本地文件发生改动</h2>
<p>首先获取远程仓库的文件： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git fetch --all</span><br></pre></td></tr></table></figure></p>
<p>接下来有两个选择： * 直接覆盖本地文件 *
将远程仓库文件作为本地的一个分支</p>
<p>直接覆盖方案： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git reset --hard origin/master</span><br></pre></td></tr></table></figure></p>
<p>创建其它分支方案： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git reset --hard origin/&lt;branch_name&gt;</span><br></pre></td></tr></table></figure></p>
<h1 id="将远程内容部署到本地">将远程内容部署到本地</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://.....</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>Reinforcement Learning for Solving the Vehicle Routing Problem</title>
    <url>/2024/06/11/DL/DRL_COM/RLSVRP/</url>
    <content><![CDATA[<p>Although the framework proposed by Bello at <a href="/2024/06/11/DL/DRL_COM/NCORL/" title="Neural Combiantorial Optimization with Reinforcement Learning">Neural Combiantorial Optimization with Reinforcement Learning</a> works well
on TSP, it is not applicable to more complicated combinatorial
optimization problems in which the system representation varies over
time, such as Vehicle Routing Problem(VRP). Thus, this paper propose a
new model to overcome drawback of previous.</p>
<p>Reference: * <a href="https://arxiv.org/abs/1802.04240">Reinforcement
Learning for Solving the Vehicle Routing Problem</a> * <a
href="https://github.com/OptMLGroup/VRP-RL">code</a></p>
<span id="more"></span>
<figure>
<img src="./figure1.png" alt="drawback of previous" />
<figcaption aria-hidden="true">drawback of previous</figcaption>
</figure>
<p>As figure 1 have been show, once change a part of elements, we must
update the whole input.</p>
<figure>
<img src="./figure2.png" alt="new model" />
<figcaption aria-hidden="true">new model</figcaption>
</figure>
<p>Therefore, in new model, thet just simply leave out the encoder RNN
and directly use the embedded inputs instead of the RNN hidden states.
By this modification, many of the computational complications disappear,
without decreasing the model’s efficiency.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Combinatorial Optimization Methods</tag>
      </tags>
  </entry>
  <entry>
    <title>KAN: Kolmogorov–Arnold Networks</title>
    <url>/2024/05/08/DL/Kolmogorov%E2%80%93Arnold_Networks/Kolmogorov%E2%80%93Arnold_Networks/</url>
    <content><![CDATA[<p>文献地址： * <a href="https://arxiv.org/abs/2404.19756">KAN:
Kolmogorov–Arnold Networks</a> * <a
href="https://github.com/KindXiaoming/pykan?tab=readme-ov-file">GitHub地址</a></p>
<p>传统的全连接神经网络用于拟合非线性函数，然而全连接网络真的是最好的架构了么？对于全连接网络有明显的缺点，显存开销大，可解释性差。</p>
<p>本文作者提出了KAN网络架构，其基于 Kolmogorov-Arnold representation
theorem。该网络架构将“参数进行非线性激活”，然后通过全连接。</p>
<figure>
<img src="./0-1.png"
alt="Multi-Layer Perceptrons (MLPs) vs. Kolmogorov-Arnold Networks (KANs)" />
<figcaption aria-hidden="true">Multi-Layer Perceptrons (MLPs)
vs. Kolmogorov-Arnold Networks (KANs)</figcaption>
</figure>
<span id="more"></span>
<p>有几个问题： 1. KAN的参数是如何确定的，架构和全连接的区别在哪里？ 2.
他这个非线性过程是不是找到一组基，然后利用这个基进行表示？ 3.
为什么它能够解决全连接的内存开销问题？ 4. 为什么说它具备可解释性？</p>
<h1 id="kolmogorov-arnold-representation-theorem">Kolmogorov-Arnold
Representation theorem</h1>
<p>MLP的有效性是因为万能近似定理，KAN的有效性是基于Kolmogorov-Arnold
Representation theorem。该定理表明：如果一个函数<span
class="math inline"><em>f</em></span>在一个区域上是多元连续函数，那么其可以写为多个单变量函数的和。例如对于平滑的函数<span
class="math inline"><em>f</em> : [0, 1]<sup><em>n</em></sup> → ℝ</span>：</p>
<p><span class="math display">$$\begin{align}
f(\mathbf{x})=f\left(x_1, \cdots, x_n\right)=\sum_{q=1}^{2 n+1}
\Phi_q\left(\sum_{p=1}^n \phi_{q, p}\left(x_p\right)\right)
\label{Kolmogorov-Arnold}
\end{align}$$</span></p>
<p>其中 <span
class="math inline"><em>ϕ</em><sub><em>q</em>, <em>p</em></sub> : [0, 1] → ℝ</span>
并且 <span
class="math inline"><em>Φ</em><sub><em>q</em></sub> : ℝ → ℝ</span>。但是不能简单的认为可以将任意的多元函数转化为1维函数函数相加，因为这样的1维函数可能是非平滑甚至是分形的。</p>
<h1 id="kan-architecture">KAN architecture</h1>
<figure>
<img src="./2-2.png" alt="B-spline" />
<figcaption aria-hidden="true">B-spline</figcaption>
</figure>
<p>如上图右边所示，训练目标是<span
class="math inline"><em>ϕ</em><sub><em>q</em>, <em>p</em></sub></span>，形如向基底展开，这里采用的基是
B-spline curve，可训练的参数是对应系数<span
class="math inline"><em>c</em></span>。左图为整体的流程，其中<span
class="math inline"><em>x</em><sub>0, 1</sub></span>与<span
class="math inline"><em>x</em><sub>0, 2</sub></span>为输入参数，输出参数为<span
class="math inline"><em>x</em><sub>2, 1</sub></span>。这样便构建了KAN的框架。</p>
<p><span class="math display">$$\begin{align}
&amp;x_{l+1, j}=\sum_{i=1}^{n_l} \tilde{x}_{l, j, i}=\sum_{i=1}^{n_l}
\phi_{l, j, i}\left(x_{l, i}\right), \quad j=1, \cdots, n_{l+1}
\label{KAN-layer}\\
&amp;\mathbf{x}_{l+1}=\underbrace{\left(\begin{array}{cccc}
\phi_{l, 1,1}(\cdot) &amp; \phi_{l, 1,2}(\cdot) &amp; \cdots &amp;
\phi_{l, 1, n_l}(\cdot) \\
\phi_{l, 2,1}(\cdot) &amp; \phi_{l, 2,2}(\cdot) &amp; \cdots &amp;
\phi_{l, 2, n_l}(\cdot) \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
\phi_{l, n_{l+1}, 1}(\cdot) &amp; \phi_{l, n_{l+1}, 2}(\cdot) &amp;
\cdots &amp; \phi_{l, n_{l+1}, n_l}(\cdot)
\end{array}\right)}_{\boldsymbol{\Phi}_l} \mathbf{x}_l \\
&amp;\operatorname{KAN}(\mathbf{x})=\left(\boldsymbol{\Phi}_{L-1} \circ
\boldsymbol{\Phi}_{L-2} \circ \cdots \circ \boldsymbol{\Phi}_1 \circ
\boldsymbol{\Phi}_0\right) \mathbf{x}
\end{align}$$</span></p>
<p>并且从 <span class="math inline">$\eqref{Kolmogorov-Arnold}$</span>
可知，用深度为2的架构已经满足，第一隐藏层将维度从<span
class="math inline"><em>d</em></span>提升到<span
class="math inline">2<em>d</em> + 1</span>，然后相加，再通过最后一层得出目标值。</p>
<p>上图中还展示了样条插值函数的拓展，更多的基更大的范围。</p>
<h2 id="implementation-details">Implementation details</h2>
<p>虽然 <span class="math inline">$\eqref{KAN-layer}$</span>
已经十分简单，但是还是有很多细节需要处理。</p>
<p><span class="math display">$$\begin{align}
&amp; \phi(x)=w(b(x)+\operatorname{spline}(x))  \\
&amp; b(x)=\operatorname{silu}(x)=x /\left(1+e^{-x}\right) \\
&amp;\operatorname{spline}(x)=\sum_i c_i B_i(x)
\end{align}$$</span></p>
<p>用 Xavier 初始化参数。</p>
<h1 id="a-toy-example-how-humans-can-interact-with-kans">A toy example:
how humans can interact with KANs</h1>
<p>对于给定的数据<span
class="math inline">(<em>x</em><sub><em>i</em></sub>, <em>y</em><sub><em>i</em></sub>, <em>f</em><sub><em>i</em></sub>), <em>i</em> = 1, 2…<em>N</em></span>，如何得出其具体形式？假设以上形式来源于公式：
<span
class="math display"><em>f</em>(<em>x</em>, <em>y</em>) = exp (sin (<em>π</em><em>x</em>) + <em>y</em><sup>2</sup>)</span></p>
<figure>
<img src="./2-4.png"
alt="An example of how to do symbolic regression with KAN" />
<figcaption aria-hidden="true">An example of how to do symbolic
regression with KAN</figcaption>
</figure>
<p>第一步，选取较大的网络进行离散化训练。第二步，减枝，截取大网络中的一部分，使得网络精简但是性能没有损失很多。第三步，用户自己猜测符号形式，或者利用给出的建议符号形式。
<span class="math display">$$\begin{equation}
\begin{aligned}
&amp; \text { fix_symbolic(0,0,0,'sin') } \\
&amp; \text { fix_symbolic(0,1,0,'x^2') } \\
&amp; \text { fix_symbolic(1,0,0,'exp'). }
\end{aligned}
\end{equation}$$</span> 第四步，进一步训练，对目标符号进行拟合。</p>
<p>后来在作者直播的分享中，这里面还有很多内容。虽然Kolmogorov-Arnold公式<span
class="math inline">$\eqref{Kolmogorov-Arnold}$</span>中知道只需要两层足够，但是实际情况可能要叠加更多的层数。这是由于简单公式的嵌套，例如<span
class="math inline">exp (<em>x</em>)</span>与<span
class="math inline"><em>x</em><sup>2</sup></span>用2层可以很好的表示，但是对于<span
class="math inline">exp (<em>e</em><sup>2</sup>)</span>在用三层，就相当于让样条函数分别拟合<span
class="math inline">exp (<em>x</em>)</span>与<span
class="math inline"><em>x</em><sup>2</sup></span>，相对容易；如果强制使用2层进行拟合，就会导致样条插值函数要拟合<span
class="math inline">exp (<em>e</em><sup>2</sup>)</span>，在可解释与拟合效果来说都比较差。</p>
<p>同时作者也解释了为什么这里其选用B样条插值函数，而非选用其它的函数。这是由于其<font color='green'>课题组经常使用B样条函数</font>。<font color='red'>这里有很多值得思考的问题，B-spline对于怎样的函数形式不能很好的拟合，替换其它的基是否效果会更好，能否将不同的基进行组合。</font></p>
<h1 id="conclusion">Conclusion</h1>
<p>回答开头提出的几个问题： 1.
KAN其实和MLP基本一致，差别在于KAN在过程中使用了展开的方式，学习的参数是一些非线性基的权重。这个和MLP是一致的，只是在MLP中参数对应的是线性的权重，然后通过更层完成对于复杂函数的拟合，换句话说，MLP在拟合拟合这些基。但是这样做有什么差别么？KAN的这种方式，更像是引入了人类经验，对于特定问题一些基底是更容易特征提取的，这样不仅能提升训练速度，同时还能够增强准确性。然而，代价就是如果选择的基并不适配这样的问题，训练慢、效果差。MLP的线性基更像是折中方案。
2. 是的，和开篇的猜测一致，人为假定一组基，然后展开。 3.
它比MLP开销少的原因在于两方面。一方面，Kolmogorov-Arnold Representation
theorem定理指出两层足够（如果目标函数较为复杂、嵌套较多，还是需要增加隐藏层），另一方面使用基可以将复杂的函数较为简单的表示。
4.
这个可解释性，就是规律的挖掘，很相之前AI-feynnman那种规则发现的程序。</p>
<h1 id="code">Code</h1>
<p><font color='red'>这里不由得感慨一下，原作者代码写的非常规范，可读性非常强。另一位厉害的人物，复现的也相当快<a
href="https://github.com/Zhangyanbo/FCN-KAN">FCN-KAN</a>。优秀的人不只是一个方面优秀啊。</font></p>
<h1 id="append">Append</h1>
<h2 id="b-splines">B-splines</h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/672199076">保姆级理解
B-Spline</a></li>
<li><a
href="https://juejin.cn/post/6844903666361565191">深入理解贝塞尔曲线</a></li>
<li></li>
</ul>
<p>B-splines函数在物理学中是不常见的，这个函数更多的用在计算机图形学上。在计算机图形学上，最早使用的是贝塞尔曲线。</p>
<p><img src="./Bezier_Curve.gif" /></p>
<p>上面是一个四阶贝塞尔曲线生成过程，通过固定的点（图中<span
class="math inline"><em>P</em><sub>0</sub><em>P</em><sub>1</sub><em>P</em><sub>2</sub><em>P</em><sub>3</sub></span>）连成线段然后按照长度比例生成图中绿线的三个端点，绿线同样按照比例，生成紫线的两个端点，紫色线段再按照比例生成红线上的点。初始的节点数越多，生成过程中产生的线段越多，图中共生成三种颜色的曲线（四个顶点），称为三阶贝塞尔曲线。</p>
<p>一阶贝塞尔曲线，由两个端点，直接生成最终的曲线： <span
class="math display">$$\begin{align}
B_1(t)=&amp;P_0+\left(P_1-P_0\right) t \\
B_1(t)=&amp;(1-t) P_0+t P_1 \quad t \in[0,1]
\end{align}$$</span></p>
<p>二阶贝塞尔曲线，由三个端点，首先生成一个线段，然后生成最终的曲线。线段上的控制点为<span
class="math inline"><em>P</em><sub>0</sub><sup>′</sup><em>P</em><sub>1</sub><sup>′</sup></span>。</p>
<p><span class="math display">$$\begin{align}
&amp;\begin{aligned}
&amp; P_0^{\prime}=(1-t) P_0+t P_1 \\
&amp; P_1^{\prime}=(1-t) P_1+t P_2
\end{aligned} \\
&amp;\begin{aligned}
B_2(t)&amp;=(1-t) P_0^{\prime}+t P_1^{\prime} \\
&amp; =(1-t)\left((1-t) P_0+t P_1\right)+t\left((1-t) P_1+t P_2\right)
\\
&amp; =(1-t)^2 P_0+2 t(1-t) P_1+t^2 P_2 \quad t \in[0,1]
\end{aligned}
\end{align}$$</span></p>
<p>直接给出三阶贝塞尔曲线表达式： <span
class="math display">$$\begin{align}
B_3(t)=(1-t)^3 P_0+3 t(1-t)^2 P_1+3 t^2(1-t) P_2+t^3 P_3, \quad t
\in[0,1]
\end{align}$$</span></p>
<p>多阶贝塞尔曲线： <span class="math display">$$\begin{align}
B(t)&amp;=\sum_{i=0}^n C_n^i P_i(1-t)^{n-i} t^i \\
&amp;=\sum_{i=0}^n P_i b_{i, n}(t), \quad t \in[0,1] \\
C_n^i &amp;= \frac{n!}{(n-i)!\cdot i!} \\
b_{i, n}(t) &amp;= C_n^i(1-t)^{n-i} t^i \quad i=0,1, \ldots, n \quad t
\in[0,1] \label{Bezier}
\end{align}$$</span></p>
<p>通过点来生成整个曲线，这就是贝塞尔曲线的主要思想。但是其存在一些问题：
* 给定点的数量，就确定了曲线的阶次 *
Bezier曲线拼接复杂（需要满足几何连续性，参数连续性等） *
Bezier曲线不能作局部修改（只能整体修改）</p>
<p>因此，提出了B-spline曲线。核心思想是修改<span
class="math inline">$\eqref{Bezier}$</span>中的<span
class="math inline"><em>b</em><sub><em>i</em>, <em>n</em></sub>(<em>t</em>)</span>。根据之前的定义可以知道其中<span
class="math inline"><em>n</em></span>是阶数为固定值，这里将其变为一个自由可以选择的值，对应的物理含义为：将固定点进行截断，只考虑几个特定的点的贝塞尔函数（即画出部分点的贝塞尔函数）。另一个改变点在于<span
class="math inline"><em>t</em></span>，在原来的定义中这是一个遍历线段的点，这里也进行截断让其变为一个自由的值<span
class="math inline"><em>t</em><sub>min</sub> ≤ <em>t</em> ≤ <em>t</em><sub>max</sub></span>，让其可以选择一部分。</p>
<p>其实B-spline和贝塞尔同宗同源，就是在其基础上进行两种截断，一个是遍历线段的范围，另一个是选取的点数量。而且，这和选取的点没有关系，仅仅是点前面的系数进行变换。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Kolmogorov–Arnold Networks</tag>
        <tag>Physics</tag>
      </tags>
  </entry>
  <entry>
    <title>Injectivity of ReLU networks (perspectives from statistical physics)</title>
    <url>/2024/08/25/DL/Injectivity_ReLU/InjecReLU/</url>
    <content><![CDATA[<p>对于ReLU激活层单向性的分析，得到变化的上下界。</p>
<p>Link: * <a href="https://arxiv.org/abs/2302.14112">Injectivity of
ReLU networks: perspectives from statistical physics</a></p>
<span id="more"></span>
<h1 id="introduction">Introduction</h1>
<p>在什么情况下，随机初始化的ReLU网络是单射性的？</p>
<p>考虑一个单层的 ReLU 函数，这个映射为<span
class="math inline"><em>φ</em><sub><strong>W</strong></sub></span>：</p>
<p><span class="math display">$$
\varphi_{\mathbf{W}}(\mathbf{x})_\mu=\sigma\left[\left(\frac{\mathbf{W}
\mathbf{x}}{\sqrt{n}}\right)_\mu\right], \quad \mu=1, \cdots, m
$$</span></p>
<p>其中 <span class="math inline"><em>n</em>, <em>m</em> ≥ 1</span>
，<span
class="math inline"><strong>x</strong> ∈ ℝ<sup><em>n</em></sup></span>
，<span
class="math inline"><em>σ</em>(<em>x</em>) := max (0, <em>x</em>)</span>
，ReLU参数满足正态分布 <span class="math inline">$W_{\mu i}
\stackrel{\text { i.i.d. }}{\sim} \mathcal{N}(0,1)$</span> 。</p>
<p>已有研究指出在热力学极限下 <span
class="math inline"><em>n</em> → ∞</span> 、 <span
class="math inline">$\frac{m}{n} \rightarrow \alpha&gt;0$</span>
，存在两个阈值 <span
class="math inline"><em>α</em><sub><em>l</em></sub> &lt; <em>α</em><sub><em>h</em></sub></span>
。当<span
class="math inline"><em>α</em> &lt; <em>α</em><sub><em>l</em></sub></span>，ReLU函数是非单射性的；当<span
class="math inline"><em>α</em> &gt; <em>α</em><sub><em>h</em></sub></span>，ReLU是单射性的。</p>
<p>这篇文章的研究内容与之前的一致，采用统计物理方法（复本对称）。</p>
<h1 id="solution">Solution</h1>
<p>研究思路是通过将单射性问题，通过一个能量模型描述，然后这个能量模型在波尔兹曼分布下研究。这样该问题就转为一个物理问题。</p>
<h2 id="injectivity">Injectivity</h2>
<p>首先需要解决如何描述单射性。提出概率 <span
class="math inline"><em>p</em><sub><em>m</em>, <em>n</em></sub></span>
用于表示映射<span
class="math inline"><em>φ</em><sub><strong>W</strong></sub></span>
是单射性的概率：</p>
<p><span
class="math display"><em>p</em><sub><em>m</em>, <em>n</em></sub> = ℙ<sub><em>V</em></sub>[<em>V</em> ∩ <em>C</em><sub><em>m</em>, <em>n</em></sub> = {0}]</span></p>
<p>其中 <span class="math inline"><em>V</em></span> 是 <span
class="math inline">ℝ<sup><em>m</em></sup></span> 的一个随即子空间，
<span
class="math inline"><em>C</em><sub><em>m</em>, <em>n</em></sub></span>
是 <span class="math inline">ℝ<sup><em>m</em></sup></span>
中一组向量，并且这组向量中每一个向量元素为正的个数要小于<span
class="math inline"><em>n</em></span> 。</p>
<p>通过这个操作，将描述单射性的问题，转化为数向量中为正的元素个数，可以定量描述了。</p>
<h2 id="statistical-physics-and-the-spherical-perceptron">Statistical
physics and the spherical perceptron</h2>
<p>接下来的任务就是通过设计能量函数，将数正数的个数，变成为能量的表述形式。</p>
<p>通过能量表示总的正元素个数： <span class="math display">$$
E_{\mathbf{W}}(\mathbf{x}):=\sum_{\mu=1}^m \theta\left[(\mathbf{W}
\mathbf{x})_\mu\right], \quad
e_{\mathbf{W}}(\mathbf{x}):=\frac{E_{\mathbf{W}}(\mathbf{x})}{n}
$$</span> 其中 <span
class="math inline"><em>θ</em>(<em>x</em>) = 𝟙(<em>x</em> &gt; 0)</span>
， <span
class="math inline"><strong>x</strong> ∈ 𝒮<sup><em>n</em> − 1</sup></span>
， <span class="math inline">𝒮<sup><em>n</em> − 1</sup></span> 是 <span
class="math inline">ℝ<sup><em>n</em></sup></span>
上的单位球。根据之前的讨论<span
class="math inline"><em>V</em> ∩ <em>C</em><sub><em>m</em>, <em>n</em></sub> = {0}</span>可以得到
<span
class="math inline"><strong>W</strong><strong>x</strong> ∈ <em>C</em><sub><em>m</em>, <em>n</em></sub> ⇔ <em>E</em><sub><strong>W</strong></sub>(<strong>x</strong>) &lt; <em>n</em></span>，将
<span
class="math inline"><em>p</em><sub><em>m</em>, <em>n</em></sub></span>重新写为：</p>
<p><span
class="math display"><em>p</em><sub><em>m</em>, <em>n</em></sub> = ℙ<sub><strong>W</strong></sub>[min<sub><strong>x</strong> ∈ 𝒮<sup><em>n</em> − 1</sup></sub><em>E</em><sub><strong>W</strong></sub>(<strong>x</strong>) ≥ <em>n</em>]</span></p>
<h2 id="thermal-relaxation-the-gibbsboltzmann-distribution">Thermal
relaxation: the Gibbs–Boltzmann distribution</h2>
<p>有了能量，接下来将其写为波尔兹曼分布：</p>
<p><span class="math display">$$
\mathrm{d} \mathbb{P}_{\beta,
\mathbf{W}}(\mathbf{x}):=\frac{1}{\mathcal{Z}_n(\mathbf{W}, \beta)}
e^{-\beta E_{\mathbf{W}}(\mathbf{x})} \mu_n(\mathrm{~d} \mathbf{x}) .
\quad\left(\mathbf{x} \in \mathcal{S}^{n-1}\right)
$$</span></p>
<p>其中<span
class="math inline"><em>b</em><em>e</em><em>t</em><em>a</em></span>为逆温度，<span
class="math inline"><em>β</em> = 0</span> 那就是球面上的平均测量， <span
class="math inline"><em>β</em> → ∞</span>则是能量最小值部分。</p>
<p>同时写出其自由能： <span class="math display">$$
\Phi_n(\mathbf{W}, \beta):=\frac{1}{n} \log \mathcal{Z}_n(\mathbf{W},
\beta)=\frac{1}{n} \log \int_{\mathcal{S}^{n-1}} \mu_n(\mathrm{~d}
\mathbf{x}) e^{-\beta E_{\mathbf{W}}(\mathbf{x})}
$$</span></p>
<h1 id="result">Result</h1>
<p>这部分就是通过副本对称破缺讨论了。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
      </tags>
  </entry>
  <entry>
    <title>Loss of plasticity in deep continual learning</title>
    <url>/2024/08/26/DL/LossPlasticity/LossPlasticity/</url>
    <content><![CDATA[<p>文章主要讨论在持续学习任务中，损失具有弹性是很关键，进而指出在深度学习中，仅仅依靠反向传播是不够的，需要结合随机、非梯度的优化方式（例如演化计算等）。</p>
<p>Link: * <a href="https://doi.org/10.1038/s41586-024-07711-7">Loss of
plasticity in deep continual learning</a> * <a
href="https://mp.weixin.qq.com/s?__biz=MzkxODQ0MTQzMg==&amp;mid=2247489568&amp;idx=1&amp;sn=606d85bf9c0df3780f4dc3ccd5835c8d&amp;chksm=c0a474736003b8a50446944aee862eb1417da7025d4892f919e940496e644a24d700688cdb9d&amp;mpshare=1&amp;scene=1&amp;srcid=0825em2QJV5witmRbRpL25ZX&amp;sharer_shareinfo=58591a65fb8a82b6749c7ee6829e99a3&amp;sharer_shareinfo_first=58591a65fb8a82b6749c7ee6829e99a3#rd">Nature正刊（演化深度持续学习）Loss
of plasticity in deep continual learning</a></p>
<span id="more"></span>
<h1 id="introduction">Introduction</h1>
<p>讨论的问题是持续学习任务，以分类任务为例，首先将几个目标进行分类，然后将成功的分类器（神经网络），然后再添加一些其它的分类目标与训练样本。核心的概念损失弹性由此产生，可以发现随着目标的增加，反向传播训练分类器会降低其在测试集上的成功率，说明这样的网络是不具有弹性的，不能进行持续学习。</p>
<figure>
<img src="./F1.png" alt="Plasticity loss in Continual ImageNet" />
<figcaption aria-hidden="true">Plasticity loss in Continual
ImageNet</figcaption>
</figure>
<p>上图是以ImageNet进行的测试，a表示逐渐增加的任务，b是不同步长、不同任务数下的比较，c中包含了其它的改良。</p>
<p>不具有弹性的原因在于激活函数的失效，意味着这些神经元失去单射性，对结果是没有作用的。</p>
<figure>
<img src="./EDF3.png" alt="Online Permuted MNIST" />
<figcaption aria-hidden="true">Online Permuted MNIST</figcaption>
</figure>
<p>上图中c展示了随着任务的数量增加，失活的神经元数量增加（ReLU激活函数）。</p>
<p>为了解决反向传播中没有弹性问题，文章提出了一种可持续反向传播的算法continual
backpropagation。</p>
<h1 id="continual-backpropagation">Continual backpropagation</h1>
<p>Continual
backpropagation的核心思想就是将一些失活的激活函数进行初始化。如何衡量是否有效，提出了效用函数<span
class="math inline"><strong>u</strong><sub><em>l</em></sub>[<em>i</em>]</span>，如果激活函数处于低效用的状况，这对其进行初始化。</p>
<p><span class="math display">$$
\mathbf{u}_l[i]=\eta \times \mathbf{u}_l[i]+(1-\eta)
\times\left|\mathbf{h}_{l, i, t}\right| \times \sum_{k=1}^{n_{l+1}} \mid
\mathbf{w}_{l, i, k, t}
$$</span></p>
<p>其中<span
class="math inline"><strong>u</strong><sub><em>l</em></sub>[<em>i</em>]</span>是对各种训练数据的积累量，<span
class="math inline"><em>h</em><sub><em>l</em>, <em>i</em>, <em>t</em></sub></span>是第l层第i个隐藏神经元第t次的输出，<span
class="math inline"><strong>w</strong><sub><em>l</em>, <em>i</em>, <em>k</em>, <em>t</em></sub></span>是权重参数，<span
class="math inline"><em>η</em></span>是衰减率。</p>
<p>对每一个神经元的效用函数从大到小进行排序，同时设置超参效用阈值<span
class="math inline"><em>m</em></span>。在每一次反向传播结束，统计超过效用阈值<span
class="math inline"><em>m</em></span>神经元数量<span
class="math inline"><em>n</em><sub>eligible </sub></span>，以比例<span
class="math inline"><em>ρ</em></span>进行累计<span
class="math inline"><em>c</em><sub><em>l</em></sub> = <em>c</em><sub><em>l</em></sub> + <em>n</em><sub>eligible
</sub> × <em>ρ</em></span>。如果超过1，那么选取一个分界<span
class="math inline"><em>r</em></span>，高效用的按照一个分布进行初始化，低效用置0。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Evolution</tag>
      </tags>
  </entry>
  <entry>
    <title>ViT and ViViT</title>
    <url>/2024/02/29/DL/ViT_ViViT/ViT_ViViT/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>将 Transformer
架构加入视觉领域，ViT与ViViT是分别是将该架构加入图片分类与视频分类领域，是该方向的两篇代表作。</p>
<ul>
<li>ViT: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE
RECOGNITION AT SCALE</li>
<li>ViViT: ViViT: A Video Vision Transformer</li>
</ul>
<span id="more"></span>
<h1 id="vision-transformervit">Vision Transformer(ViT)</h1>
<p>ViT尽可能的遵循原始<a
href="https://arxiv.org/abs/1706.03762">Transformer</a>。</p>
<figure>
<img src="./ViT.png" alt="ViT structure" />
<figcaption aria-hidden="true">ViT structure</figcaption>
</figure>
<p>标准的 Transformer
的输入是一维序列，因此首先需要将二维的图像整形为一维的序列。将其作为整体，称作patch，输入到Transformer进行
embedding。</p>
<p>位置编码直接相加进path embedding。</p>
<p>Trandformer Encoder架构为：</p>
<figure>
<img src="./transformer_encoder.png" alt="Trandformer Encoder" />
<figcaption aria-hidden="true">Trandformer Encoder</figcaption>
</figure>
<p>数学公式总结为： <span class="math display">$$\begin{align}
z_0 &amp;= [x_\text{class}; x_p^1 E;x_p^2 E;\cdots ;x_p^N
E]+E_\text{pos}\quad E\in \mathcal R^{(p^2\cdot C)\times D},E_{pos}\in
\mathcal R^{(N+1)\times D} \\
z'_l &amp;= MSA(LN(z_{l-1}))+z_{l-1} \quad l=1\cdots L \\
z_l &amp;= MLP(LN(z'_l))+z'_l \quad l=1\cdots L \\
y &amp;= LN(z_L^0)
\end{align}$$</span></p>
<p>其中<span class="math inline"><em>C</em></span>是通道数，<span
class="math inline"><em>D</em></span>是样本数，<span
class="math inline"><em>p</em><sup>2</sup></span>是分块之后的数量，MSA是multiheaded
selfattention，MLP是多层感知机，LN是 Layernorm，并且<span
class="math inline"><em>z</em><sub>0</sub><sup>0</sup> = <em>x</em><sub>class</sub></span></p>
<h1 id="video-vision-transformervivit">Video Vision
Transformer(ViViT)</h1>
<p>基于图像，进一步整合时空的Transformer结构。能够对视频进行有效的处理。</p>
<figure>
<img src="./ViViT.png" alt="ViViT" />
<figcaption aria-hidden="true">ViViT</figcaption>
</figure>
<p>除了ViT提出的patch embedding，在处理时空信息上本文提出了Tubelet
embedding。</p>
<figure>
<img src="./tubelet_embedding.png" alt="Tubelet embedding" />
<figcaption aria-hidden="true">Tubelet embedding</figcaption>
</figure>
<p>其在时间信息上同样进行编码，组成一个包含部分时间与部分空间信息的embedding。这样就成功让Transformer可以输入时间与空间信息，接下来需要处理的是patch
embedding 与 tubelet embedding 两部分信息如何输入进
transformer。虽然其同为编码信息，但是时间和空间的信息如何有效的耦合就是新的问题。这篇文章提出了四种架构。</p>
<h2 id="spation-temporal-attendtion">Spation-temporal attendtion</h2>
<p>直接认为两者地位等价，作为相同的token进行输入。显然则会导致输入的信息过于庞大，这是最吃资源的一种。</p>
<h2 id="factorised-encoder">Factorised encoder</h2>
<figure>
<img src="./factorised_encoder.png" alt="Factorised" />
<figcaption aria-hidden="true">Factorised</figcaption>
</figure>
<p>这种方案包含两层encoder，第一层将在同一个tubelet 附近的
patch进行encoder，然后下一步将其在时间上进行encoder。</p>
<h2 id="factorised-self-attention">Factorised self-attention</h2>
<figure>
<img src="./Factorised_self-attention.png"
alt="Factorised self-attention" />
<figcaption aria-hidden="true">Factorised self-attention</figcaption>
</figure>
<p>与第一种方案一样，同样认为等价的地位，但只进行self-attention操作。首先先处理空间，然后是时间。</p>
<h2 id="factorised-dot-product-attention">Factorised dot-product
attention</h2>
<figure>
<img src="./Factorised%20dot-product%20attention.png"
alt="Factorised dot-product attention" />
<figcaption aria-hidden="true">Factorised dot-product
attention</figcaption>
</figure>
<p>分别处理时间与空间上的embedding，然后拼接再作为一个整体。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>ViT</tag>
        <tag>ViViT</tag>
        <tag>Transform</tag>
      </tags>
  </entry>
  <entry>
    <title>Score matching model</title>
    <url>/2024/01/19/DL/Score_matching_model/Score_matching_model/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>这篇文章主要描述基于得分匹配（Score matching
model）的想法，以及之后主要的修改思路。这种思路是生成模型的一种，与GAN、normal-flow等模型具备同样的功能。</p>
<p>本篇文章大量借鉴<a
href="https://bobondemon.github.io/2022/01/08/Estimation-of-Non-Normalized-Statistical-Models-by-Score-Matching/">棒棒生博客</a>，推荐阅读原文博客。本文章在其基础上加入一些作者本人的思考，并且统一符号，增加阅读流畅性。</p>
<span id="more"></span>
<h1 id="得分匹配算法score-matching-model">得分匹配算法（Score matching
model）</h1>
<p>得分匹配这种思想首先发表于论文–Estimation of Non-Normalized
Statistical Models by Score Matching。</p>
<h2 id="拟合目标">拟合目标</h2>
<p>图像、声音、热力学过程等在数学形式上应该是一种概率分布，想要模仿生成相应的事物需要的就是能够得知这种概率分布。但是这种概率分布往往在一个高维空间，难以凭借人类的经验直接获得，因此此时借助神经网络的拟合能力，完成概率密度分布函数的拟合。</p>
<p>利用 <span class="math inline">$\bf{x}$</span>
生成一个随机的目标概率密度分布 <span
class="math inline">$p_{\bf{x}}(\cdot)$</span>，接下来尝试拟合该目标函数，拟合函数的概率密度分布记为<span
class="math inline"><em>p</em>(⋅; <em>θ</em>)</span>，其中<span
class="math inline"><em>θ</em></span>是一个m维空间的向量。目标是从<span
class="math inline">$\bf{x}$</span>中估计<span
class="math inline"><em>θ</em></span>。</p>
<p>将拟合函数<span
class="math inline"><em>p</em>(⋅; <em>θ</em>)</span>进一步分解开来：
<span
class="math display">$$p(\xi;\theta)=\frac{1}{Z(\theta)}q(\xi;\theta)$$</span>
其中：<span
class="math inline"><em>Z</em>(<em>θ</em>) = ∫<sub><em>ξ</em> ∈ ℝ<sup><em>n</em></sup></sub><em>q</em>(<em>ξ</em>; <em>θ</em>)d<em>ξ</em></span>。这样只需要生成函数<span
class="math inline"><em>q</em>(⋅; <em>θ</em>)</span>，不需要关心归一化的问题。</p>
<p>通常非归一化采样的方法是马尔科夫链蒙特卡洛模拟。典型的是Ising模型的模拟过程。在得到一系列数据<span
class="math inline">{<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub>...}</span>后，使用极大似然估计方法（Maximum
Likelihood Estimation, MLE）估计<span
class="math inline"><em>θ</em></span>： <span
class="math display">$$\theta_{MLE}=\text{argmax}_{\theta}\sum_{t=1}^T
\text{ln} p(x_t;\theta)$$</span></p>
<p>然而在实际中这种方案是行不通的，因为配分函数<span
class="math inline"><em>Z</em>(<em>θ</em>)</span>难以计算。计算配分函数是一块难啃的骨头，许多物理学上的困难本质就是来源于如何高效计算配分函数，因此很多方法例如高温展开、信念传播算法等本质就是在解决如何在牺牲一些精确性的条件下，高效计算配分函数。</p>
<p>直接计算存在困难，并且配分函数作为一个仅仅为<span
class="math inline"><em>θ</em></span>的函数存在，那就转而计算梯度，试图将配分函数在求梯度的过程中磨消。定义：</p>
<p><span class="math display">$$
\psi(\xi;\theta)=
    \left(
        \begin{matrix}
            \frac{\partial ln p(\xi;\theta)}{\partial \xi_1} \\
            \cdots \\
            \frac{\partial ln p(\xi;\theta)}{\partial \xi_n}
        \end{matrix}
    \right)
=
    \left(
        \begin{matrix}
            \psi_1 (\xi;\theta) \\
            \cdots \\
            \psi_n (\xi;\theta)
        \end{matrix}
    \right)
=\nabla_\xi ln p(\xi;\theta)
$$</span></p>
<p>本质上求偏导并不依赖于<span
class="math inline"><em>Z</em>(<em>θ</em>)</span>，可以获得： <span
class="math display"><em>ψ</em>(<em>ξ</em>; <em>θ</em>) = ∇<sub><em>ξ</em></sub><em>l</em><em>n</em><em>q</em>(<em>ξ</em>; <em>θ</em>)</span></p>
<figure>
<img src="./gradient.png" title="考虑一个分布的梯度图像"
alt="梯度图片" />
<figcaption aria-hidden="true">梯度图片</figcaption>
</figure>
<p>上方是一个分布的梯度图像，可以看出如果梯度方向大小均相同的情况下，两个应该是同一种分布。这里可以通过分析下面的函数得到，如果<span
class="math inline"><em>J</em>(<em>θ</em>) = 0</span>,由于<span
class="math inline"><em>p</em><sub><em>x</em></sub>(<em>ξ</em>) ≥ 0</span>，必然存在<span
class="math inline"><em>ψ</em>(<em>ξ</em>; <em>θ</em>) − <em>ψ</em><sub><em>x</em></sub>(<em>ξ</em>) ≡ 0</span>，则说明<span
class="math inline"><em>θ</em> = <em>θ</em><sup>*</sup></span>，成功学习到分布。这也证明了解的存在唯一性。</p>
<h2
id="直观的目标函数explicit-score-matchingesm">直观的目标函数（Explicit
Score Matching，ESM）</h2>
<p>将任务目标函数写为考虑两个函数的梯度的均方误差(MSE)： <span
class="math display">$$\begin{equation} J_{ESM}(\theta)=\frac{1}{2}
\int_{\xi\in\mathbb{R}^n}p_x(\xi)||\psi(\xi;\theta)-\psi_x(\xi)||^2\mathrm{d}\xi
\end{equation}$$</span> 其中<span
class="math inline"><em>ψ</em><sub><em>x</em></sub>(<em>ξ</em>) = ∇<sub><em>ξ</em></sub><em>l</em><em>n</em><em>p</em><sub><em>x</em></sub>(<em>ξ</em>)</span>，为目标概率密度分布的梯度分布。优化目标为：
<span
class="math display"><em>θ̂</em> = argmax<sub><em>θ</em></sub><em>J</em>(<em>θ</em>)</span></p>
<p>可以通过重要性采样得到<span
class="math inline"><em>T</em></span>个数据点<span
class="math inline">$\bf{x}(1), \bf{x}(2)
\cdots\bf{x}(T)$</span>，从而计算经验期望值： <span
class="math display">$$\begin{equation}
\tilde{J}_{ESM}(\theta)=\frac{1}{T}\sum_{t=1}^T\sum_{i=1}^n[\psi(\xi;\theta)-\psi_x(\xi)]^2
\end{equation}$$</span></p>
<h2
id="隐藏的目标函数implicit-score-matchingism">隐藏的目标函数（Implicit
Score Matching，ISM）</h2>
<p>在计算均方误差的时候，需要知道<span
class="math inline"><em>ψ</em><sub><em>x</em></sub>(<em>ξ</em>)</span>，然而知道确切的目标函数形式是不可能的。因此，虽然以上逻辑是合理的，但是本质上是不计算实施的，需要进行一定的调整。
<span class="math display">$$
\begin{align*}
J(\theta)&amp;=\frac{1}{2}
\int_{\xi\in\mathbb{R}^n}p_x(\xi)||\psi(\xi;\theta)-\psi_x(\xi)||^2\mathrm{d}\xi
\\
&amp;=\int_{\xi\in\mathbb{R}^n}p_x(\xi)[\frac{1}{2}
\psi(\xi;\theta)^2+\frac{1}{2}
\psi_x(\xi)^2-\psi(\xi;\theta)\psi_x(\xi)]\mathrm{d}\xi \\
\end{align*}
$$</span></p>
<p>针对后一部分结果进行简化，将<span
class="math inline"><em>ψ</em></span>展开为具体的第<span
class="math inline"><em>i</em></span>部分： <span
class="math display">$$
\begin{align*}
&amp;\int_{\xi\in\mathbb{R}^n}-p_x(\xi)\psi(\xi;\theta)\psi_x(\xi)\mathrm{d}\xi
\\
&amp;=
-\sum_i\int_{\xi\in\mathbb{R}^n}p_x(\xi)\psi_i(\xi;\theta)\psi_{x,i}(\xi)\mathrm{d}\xi
\\
&amp;=-\sum_i\int_{\xi\in\mathbb{R}^n}p_x(\xi)\psi_i(\xi;\theta)\frac{\partial\psi_{x}(\xi)}{\partial
\xi_i} \mathrm{d}\xi \\
&amp;=-\sum_i\int_{\xi\in\mathbb{R}^n}p_x(\xi)\psi_i(\xi;\theta)\frac{\partial
ln p_x(\xi)}{\partial \xi_i} \mathrm{d}\xi \\
&amp;=-\sum_i\int_{\xi\in\mathbb{R}^n}\psi_i(\xi;\theta)\frac{\partial
p_x(\xi)}{\partial \xi_i} \mathrm{d}\xi \\
&amp;=-\sum_i\left[ \psi_i(\xi;\theta)p_x(\xi)
|_{\xi_i=-\infty}^{\xi_i=\infty} - \int_{\xi\in\mathbb{R}^n}
p_x(\xi)\frac{\partial \psi_i(\xi;\theta)}{\partial \xi_i} \mathrm{d}\xi
\right]\\
\end{align*}
$$</span></p>
<p>将其中第一部分进行更详细的展开： <span class="math display">$$
\begin{align*}
\psi_1(\xi;\theta)p_x(\xi) |_{\xi_1=-\infty}^{\xi_1=\infty}
&amp;=  \lim\limits_{a\to
\infty,b\to\infty}[\psi(a,\xi_2\cdots\xi_n;\theta)p_x(a,\xi_2\cdots\xi_n;\theta)-\psi(b,\xi_2\cdots\xi_n;\theta)p_x(b,\xi_2\cdots\xi_n;\theta)]
\\
&amp;= 0
\end{align*}
$$</span> 后面的原因来源于一个简单的假设，当<span
class="math inline">||<em>ξ</em>|| → ∞</span>： <span
class="math display"><em>p</em><sub><em>x</em></sub>(<em>ξ</em>)<em>ψ</em>(<em>ξ</em>; <em>θ</em>) → 0</span></p>
<p>可以这样理解该假设，当采样数量巨大或者分布为连续，单一采样所占概率分布很小，可以视为直接为0。因此得到最后的结果：
<span class="math display">$$
\begin{align*}
\int_{\xi\in\mathbb{R}^n}-p_x(\xi)\psi(\xi;\theta)\psi_x(\xi)\mathrm{d}\xi
&amp;=\sum_i \int_{\xi\in\mathbb{R}^n} p_x(\xi)\frac{\partial
\psi_i(\xi;\theta)}{\partial \xi_i} \mathrm{d}\xi\\
&amp;=\int_{\xi\in\mathbb{R}^n} p_x(\xi)\partial_{i} \psi_i(\xi;\theta)
\mathrm{d}\xi\\
\end{align*}
$$</span></p>
<p>$$ $$</p>
<p>其中第二项为目标函数的积分，结果应该为一个常数，对于优化目标函数来说一个常数并没有实际意义，因此可以直接消去：
<span class="math display">$$
\begin{align}
J_{ISM}(\theta) &amp;= \int_{\xi\in\mathbb{R}^n}p_x(\xi)[\frac{1}{2}
\psi(\xi;\theta)^2+\partial_{i} \psi_i(\xi;\theta)]\mathrm{d}\xi \\
&amp;= \mathbb{E}_{p_x(\xi)}[\frac{1}{2}
\psi(\xi;\theta)^2+\text{tr}(\nabla_{\xi} \psi_{\xi}(\xi;\theta))] \\
\tilde{J}_{ISM}(\theta) &amp;=
\frac{1}{T}\sum_{t=1}^T\sum_{i=1}^n[\frac{1}{2}
\psi(x_t;\theta)^2+\partial_{i} \psi_i(x_t;\theta)]
\end{align}
$$</span></p>
<h2 id="总结">总结</h2>
<p>首先通过概率密度分布拟合明确了任务目标，通过调节参数<span
class="math inline"><em>θ</em></span>使得<span
class="math inline">$p_{\bf X}(\cdot)$</span>与<span
class="math inline"><em>p</em>(⋅; <em>θ</em>)</span>分布尽可能相同。但是直接计算<span
class="math inline"><em>p</em>(⋅; <em>θ</em>)</span>存在困难，因此将任务目标进行改变，转而寻<strong>求概率分布密度函数的梯度</strong>尽可能相似。然后求解公式中包含目标函数，无法直接求解，通过分布积分的方法转化为只需要拟合函数便可以求解。</p>
<p>然而在计算ISM的时候存在两点问题： *
需要计算二次导数，这会导致计算图大小升高 *
当输入数据维度很大的时候很难处理</p>
<h1 id="噪声得分匹配denoising-score-matchingdsm">噪声得分匹配（Denoising
Score Matching，DSM）</h1>
<p>本算法的思想来源于A Connection Between Score Matching and Denoising
Autoencoders。</p>
<p>DSM的提出，可以很好的解决ISM存在的两个问题。</p>
<h2 id="噪声">噪声</h2>
<p>将目标函数加入噪声 <span
class="math inline"><em>p</em><sub><em>x</em><sub><em>σ</em></sub></sub></span>，其中<span
class="math inline"><em>σ</em></span>表示噪声，那么ESM与ISM可以改写为：</p>
<p><span class="math display">$$
\begin{align}
J_{ESM,p_{\sigma}}(\theta) &amp;=\mathbb{E}_{p_\sigma(\xi)}\left[
\frac{1}{2} ||\psi(\xi;\theta)-\psi_{x_\sigma}(\xi)||^2\right] \\
&amp;= \mathbb{E}_{p_\sigma(\xi)}\left[ \frac{1}{2}
||\psi(\xi;\theta)-\frac{\partial ln p_{\sigma}(\xi)}{\partial
\xi}||^2\right] \\
J_{ISM,p_\sigma}(\theta) &amp;= \mathbb{E}_{p_\sigma(\xi)}\left[
\frac{1}{2} \psi(\xi;\theta)^2+\text{tr}(\nabla_{\xi} \psi
(\xi;\theta))\right] \\
\end{align}
$$</span></p>
<p>这两个式子在变化前后，形式基本一样。区别的地方那个在于学习的目标函数区别，后者学习的目标函数是包含噪音之后的目标函数。而且可以看出，在变化之后，同样计算二阶导数，计算难度依然没有发生改变。</p>
<p>引入联合概率密度分布<span class="math inline">$p_\sigma(\tilde{\bf
x}, {\bf x})=p_\sigma(\tilde{\bf x}|{\bf x})p_0({\bf
x})$</span>，其中<span
class="math inline"><em>x̃</em></span>是在准确分布<span
class="math inline"><em>x</em></span>基础上的随机取值，定义噪音得分匹配(DSM)：</p>
<p><span class="math display">$$
J_{DSM,p_{\sigma}}(\theta)=\mathbb{E}_{p_\sigma(\tilde x,x)}\left[
\frac{1}{2} ||\psi(\tilde x;\theta)-\frac{\partial ln p_{\sigma}(\tilde
x|x)}{\partial \tilde x}||^2\right]
$$</span></p>
<p>假设为高斯核函数关系<span class="math inline">$p_\sigma(\tilde
x|x)=e^{-\frac{||x-\tilde x||^2}{2\sigma^2}}$</span>：</p>
<p><span class="math display">$$
\frac{\partial p_{\sigma}(\tilde x|x)}{\partial \tilde x} =
\frac{x-\tilde x}{\sigma^2}
$$</span></p>
<p>因此： <span class="math display">$$
\begin{equation}
J_{DSM,p_{\sigma}}(\theta)=\mathbb{E}_{p_\sigma(\tilde x,x)}\left[
\frac{1}{2} ||\psi(\tilde x;\theta)-\frac{x-\tilde
x}{\sigma^2}||^2\right]
\end{equation}
$$</span></p>
<p>可以发现，在通过引入噪声函数之后，成功避免了二次导数的计算。然而DSM的引入是直接定义，并没有证明ESM的等价性。</p>
<h2 id="等价性证明">等价性证明</h2>
<p><span class="math display">$$
\begin{align*}
J_{ESM,p_{\sigma}}(\theta) &amp;= \mathbb{E}_{p_\sigma(\tilde x)}\left[
\frac{1}{2} ||\psi(\tilde x;\theta)-\frac{\partial ln p_{\sigma}(\tilde
x)}{\partial \tilde x}||^2\right] \\
&amp;= \mathbb{E}_{p_\sigma(\tilde x)}\left[ \frac{1}{2} ||\psi(\tilde
x;\theta)||^2\right]-S(\theta)+C_2\\
\end{align*}
$$</span></p>
<p>其中<span class="math inline">$C_2=\mathbb{E}_{p_\sigma(\tilde
x)}\left[ \frac{1}{2}||\frac{\partial ln p_{\sigma}(\tilde x)}{\partial
\tilde x}||^2 \right]$</span>，并不依赖于参数<span
class="math inline"><em>θ</em></span>，可以直接忽略。</p>
<p><span class="math display">$$
\begin{align*}
S(\theta) &amp;= \mathbb{E}_{p_\sigma(\tilde x)} \left[
&lt;\psi(\tilde{\bf x};\theta),\frac{\partial lnp_\sigma(\tilde{\bf
x})}{\partial \tilde{\bf x}}&gt; \right] \\
&amp;= \int_{\tilde{\bf x}} p_\sigma(\tilde{\bf x})&lt;\psi(\tilde{\bf
x};\theta),\frac{\partial lnp_\sigma(\tilde{\bf x})}{\partial \tilde{\bf
x}}&gt; \mathrm{d}\tilde{\bf x} \\
&amp;= \int_{\tilde{\bf x}} p_\sigma(\tilde{\bf x})&lt;\psi(\tilde{\bf
x};\theta),\frac{1}{p_\sigma(\tilde{\bf x})} \frac{\partial
p_\sigma(\tilde{\bf x})}{\partial \tilde{\bf x}}&gt;
\mathrm{d}\tilde{\bf x} \\
&amp;= \int_{\tilde{\bf x}} &lt;\psi(\tilde{\bf x};\theta),
\frac{\partial p_\sigma(\tilde{\bf x})}{\partial \tilde{\bf x}}&gt;
\mathrm{d}\tilde{\bf x} \\
&amp;= \int_{\tilde{\bf x}} &lt;\psi(\tilde{\bf x};\theta),
\frac{\partial \int_{\bf x} p_\sigma(\tilde{\bf x}|{\bf x})p_\sigma({\bf
x})\mathrm{d}{\bf x}}{\partial \tilde{\bf x}}&gt; \mathrm{d}\tilde{\bf
x} \\
&amp;= \int_{\tilde{\bf x}} \int_{\bf x} p_\sigma({\bf
x})&lt;\psi(\tilde{\bf x};\theta), \frac{\partial p_\sigma(\tilde{\bf
x}|{\bf x})}{\partial \tilde{\bf x}}&gt; \mathrm{d}{\bf x}
\mathrm{d}\tilde{\bf x} \\
&amp;= \int_{\tilde{\bf x}} \int_{\bf x} p_\sigma({\bf
x})p_\sigma(\tilde{\bf x}|{\bf x})&lt;\psi(\tilde{\bf x};\theta),
\frac{\partial ln p_\sigma(\tilde{\bf x}|{\bf x})}{\partial \tilde{\bf
x}}&gt; \mathrm{d}{\bf x} \mathrm{d}\tilde{\bf x} \\
&amp;= \int_{\tilde{\bf x}} \int_{\bf x} p_\sigma(\tilde{\bf x},{\bf
x})&lt;\psi(\tilde{\bf x};\theta), \frac{\partial ln p_\sigma(\tilde{\bf
x}|{\bf x})}{\partial \tilde{\bf x}}&gt; \mathrm{d}{\bf x}
\mathrm{d}\tilde{\bf x} \\
&amp;= \mathbb{E}_{p_\sigma(\tilde x, x)} \left[ &lt;\psi(\tilde{\bf
x};\theta),\frac{\partial lnp_\sigma(\tilde{\bf x}|{\bf x})}{\partial
\tilde{\bf x}}&gt; \right] \\
\end{align*}
$$</span></p>
<p>从而得到： <span class="math display">$$
\begin{align*}
J_{ESM,p_{\sigma}}(\theta) &amp;= \mathbb{E}_{p_\sigma(\tilde x)}\left[
\frac{1}{2} ||\psi(\tilde
x;\theta)||^2\right]-\mathbb{E}_{p_\sigma(\tilde x, x)} \left[
&lt;\psi(\tilde{\bf x};\theta),\frac{\partial lnp_\sigma(\tilde{\bf
x}|{\bf x})}{\partial \tilde{\bf x}}&gt; \right]+C_2\\
\end{align*}
$$</span></p>
<p>另一方面，针对DSM进行分解： <span class="math display">$$
\begin{align*}
J_{DSM,p_{\sigma}}(\theta)&amp;=\mathbb{E}_{p_\sigma(\tilde x,x)}\left[
\frac{1}{2} ||\psi(\tilde x;\theta)-\frac{\partial ln p_{\sigma}(\tilde
x|x)}{\partial \tilde x}||^2\right] \\
&amp;= \mathbb{E}_{p_\sigma(\tilde x)}\left[ \frac{1}{2} ||\psi(\tilde
x;\theta)||^2 \right]-\mathbb{E}_{p_\sigma(\tilde x, x)} \left[
&lt;\psi(\tilde{\bf x};\theta),\frac{\partial lnp_\sigma(\tilde{\bf
x}|{\bf x})}{\partial \tilde{\bf x}}&gt; \right]+C_3
\end{align*}
$$</span></p>
<p>其中<span class="math inline">$C_3=\mathbb{E}_{p_\sigma(\tilde x,
x)}\left[\frac{1}{2}||\frac{\partial ln p_{\sigma}(\tilde x|x)}{\partial
\tilde x}||^2\right]$</span>，同样不依赖于<span
class="math inline"><em>θ</em></span>参数，可以认为是一个常量。因此可以得到DSM与ESM的关系：
<span class="math display">$$
\begin{align}
J_{DSM,p_{\sigma}}(\theta) = J_{ESM,p_{\sigma}}(\theta)-C_3+C_2
\end{align}
$$</span></p>
<p>其中<span
class="math inline"><em>C</em><sub>2</sub>, <em>C</em><sub>3</sub></span>与优化目标<span
class="math inline"><em>θ</em></span>无关，因此可以直接忽略。因此可以证明两者是等价的。</p>
<h2 id="总结与分析">总结与分析</h2>
<p>相对于ESM方法，引入噪音简化二阶偏导为一个关于方差<span
class="math inline"><em>σ</em></span>的函数。并且成功证明，两者等价。</p>
<p>然而这个方法存在一些问题： * 学习到的是加上噪音之后的分布，不是原分布
* 加入的方差<span class="math inline"><em>σ</em></span>很难控制调整</p>
<h1 id="切片匹配得分sliced-score-matchingssm">切片匹配得分（Sliced Score
Matching，SSM）</h1>
<p>来源于文章——Sliced Score Matching: A Scalable Approach to Density and
Score Estimation。</p>
<h2 id="降维思想">降维思想</h2>
<p>直接对梯度进行比较，会因为维度较高产生问题，可以尝试通过一个函数<span
class="math inline"><em>V</em></span>，将高维度映射到低纬度上。</p>
<p><span class="math display">$$
\begin{align*}
J_{SSM}(\theta)&amp;=\mathbb{E}_{\xi}\left[\frac{1}{2}||{\bf v}^T
\psi(\xi;\theta)- {\bf v}^T\psi_x(\xi)||^2\right]\\
&amp;= \mathbb{E}_{p_x(\xi)}[\frac{1}{2} {\bf v}^T \psi(\xi;\theta)^2
{\bf v}+\text{tr}({\bf v}^T\nabla_{\xi} \psi_{\xi}(\xi;\theta){\bf v})]
\\
\end{align*}
$$</span></p>
<p>要求<span class="math inline">${\bf v}\sim p_v, \mathbb{E}_{p_v}[{\bf
v}{\bf v}^T]&gt;0, \mathbb{E}_{p_v}[||{\bf
v}||^2_2]&lt;\infty$</span>，这是因为需要最后出一个与<span
class="math inline"><em>θ</em></span>无关的常数。这种分布也是容易找到的，例如正态分布、均匀分布等。</p>
<!--
# Score Matching model with Langevin Dynamics

The idea from paper -- Generative Modeling by Estimating Gradients of the
Data Distribution.

## Langevin Dynamics
-->
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Score matching model</tag>
        <tag>Explicit Score Matching</tag>
        <tag>Implicit Score Matching</tag>
        <tag>Denoising Score Matching</tag>
        <tag>Sliced Score Matching</tag>
      </tags>
  </entry>
  <entry>
    <title>A Survey of Monte Carlo Tree Search Methods</title>
    <url>/2024/04/17/DL/Survey_MCTS/Survey_MCTS/</url>
    <content><![CDATA[<p>本文为<a
href="https://ieeexplore.ieee.org/document/6145622">蒙特卡洛树搜索综述</a>节选。</p>
<p>蒙特卡洛树搜索（Monte Carlo Tree
Search，简称MCTS）是一种用于决策过程的模拟和搜索算法，特别适用于解决复杂的顺序决策问题。以下是MCTS的几个核心步骤：</p>
<ol type="1">
<li>选择（Selection）：从根节点开始，根据某种策略（如UCB1公式），选择一条路径直到达到一个未扩展的节点。</li>
<li>拓展（Expansion）：在当前选中的节点上添加一个新的子节点。</li>
<li>模拟（Simulation）：从新扩展的节点开始进行随机模拟（即走子），直到游戏结束，并记录结果。</li>
<li>反向传播（Backpropagation）：将模拟的结果回传，更新沿途节点的统计信息。</li>
</ol>
<p>值得一提的是，MCTS通过不断重复上述过程，逐渐构建出一个代表不同行动及其结果概率的搜索树。最终，算法会选择具有最高胜率的行动作为推荐的行动。</p>
<p>此外，MCTS与传统的博弈树搜索相比，它不需要构建整个博弈树，而是通过模拟来评估局面，这使得它能够处理更大规模的搜索空间。然而，MCTS也存在一些局限性，比如对于非零和游戏的适应性、在面对随机性或不完全信息时的表现等问题。尽管如此，MCTS仍然是现代人工智能领域的一个重要工具，尤其是在棋类游戏中的应用，如AlphaGo/Zero等著名程序就是基于MCTS的变体。</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>包含决策过程（decision theory）、博弈论（game theory）、Monte Carlo
以及 bandit-based methods。</p>
<h2 id="decision-theory">Decision Theory</h2>
<p>本质是马尔可夫决策决策过程（Markov Decision Processes，简记为
MDPs），通过一系列 状态-动作-回报 系列完成对于决策过程的刻画。</p>
<p>部分观测的MDP（Partially Observable
MDPs，简记为POMDP），每次获得的状态只是整体状态的一部。</p>
<h2 id="game-theory">Game Theory</h2>
<h2 id="monte-carlo-methods">Monte Carlo Methods</h2>
<h2 id="bandit-based-methods">Bandit-Based Methods</h2>
<p>多臂老虎机问题（Multi-armed Bandit
problems）是知名的系列决策问题。该问题的难点在于平衡探索与开发（Exploration
and
Exploitation），以推荐系统为例，在已经知道用户兴趣点之后需要进行“开发”，但是现有的兴趣点可能不是用户最感兴趣的，或者该兴趣点会随着时间改变，因此需要进行“探索”，不断试探用户新的感兴趣内容。在Spin
Glass求基态问题中，表现为是在这个结构中继续降低能量，还是选择升高温度跳出当前局域范围进行探索。<a
href="https://zhuanlan.zhihu.com/p/80261581">Bandit
算法</a>用于解决该问题。</p>
<p>一种有效的方案是基于上置信界（upper confidence bound
简记UCB）。最简单的UCB算法为<a
href="https://link.springer.com/article/10.1023/A:1013689704352">UCB1</a>。</p>
<p><font color='red'>细致平衡和这个感觉类似，这两者是否有关系？</font></p>
<h1 id="monte-carlo-tree-search">Monte Carlo Tree Search</h1>
<p>MCTS依靠于两个基本概念：动作的真实回报，可以依靠随机模拟近似；这些回报可以有效的调整策略去接近最优策略。</p>
<h2 id="algorithm">Algorithm</h2>
<p>基础算法通过迭代构建树，从根节点出发搜索子节点，然后选取最好的子节点作为下一次搜索的根节点。在这个迭代过程中有4个典型步骤：
1)
<strong>选择</strong>：从根节点开始递归应用子节点选择策略遍历整个树，直到达到收敛节点。
2) <strong>探索</strong>：产生动作，将更多节点添加进树中。 3)
<strong>模拟评估</strong>：通过策略模拟给出一个节点的评估分数。 4)
<strong>反向传播</strong>：通树结构反向传播模拟评估结果。</p>
<p>采用以下两个直接的策略： 1) Tree policy:
从搜索树中选择或者探索一个叶子节点。<font color='red'>如何平衡探索与贪心选择，是这个算法的关键。</font>
2) Default policy:
模拟评估非终止状态的分数。<font color='red'>这里一直没有明白，算法并没有对该模型的知识，如何评估一个局面的好坏呢？如果已经对这个问题已经有了很好的了解，那直接贪心就可以了，为什么还要有MCTS？</font></p>
<p>伪代码为： <img src="./3_A_1.png" alt="Algorithm" /></p>
<p><span class="math inline"><em>v</em><sub>0</sub></span>是根节点<span
class="math inline"><em>s</em><sub>0</sub></span>的评分，其中<span
class="math inline"><em>v</em><sub><em>l</em></sub></span>是叶子节点<span
class="math inline"><em>s</em><sub><em>l</em></sub></span>的评分，<span
class="math inline"><em>Δ</em></span>是从探索节点返回的值。</p>
<figure>
<img src="./3_A_2.png" alt="general MCTS approach" />
<figcaption aria-hidden="true">general MCTS approach</figcaption>
</figure>
<p>上面展示了一次过程的示意图。</p>
<h2 id="development">Development</h2>
<figure>
<img src="./3_B_1.png" alt="development of MCTS" />
<figcaption aria-hidden="true">development of MCTS</figcaption>
</figure>
<h2 id="tree-policy">Tree policy</h2>
<p>Upper Confidence Bound for Trees (UCT)算法是Tree
policy中的一种，用于解决exploration–exploitation dilemma.</p>
<p>在子节点<span class="math inline"><em>j</em></span>去选择最大化：
<span class="math display">$$\begin{align}
\mathrm{UCT}=\bar{X}_j+2 C_p \sqrt{\frac{2 \ln n}{n_j}} \label{UCB}
\end{align}$$</span></p>
<p>其中 <span class="math inline"><em>n</em></span>
是当前节点被遍历过的次数，<span
class="math inline"><em>n</em><sub><em>j</em></sub></span>是子节点<span
class="math inline"><em>j</em></span>被遍历过的次数，并且限制<span
class="math inline"><em>C</em><sub><em>p</em></sub> &gt; 0</span>。如果超过一个子节点有相同的最大值，通常随机选择。<span
class="math inline"><em>X</em><sub><em>i</em>, <em>t</em></sub></span>和<span
class="math inline"><em>X̄</em><sub><em>j</em></sub></span>是选择<span
class="math inline"><em>j</em></span>的平均回报，值在范围<span
class="math inline">[0, 1]</span>上。通常设置<span
class="math inline"><em>n</em><sub><em>j</em></sub> = 0</span>用于鼓励探索未曾探索过的区域。<span
class="math inline"><em>C</em><sub><em>p</em></sub></span>可以用来调节探索的倾向，有很多文献对这个的选择有很多讨论。</p>
<p><span
class="math inline">$\eqref{UCB}$</span>存在这样的一种思想，如果子节点都探索过，倾向于选择收益高的节点；如果子节点存在没有探索过的节点，倾向于选择没有探索过的节点。</p>
<h2 id="default-policy">Default policy</h2>
<blockquote>
<p>In the simplest case, this default policy is uniformly random.</p>
</blockquote>
<p>最简单的处理方案是随机产生。</p>
<p><font color='blue'>看这部分算法原本目的是想要了解这部分内容，但是现在发现MCTS的核心思想在于如何平衡探索和开发。这部分策略本质上是评价函数，如何构建评价函数，从而可以对状态进行评分。这其实是强化学习的任务。MCTS的任务是在知道这部分内容的前提下，将评估函数很好的与实际应用结合。</font></p>
<p>该函数的设计本质上是一个很难的问题。综述中给出了如下的设计提升方案：
* Rule-Based Simulation Policy * Contextual Monte Carlo Search * Fill
the Board * Learning a Simulation Policy * Using History Heuristics *
Evaluation Function * Simulation Balancing * Last Good Reply *
Patterns</p>
<h1 id="code">Code</h1>
<p><img src="./3_B_2_1.png" alt="UCB" /> <img src="./3_B_2_2.png"
alt="UCB" /> 以上式算法的伪代码。</p>
<p>下面分析GitHub上关于MCTS的实现，<a
href="https://github.com/suragnair/alpha-zero-general/blob/master/MCTS.py">代码地址</a>。</p>
<p>首先导入相关的包：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">EPS = <span class="number">1e-8</span></span><br><span class="line"></span><br><span class="line">log = logging.getLogger(__name__)</span><br></pre></td></tr></table></figure>
<p>接下来创建名为<code>MCTS</code>的类：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MCTS</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This class handles the MCTS tree.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, game, nnet, args</span>):</span><br><span class="line">        <span class="variable language_">self</span>.game = game</span><br><span class="line">        <span class="variable language_">self</span>.nnet = nnet</span><br><span class="line">        <span class="variable language_">self</span>.args = args</span><br><span class="line">        <span class="comment"># 以下内容全部用字典表示，key是状态对应的字符串</span></span><br><span class="line">        <span class="comment"># 存储状态的Q值</span></span><br><span class="line">        <span class="variable language_">self</span>.Qsa = &#123;&#125;  <span class="comment"># stores Q values for s,a (as defined in the paper)</span></span><br><span class="line">        <span class="comment"># 存储已经探索过状态-动作对的次数</span></span><br><span class="line">        <span class="variable language_">self</span>.Nsa = &#123;&#125;  <span class="comment"># stores #times edge s,a was visited</span></span><br><span class="line">        <span class="comment"># 存储已经探索过状态的次数</span></span><br><span class="line">        <span class="variable language_">self</span>.Ns = &#123;&#125;  <span class="comment"># stores #times board s was visited</span></span><br><span class="line">        <span class="comment"># 存储初始策略</span></span><br><span class="line">        <span class="variable language_">self</span>.Ps = &#123;&#125;  <span class="comment"># stores initial policy (returned by neural net)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储游戏终止状态</span></span><br><span class="line">        <span class="variable language_">self</span>.Es = &#123;&#125;  <span class="comment"># stores game.getGameEnded ended for board s</span></span><br><span class="line">        <span class="comment"># 存储该状态下接下来合法移动</span></span><br><span class="line">        <span class="variable language_">self</span>.Vs = &#123;&#125;  <span class="comment"># stores game.getValidMoves for board s</span></span><br></pre></td></tr></table></figure>
<p>以下<code>search</code>与<code>getActionProb</code>函数，均是<code>MCTS</code>类中的函数。</p>
<p><code>search</code>函数执行一次搜索过程，从一个节点开始，不断探索子节点，直到达到终态。每一步选择最大UCB的过程。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MCTS</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, game, nnet, args</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, canonicalBoard</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This function performs one iteration of MCTS. It is recursively called</span></span><br><span class="line"><span class="string">        till a leaf node is found. The action chosen at each node is one that</span></span><br><span class="line"><span class="string">        has the maximum upper confidence bound as in the paper.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Once a leaf node is found, the neural network is called to return an</span></span><br><span class="line"><span class="string">        initial policy P and a value v for the state. This value is propagated</span></span><br><span class="line"><span class="string">        up the search path. In case the leaf node is a terminal state, the</span></span><br><span class="line"><span class="string">        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are</span></span><br><span class="line"><span class="string">        updated.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        NOTE: the return values are the negative of the value of the current</span></span><br><span class="line"><span class="string">        state. This is done since v is in [-1,1] and if v is the value of a</span></span><br><span class="line"><span class="string">        state for the current player, then its value is -v for the other player.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            v: the negative of the value of the current canonicalBoard</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将状态转化为字符串的形式</span></span><br><span class="line">        s = <span class="variable language_">self</span>.game.stringRepresentation(canonicalBoard)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果状态不在之前是否记录表中，则重新检测是否为终态。</span></span><br><span class="line">        <span class="keyword">if</span> s <span class="keyword">not</span> <span class="keyword">in</span> <span class="variable language_">self</span>.Es:</span><br><span class="line">            <span class="comment"># 是终态返回 1 or -1（表示胜负结果），不是终态返回 0</span></span><br><span class="line">            <span class="variable language_">self</span>.Es[s] = <span class="variable language_">self</span>.game.getGameEnded(canonicalBoard, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果已经终止，直接返回结果</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.Es[s] != <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># terminal node</span></span><br><span class="line">            <span class="keyword">return</span> -<span class="variable language_">self</span>.Es[s]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果不在策略的记录表中，产生初始策略</span></span><br><span class="line">        <span class="keyword">if</span> s <span class="keyword">not</span> <span class="keyword">in</span> <span class="variable language_">self</span>.Ps:</span><br><span class="line">            <span class="comment"># leaf node</span></span><br><span class="line">            <span class="comment"># 神经网络评估该状态价值</span></span><br><span class="line">            <span class="variable language_">self</span>.Ps[s], v = <span class="variable language_">self</span>.nnet.predict(canonicalBoard)</span><br><span class="line">            <span class="comment"># 产生所有合法移动</span></span><br><span class="line">            valids = <span class="variable language_">self</span>.game.getValidMoves(canonicalBoard, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 遮盖不合法移动</span></span><br><span class="line">            <span class="variable language_">self</span>.Ps[s] = <span class="variable language_">self</span>.Ps[s] * valids  <span class="comment"># masking invalid moves</span></span><br><span class="line">            sum_Ps_s = np.<span class="built_in">sum</span>(<span class="variable language_">self</span>.Ps[s])</span><br><span class="line">            <span class="keyword">if</span> sum_Ps_s &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="variable language_">self</span>.Ps[s] /= sum_Ps_s  <span class="comment"># renormalize</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># if all valid moves were masked make all valid moves equally probable</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># NB! All valid moves may be masked if either your NNet architecture is insufficient or you&#x27;ve get overfitting or something else.</span></span><br><span class="line">                <span class="comment"># If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   </span></span><br><span class="line">                log.error(<span class="string">&quot;All valid moves were masked, doing a workaround.&quot;</span>)</span><br><span class="line">                <span class="variable language_">self</span>.Ps[s] = <span class="variable language_">self</span>.Ps[s] + valids</span><br><span class="line">                <span class="variable language_">self</span>.Ps[s] /= np.<span class="built_in">sum</span>(<span class="variable language_">self</span>.Ps[s])</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.Vs[s] = valids</span><br><span class="line">            <span class="variable language_">self</span>.Ns[s] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">return</span> -v</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 给予当前状态的合法移动</span></span><br><span class="line">        valids = <span class="variable language_">self</span>.Vs[s]</span><br><span class="line">        <span class="comment"># 初始化当前最好UCB，当前最好选择</span></span><br><span class="line">        cur_best = -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">        best_act = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pick the action with the highest upper confidence bound</span></span><br><span class="line">        <span class="comment"># 选择UCB最高的动作，返回所有可能的动作状态数</span></span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.game.getActionSize()):</span><br><span class="line">            <span class="keyword">if</span> valids[a]:   <span class="comment">#检测是否合法</span></span><br><span class="line">                <span class="comment"># 计算UCB</span></span><br><span class="line">                <span class="keyword">if</span> (s, a) <span class="keyword">in</span> <span class="variable language_">self</span>.Qsa:</span><br><span class="line">                    u = <span class="variable language_">self</span>.Qsa[(s, a)] + <span class="variable language_">self</span>.args.cpuct * <span class="variable language_">self</span>.Ps[s][a] * math.sqrt(<span class="variable language_">self</span>.Ns[s]) / (</span><br><span class="line">                            <span class="number">1</span> + <span class="variable language_">self</span>.Nsa[(s, a)])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    u = <span class="variable language_">self</span>.args.cpuct * <span class="variable language_">self</span>.Ps[s][a] * math.sqrt(<span class="variable language_">self</span>.Ns[s] + EPS)  <span class="comment"># Q = 0 ?</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> u &gt; cur_best:</span><br><span class="line">                    cur_best = u</span><br><span class="line">                    best_act = a</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选择子节点并更新状态</span></span><br><span class="line">        a = best_act</span><br><span class="line">        next_s, next_player = <span class="variable language_">self</span>.game.getNextState(canonicalBoard, <span class="number">1</span>, a)</span><br><span class="line">        next_s = <span class="variable language_">self</span>.game.getCanonicalForm(next_s, next_player)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 子节点进行迭代</span></span><br><span class="line">        v = <span class="variable language_">self</span>.search(next_s)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新X与Nsa</span></span><br><span class="line">        <span class="keyword">if</span> (s, a) <span class="keyword">in</span> <span class="variable language_">self</span>.Qsa:</span><br><span class="line">            <span class="variable language_">self</span>.Qsa[(s, a)] = (<span class="variable language_">self</span>.Nsa[(s, a)] * <span class="variable language_">self</span>.Qsa[(s, a)] + v) / (<span class="variable language_">self</span>.Nsa[(s, a)] + <span class="number">1</span>)</span><br><span class="line">            <span class="variable language_">self</span>.Nsa[(s, a)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.Qsa[(s, a)] = v</span><br><span class="line">            <span class="variable language_">self</span>.Nsa[(s, a)] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.Ns[s] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> -v</span><br></pre></td></tr></table></figure>
<p><code>getActionProb</code>为MCTS的每一步搜索</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MCTS</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, game, nnet, args</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 温度相当于给予不同动作一个概率，如果temp=1则直接贪心选择 </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getActionProb</span>(<span class="params">self, canonicalBoard, temp=<span class="number">1</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This function performs numMCTSSims simulations of MCTS starting from</span></span><br><span class="line"><span class="string">        canonicalBoard.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            probs: a policy vector where the probability of the ith action is</span></span><br><span class="line"><span class="string">                   proportional to Nsa[(s,a)]**(1./temp)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 进行多次模拟</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.args.numMCTSSims):</span><br><span class="line">            <span class="variable language_">self</span>.search(canonicalBoard)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将状态编码为字符串</span></span><br><span class="line">        s = <span class="variable language_">self</span>.game.stringRepresentation(canonicalBoard)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历当前状态下，所有合法动作已经探索过的次数</span></span><br><span class="line">        counts = [<span class="variable language_">self</span>.Nsa[(s, a)] <span class="keyword">if</span> (s, a) <span class="keyword">in</span> <span class="variable language_">self</span>.Nsa <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.game.getActionSize())]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果温度为0则贪心的返回</span></span><br><span class="line">        <span class="keyword">if</span> temp == <span class="number">0</span>:</span><br><span class="line">            bestAs = np.array(np.argwhere(counts == np.<span class="built_in">max</span>(counts))).flatten()</span><br><span class="line">            bestA = np.random.choice(bestAs)</span><br><span class="line">            probs = [<span class="number">0</span>] * <span class="built_in">len</span>(counts)</span><br><span class="line">            probs[bestA] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> probs</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 温度非0,给予每个动作一个归一化的被选择几率值</span></span><br><span class="line">        counts = [x ** (<span class="number">1.</span> / temp) <span class="keyword">for</span> x <span class="keyword">in</span> counts]</span><br><span class="line">        counts_sum = <span class="built_in">float</span>(<span class="built_in">sum</span>(counts))</span><br><span class="line">        probs = [x / counts_sum <span class="keyword">for</span> x <span class="keyword">in</span> counts]</span><br><span class="line">        <span class="keyword">return</span> probs</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Monte Carlo</tag>
        <tag>Monte Carlo Tree Search</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</title>
    <url>/2024/01/22/DL/diffusion_process/diffusion_process/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>本篇文章来自于： Deep Unsupervised Learning using Nonequilibrium
Thermodynamics，arXiv:1503.03585v8 [cs.LG] 18 Nov 2015</p>
<p>该篇文章为首次提出Deffusion
Model的概念。算法的主要目标是构造一个前向传播、扩散的过程，通过这个过程可以将复杂的分布逐渐变为一个简单的分布。</p>
<figure>
<img src="./process.png" alt="生成图片" />
<figcaption aria-hidden="true">生成图片</figcaption>
</figure>
<span id="more"></span>
<p>其中第一行是 swiss roll
数据，通过扩散过程，从左到右，逐渐变为一个高斯分布。第二行是训练的模型，从右到左逐步从高斯分布生成原始的数据分布。</p>
<h1 id="forward-trajectory">Forward Trajectory</h1>
<p>数据分布为<span
class="math inline"><em>q</em>(<em>x</em><sup>(0)</sup>)</span>，最终分布<span
class="math inline"><em>π</em>(<em>y</em>)</span>，其中利用马尔科夫扩散核<span
class="math inline"><em>T</em><sub><em>π</em></sub>(<em>y</em>|<em>y</em><sup>′</sup>; <em>β</em>)</span>，<span
class="math inline"><em>β</em></span>为扩散率。</p>
<p><span class="math display">$$
\begin{align}
\pi(y) &amp;= \int \mathrm{d}y' T_{\pi}(y|y';\beta)\pi (y') \\
q(x^{(t)}|x^{(t-1)}) &amp;= T_{\pi}(x^{(t)}|x^{(t-1)};\beta_t) \\
q(x^{(0\dots T)}) &amp;= q(x^{(0)})\prod_{t=1}^{T} q(x^{(t)}|x^{(t-1)})
\\
\end{align}
$$</span></p>
<h1 id="reverse-trajectory">Reverse Trajectory</h1>
<p><span class="math inline"><em>p</em></span>为逆向使用数据的过程。</p>
<p><span class="math display">$$
\begin{align}
p(x^{(T)}) &amp;= \pi(x^{(T)}) \\
p(x^{(0\dots T)}) &amp;= p(x^{(T)})\prod_{t=T}^{1} p(x^{(t-1)}|x^{(t)})
\\
\end{align}
$$</span></p>
<h1 id="model-probability">Model Probability</h1>
<p><span
class="math display"><em>p</em>(<em>x</em><sup>(0)</sup>) = ∫d<em>x</em><sup>(1⋯<em>T</em>)</sup><em>p</em>(<em>x</em><sup>(0⋯<em>T</em>)</sup>)</span></p>
<p>但是，事实上逆向轨迹几乎不可能被追踪，因此需要借助前向过程。 <span
class="math display">$$
\begin{align}
p(x^{(0)})&amp;=\int \mathrm{d}x^{(1\cdots T)}p(x^{(0\cdots
T)})\frac{q(x^{(1\cdots T)|x^{(0)}})}{q(x^{(1\cdots T)|x^{(0)}})} \\
&amp;=\int \mathrm{d}x^{(1\cdots T)}q(x^{(1\cdots
T)}|x^{(0)})\frac{p(x^{(0\cdots T)})}{q(x^{(1\cdots T)}|x^{(0)})} \\
&amp;=\int \mathrm{d}x^{(1\cdots T)}q(x^{(1\cdots
T)}|x^{(0)})p(x^{(T)})\prod_{t=T}^{1}\frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t)}|x^{(t-1)})}
\\
\end{align}
$$</span></p>
<h1 id="训练">训练</h1>
<p>目标是为了最小化模型似然估计。</p>
<p><span class="math display">$$
\begin{align}
L &amp;= \int \mathrm{d}x^{(0)}q(x^{(0)})\ln p(x^{(0)}) \\
&amp;= \int \mathrm{d}x^{(0)}q(x^{(0)})\ln \left( \int
\mathrm{d}x^{(1\cdots T)}q(x^{(1\cdots
T)}|x^{(0)})p(x^{(T)})\prod_{t=T}^{1}\frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t)}|x^{(t-1)})}\right)
\\
&amp;\geq \int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)})\ln \left(
p(x^{(T)})\prod_{t=T}^{1}\frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t)}|x^{(t-1)})}\right)
\\
&amp;= \int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)})\ln  p(x^{(T)}) +
\int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)}) \sum_{t=T}^{1}\ln\left(
\frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t)}|x^{(t-1)})}\right)\\
&amp;= \int \mathrm{d}x^{(T)}q(x^{(T)})\ln  \pi(x^{(T)}) + \int
\mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)}) \sum_{t=T}^{1}\ln\left(
\frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t)}|x^{(t-1)})}\right)\\
&amp;= \int \mathrm{d}x^{(T)}q(x^{(T)})\ln  \pi(x^{(T)}) +
\sum_{t=1}^{T} \int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)})
\ln\left( \frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t)}|x^{(t-1)})}\right)\\
&amp;= \sum_{t=1}^{T} \int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)})
\ln\left( \frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t)}|x^{(t-1)})}\right)-H_p
(x^{T})\\
&amp;= \sum_{t=2}^{T} \int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)})
\ln\left( \frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t)}|x^{(t-1)})}\right)-H_p
(x^{T})+\int \mathrm{d}x^{(0,1)}q(x^{(0, 1)}) \ln\left(
\frac{p(x^{(0)}|x^{(1)})}{q(x^{(1)}|x^{(0)})}\right)\\
&amp;= \sum_{t=2}^{T} \int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)})
\ln\left( \frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t)}|x^{(t-1)})}\right)-H_p
(x^{T})+\int \mathrm{d}x^{(0,1)}q(x^{(0, 1)}) \ln\left(
\frac{\pi(x^{(0)})}{\pi(x^{(1)})}\right)\\
&amp;= \sum_{t=2}^{T} \int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)})
\ln\left( \frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t)}|x^{(t-1)})}\right)-H_p
(x^{T})\\
&amp;= \sum_{t=2}^{T} \int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)})
\ln\left( \frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t)}|x^{(t-1)},
x^{(0)})}\right)-H_p (x^{T})\\
&amp;= \sum_{t=2}^{T} \int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)})
\ln\left( \frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t-1)}|x^{(t)},
x^{(0)})}  \frac{q(x^{(t-1)}|x^{(0)})}{q(x^{(t)}|x^{(0)})}  \right)-H_p
(x^{T})\\
&amp;= \sum_{t=2}^{T} \int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)})
\ln\left( \frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t-1)}|x^{(t)},
x^{(0)})}\right)+\sum_{t=2}^{T} \int \mathrm{d}x^{(0\cdots
T)}q(x^{(0\cdots T)}) \ln \left(
\frac{q(x^{(t-1)}|x^{(0)})}{q(x^{(t)}|x^{(0)})}\right) -H_p (x^{T})\\
&amp;= \sum_{t=2}^{T} \int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)})
\ln\left( \frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t-1)}|x^{(t)},
x^{(0)})}\right)+\sum_{t=2}^{T} \int \mathrm{d}x^{(0\cdots
T)}q(x^{(0\cdots T)})  \left( \ln
q(x^{(t-1)}|x^{(0)})-\ln{q(x^{(t)}|x^{(0)})}\right) -H_p (x^{T})\\
&amp;= \sum_{t=2}^{T} \int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)})
\ln\left( \frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t-1)}|x^{(t)},
x^{(0)})}\right)+\sum_{t=2}^{T}  \left(
H_q(x^{(t)}|x^{(0)})-H_q(x^{(t-1)}|x^{(0)})\right) -H_p (x^{T})\\
&amp;= \sum_{t=2}^{T} \int \mathrm{d}x^{(0\cdots T)}q(x^{(0\cdots T)})
\ln\left( \frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t-1)}|x^{(t)},
x^{(0)})}\right)+  H_q(x^{(T)}|x^{(0)})-H_q(x^{(1)}|x^{(0)})-H_p
(x^{T})\\
&amp;= -\sum_{t=2}^{T} \int \mathrm{d}x^{(0, t)}q(x^{(0, t)})
\text{D}_{KL}\left( {q(x^{(t-1)}|x^{(t)},
x^{(0)})}||{p(x^{(t-1)}|x^{(t)})}
\right)+  H_q(x^{(T)}|x^{(0)})-H_q(x^{(1)}|x^{(0)})-H_p (x^{T})\\
&amp;=K
\end{align}
$$</span></p>
<p>其中公式（16）定义<span
class="math inline"><em>H</em><sub><em>p</em></sub>(<em>x</em><sup><em>T</em></sup>) = −∫d<em>x</em><sup>(<em>T</em>)</sup><em>q</em>(<em>x</em><sup>(<em>T</em>)</sup>)ln <em>π</em>(<em>x</em><sup>(<em>T</em>)</sup>)</span>；公式（20）因为这个过程是马尔科夫过程，只与前一个状态有关；公式（21）为贝叶斯公式。经过以上的变换，成功找到下界，任务目标变为:</p>
<p><span
class="math display"><em>p̂</em>(<em>x</em><sup>(<em>t</em> − 1)</sup>|<em>x</em><sup>(<em>t</em>)</sup>) = argmax<sub><em>p</em>(<em>x</em><sup>(<em>t</em> − 1)</sup>|<em>x</em><sup>(<em>t</em>)</sup>)</sub><em>K</em> </span></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Nonequilibrium Thermodynamics</tag>
        <tag>Unsupervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Sora原理分析</title>
    <url>/2024/02/28/DL/sora/sora/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p><a
href="https://openai.com/research/video-generation-models-as-world-simulators">sora</a>完成了文本生成视频的任务，其中视频的时长与连贯性都有非常惊艳的效果，不仅将视频时长拓展到了60S的水平，而且即使视频中发生物品遮挡，在之后也能成功接上，视频整体非常连贯。</p>
<p>感谢<a
href="https://datawhaler.feishu.cn/wiki/FA8UwCrsNiEO0gkh9Uici8RUn5f">Datawhale开源社区</a>提供相关资源。</p>
<span id="more"></span>
<h1 id="vision-transformer">Vision Transformer</h1>
<p>基于官网的介绍： &gt; We leverage a transformer architecture that
operates on spacetime patches of video and image latent codes.</p>
<p>可见sora的基本架构来源于Transform，这里需要先介绍该领域的经典模型<a
href="https://arxiv.org/pdf/2010.11929.pdf">Vision
Transformer(ViT)</a>。</p>
<p>Transform 是广泛应用于 NLP 的架构，会将不同的 token
以序列方式进行处理。在 CV 上，也有很多将 Transform 引入的尝试，ViT
基本将原生的架构引入视觉领域。</p>
<figure>
<img src="./ViT.png" alt="ViT" />
<figcaption aria-hidden="true">ViT</figcaption>
</figure>
<p>首先图片不能直接将像素作为一个基本单位作为一个token，这样输入规模将会过大。需要进行分割，将整张图片进行分割，左下角将整张图片分为9个部分，每个部分作为基本的单位，然后将位置信息直接相加，称为
Patch ，接下来将其线性化，输入到Transform Encoder。</p>
<h1 id="video-vision-transformer">Video Vision Transformer</h1>
<blockquote>
<p>At a high level, we turn videos into patches by first compressing
videos into a lower-dimensional latent space, and subsequently
decomposing the representation into spacetime patches.</p>
</blockquote>
<p>将视频同样进行切块的技术，考虑之前的文章<a
href="https://arxiv.org/pdf/2103.15691.pdf">ViViT: A Video Vision
Transformer(ViViT)</a></p>
<figure>
<img src="./fig1.png" alt="Turning visual data into patches" />
<figcaption aria-hidden="true">Turning visual data into
patches</figcaption>
</figure>
<p>针对于时间部分，同样是划分为小部分块，称为
tuplet。然后以一定序列的方式进行编码，一维化输入到 Transform。</p>
<figure>
<img src="./tuplet.png" alt="Tuplet" />
<figcaption aria-hidden="true">Tuplet</figcaption>
</figure>
<p>训练过程为：</p>
<figure>
<img src="./ViViT.png" alt="ViViT" />
<figcaption aria-hidden="true">ViViT</figcaption>
</figure>
<h1 id="scaling-transformers-for-video-generation">Scaling transformers
for video generation</h1>
<p>整个训练过称为 diffusion model</p>
<figure>
<img src="./diffusion_model.png" alt="diffusion_model" />
<figcaption aria-hidden="true">diffusion_model</figcaption>
</figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Transform</tag>
        <tag>Sora</tag>
      </tags>
  </entry>
  <entry>
    <title>Bayesian Optimization</title>
    <url>/2024/11/08/Math/BayesianOpt/BayesianOpt/</url>
    <content><![CDATA[<p>贝叶斯优化（Bayesian Optimization）是一种基于贝叶斯定理:</p>
<p><span class="math display">$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$</span></p>
<p>的全局优化方法，通常用于在计算代价高昂的情况下优化黑箱函数。它主要用于高效地寻找目标函数的最优解，尤其在函数不可微、函数形状复杂、或者评估函数代价昂贵（如深度学习模型的超参数优化）时特别有效。</p>
<span id="more"></span>
<h1 id="贝叶斯优化基本思想">贝叶斯优化基本思想</h1>
<p>贝叶斯优化的核心是利用已有的观测数据，构建目标函数的近似模型（通常是高斯过程或其他代理模型），然后在这个近似模型上寻找最优解。具体来说，贝叶斯优化通过以下步骤进行：</p>
<ol type="1">
<li><p><strong>构建代理模型</strong>：使用已有的观测数据构建一个代理模型（如高斯过程回归），该模型可以近似目标函数。这个代理模型既能预测目标函数的输出，也能量化预测的不确定性。</p></li>
<li><p><strong>选择采样点</strong>：基于代理模型的输出和不确定性，使用一种称为<strong>采集函数</strong>（Acquisition
Function）的策略，确定下一步要评估的采样点。采集函数在模型不确定性大的区域更倾向于采样，确保探索性。</p></li>
<li><p><strong>更新代理模型</strong>：在新的采样点上评估目标函数并获取真实值，将新数据加入已有数据中，以更新代理模型。</p></li>
<li><p><strong>迭代进行</strong>：重复采样、更新代理模型和优化采集函数，直到满足预设的停止条件（如达到指定次数或精度要求）。</p></li>
</ol>
<p>通过上述步骤，贝叶斯优化逐步将代理模型拟合得更加精准，以更少的评估次数找到目标函数的最优解。</p>
<h1 id="采集函数acquisition-function">采集函数（Acquisition
Function）</h1>
<p>采集函数用于在当前代理模型的基础上选择下一步的采样点，它平衡了探索（探索不确定区域）和开发（在最优点附近深入搜索）的需求。常见的采集函数有：</p>
<ul>
<li><strong>期望提升（Expected Improvement,
EI）</strong>：在当前最优解的基础上，期望获得提升的采样点。</li>
<li><strong>置信上限（Upper Confidence Bound,
UCB）</strong>：考虑模型预测的均值和不确定性，选择具有高置信上限的采样点。</li>
<li><strong>概率提升（Probability of Improvement,
PI）</strong>：选择在当前最优值基础上改进概率最大的采样点。</li>
</ul>
<p>不同的采集函数适用于不同的应用场景，可根据具体需求选择。</p>
<h1 id="示例">示例</h1>
<p>使用 <code>sklearn</code>
中的高斯过程（GaussianProcessRegressor）作为代理模型。使用基本的库为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line"><span class="keyword">from</span> sklearn.gaussian_process <span class="keyword">import</span> GaussianProcessRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.gaussian_process.kernels <span class="keyword">import</span> Matern</span><br></pre></td></tr></table></figure>
<p>定义目标函数，为简单起见，使用一个一维抛物线函数。实际应用中，这个目标函数可以是昂贵的黑箱函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">objective_function</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> (x - <span class="number">2</span>) ** <span class="number">2</span> + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>使用期望提升（Expected Improvement,
EI）采集函数来选择下一个评估点。EI 采集函数的公式如下：</p>
<p><span
class="math display">EI(<em>x</em>) = max (0, <em>μ</em>(<em>x</em>) − <em>f</em>(<em>x</em><sup>+</sup>) − <em>ξ</em>) × <em>Φ</em>(<em>z</em>) + <em>σ</em>(<em>x</em>)<em>ϕ</em>(<em>z</em>)</span></p>
<p>其中 <span class="math inline"><em>μ</em>(<em>x</em>)</span> 和 <span
class="math inline"><em>σ</em>(<em>x</em>)</span> 是代理模型在 <span
class="math inline"><em>x</em></span> 处的预测均值和标准差，<span
class="math inline"><em>f</em>(<em>x</em><sup>+</sup>)</span>
是当前最优值，<span class="math inline"><em>ξ</em></span>
是平衡探索和开发的参数（通常设为 0.01）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">expected_improvement</span>(<span class="params">X, X_sample, Y_sample, model, xi=<span class="number">0.01</span></span>):</span><br><span class="line">    mu, sigma = model.predict(X, return_std=<span class="literal">True</span>)</span><br><span class="line">    mu_sample_opt = np.<span class="built_in">min</span>(Y_sample)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> np.errstate(divide=<span class="string">&#x27;warn&#x27;</span>):</span><br><span class="line">        imp = mu_sample_opt - mu - xi</span><br><span class="line">        Z = imp / sigma</span><br><span class="line">        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)</span><br><span class="line">        ei[sigma == <span class="number">0.0</span>] = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">return</span> ei</span><br></pre></td></tr></table></figure>
<p><span class="math inline"><em>Φ</em>(<em>Z</em>)</span>
是标准正态分布的累积分布函数，<span
class="math inline"><em>ϕ</em>(<em>Z</em>)</span>
是标准正态分布的概率密度函数。</p>
<p>根据 EI
值找到最优的候选点，我们在定义的搜索空间内对采集函数进行最大化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">propose_location</span>(<span class="params">acquisition, X_sample, Y_sample, model, bounds, n_restarts=<span class="number">25</span></span>):</span><br><span class="line">    dim = X_sample.shape[<span class="number">1</span>]</span><br><span class="line">    min_val = <span class="number">1</span></span><br><span class="line">    min_x = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x0 <span class="keyword">in</span> np.random.uniform(bounds[:, <span class="number">0</span>], bounds[:, <span class="number">1</span>], size=(n_restarts, dim)):</span><br><span class="line">        res = minimize(<span class="keyword">lambda</span> x: -acquisition(x.reshape(-<span class="number">1</span>, dim), X_sample, Y_sample, model), </span><br><span class="line">                       x0=x0, bounds=bounds, method=<span class="string">&#x27;L-BFGS-B&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> res.fun &lt; min_val:</span><br><span class="line">            min_val = res.fun</span><br><span class="line">            min_x = res.x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> min_x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>在初始化采样点后，我们不断进行以下循环：更新代理模型、选择新的评估点、更新数据，直到达到预设条件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义搜索空间</span></span><br><span class="line">bounds = np.array([[-<span class="number">5.0</span>, <span class="number">5.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化样本点</span></span><br><span class="line">X_sample = np.random.uniform(bounds[:, <span class="number">0</span>], bounds[:, <span class="number">1</span>], size=(<span class="number">5</span>, <span class="number">1</span>))</span><br><span class="line">Y_sample = objective_function(X_sample)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置高斯过程代理模型</span></span><br><span class="line">kernel = Matern(length_scale=<span class="number">1.0</span>)</span><br><span class="line">model = GaussianProcessRegressor(kernel=kernel, alpha=<span class="number">1e-6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化主循环</span></span><br><span class="line">n_iterations = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_iterations):</span><br><span class="line">    <span class="comment"># 更新代理模型</span></span><br><span class="line">    model.fit(X_sample, Y_sample)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择下一个评估点</span></span><br><span class="line">    X_next = propose_location(expected_improvement, X_sample, Y_sample, model, bounds)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 评估目标函数</span></span><br><span class="line">    Y_next = objective_function(X_next)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新样本集合</span></span><br><span class="line">    X_sample = np.vstack((X_sample, X_next))</span><br><span class="line">    Y_sample = np.vstack((Y_sample, Y_next))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出找到的最优解</span></span><br><span class="line">best_index = np.argmin(Y_sample)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最佳输入点:&quot;</span>, X_sample[best_index])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最佳输出值:&quot;</span>, Y_sample[best_index])</span><br></pre></td></tr></table></figure>
<h1 id="采集函数与active-learning和mcts的异同">采集函数与Active
Learning和MCTS的异同</h1>
<p>贝叶斯优化中的采集函数、主动学习（Active
Learning）、蒙特卡洛树搜索（MCTS）中的探索与利用策略，这三者确实在概念和策略上有很多相似之处，但它们的目标和应用场景有所不同。它们共享的核心思想是
<strong>平衡探索和利用</strong>，即在信息不足的情况下找到最佳的选择。然而，三者的具体实现方式、关注点和目标存在一些差异。</p>
<h3 id="共同点探索与利用的平衡">1. 共同点：探索与利用的平衡</h3>
<p>贝叶斯优化、主动学习和 MCTS
都试图在有限资源下（如计算次数、样本数量、计算成本）高效地做出最优决策。为此，它们都需要在
<strong>探索</strong>（了解更多新的信息）和
<strong>利用</strong>（利用已有信息获得最优解）之间找到平衡。具体来说：</p>
<ul>
<li><strong>探索</strong>：在不确定性较高的地方进行采样、选点或扩展，使得对系统（目标函数、分类边界、搜索树）的了解更全面。</li>
<li><strong>利用</strong>：集中在已知效果较好的地方进行进一步优化，尽快找到最佳解或正确答案。</li>
</ul>
<h3 id="三者的区别">2. 三者的区别</h3>
<h4 id="贝叶斯优化">1) 贝叶斯优化</h4>
<ul>
<li><strong>目标</strong>：贝叶斯优化的目标是
<strong>优化复杂且代价高昂的黑箱函数</strong>。它通过少量采样逐步找到目标函数的最优值。</li>
<li><strong>采集函数</strong>：通过代理模型（如高斯过程）预测函数的均值和不确定性，并通过采集函数（如期望提升、置信上限）选择最优采样点。</li>
<li><strong>探索与利用的平衡</strong>：采集函数的设计考虑了对最优解改进的潜力和不确定性，使得采样点既能优化当前最优值又能探索未观测区域。</li>
</ul>
<h4 id="主动学习active-learning">2) 主动学习（Active Learning）</h4>
<ul>
<li><strong>目标</strong>：主动学习的目标是
<strong>在标签代价高昂的情况下，通过有选择地标注数据提升模型性能</strong>。它在机器学习场景中选择对模型改进最有效的未标注样本，从而用最少的标注成本提升模型准确性。</li>
<li><strong>策略</strong>：主动学习的策略往往基于样本的
<strong>不确定性</strong> 或
<strong>信息量</strong>，例如选择模型预测最不确定的样本（最大熵、最小置信度）或对决策边界贡献最大的样本。</li>
<li><strong>探索与利用的平衡</strong>：在主动学习中，探索和利用的平衡意味着在选择样本时既要减少模型的不确定性，也要选择对决策边界有贡献的样本，使模型尽快准确地覆盖数据分布。</li>
</ul>
<h4 id="蒙特卡洛树搜索mcts">3) 蒙特卡洛树搜索（MCTS）</h4>
<ul>
<li><strong>目标</strong>：MCTS 主要用于
<strong>决策问题中的路径选择</strong>，如在游戏中找到最优策略。MCTS
是一种
<strong>树搜索算法</strong>，通过模拟和采样在巨大搜索空间中找到最优行动路径。</li>
<li><strong>策略</strong>：MCTS 的策略在于对节点进行扩展和选择，常用
<strong>上置信上限（UCB）</strong> 来决定探索和利用。UCB
通过平衡节点的获胜概率和探索价值，决定是否深入已有路径或探索新路径。</li>
<li><strong>探索与利用的平衡</strong>：MCTS
的探索利用平衡在于确保既不会忽略高潜力的节点，也不会在当前优节点过度深入。它动态地在不同节点之间分配模拟次数，逐步找到全局最优路径。</li>
</ul>
<h3 id="关系与差异总结">3. 关系与差异总结</h3>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 26%" />
<col style="width: 21%" />
<col style="width: 36%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>应用场景</th>
<th>主要目标</th>
<th>探索/利用实现</th>
</tr>
</thead>
<tbody>
<tr>
<td>贝叶斯优化</td>
<td>黑箱函数优化</td>
<td>最小化评估代价，找最优值</td>
<td>采集函数（EI、UCB、PI等）</td>
</tr>
<tr>
<td>主动学习</td>
<td>样本选择、模型训练</td>
<td>最少样本提升模型准确性</td>
<td>不确定性采样、信息量最大化</td>
</tr>
<tr>
<td>MCTS</td>
<td>决策、路径规划</td>
<td>找到最优路径</td>
<td>上置信上限（UCB）</td>
</tr>
</tbody>
</table>
<h3 id="三者是否是同样的东西">三者是否是同样的东西？</h3>
<p>尽管三者具有相似的探索-利用平衡机制，它们
<strong>不是完全相同的</strong>。具体来说：</p>
<ul>
<li><strong>贝叶斯优化</strong>
是一个基于概率模型的黑箱优化方法，目的是在复杂函数中找到全局最优解。</li>
<li><strong>主动学习</strong>
是一种选择性采样策略，用于通过最少标注数据训练出高性能模型。</li>
<li><strong>MCTS</strong>
是一种决策树搜索算法，用于路径和策略选择。</li>
</ul>
<p>三者共享的思想是
<strong>在信息不完全或资源有限的情况下，通过探索-利用平衡来找到最优解</strong>。这使得它们在本质上有相通之处，但因目标和应用场景不同，它们的实现细节和关注点各不相同。</p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Bayesian Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>KL散度在VAE中的代码实现</title>
    <url>/2024/04/08/Math/KL_divergence_in_code/KL_divergence_in_code/</url>
    <content><![CDATA[<p><font color='red'>
能否使用除Gauss分布外的其他分布？为什么一定要选取Gauss分布呢？
</font></p>
<p>这篇笔记用于记录在VAE计算过程中，KL散度计算的实现方式，以及背后的原理分析。</p>
<p>参考资料： * <a
href="https://zhuanlan.zhihu.com/p/345095899#">KL散度的推导过程</a></p>
<span id="more"></span>
<p>在VAE中，为了解决随机采样无法求梯度的问题，假设潜变量空间的参数满足Gauss分布（一维情况）：
<span class="math display">$$\begin{align}
\mathcal{N}(\mu, \sigma)=\frac{1}{\sqrt{2 \pi \sigma^2}}
e^{-\frac{(x-\mu)^2}{2 \sigma^2}}
\end{align}$$</span>
然后利用均值和方差进行抽样。这种技巧称为重参数化（reparameterization）。</p>
<p>在应用重参数化技巧后，需要计算两个分布的差距，因此引入KL散度计算：
<span class="math display">$$\begin{align}
\text{KL}\left(p_1(x) \| p_2(x)\right)=\int_x p_1(x) \ln
\frac{p_1(x)}{p_2(x)} d x
\end{align}$$</span></p>
<p>针对两个一维高斯分布: <span
class="math inline"><em>p</em><sub>1</sub> = 𝒩<sub>1</sub>(<em>μ</em><sub>1</sub>, <em>σ</em><sub>1</sub>), <em>p</em><sub>2</sub> = 𝒩<sub>2</sub>(<em>μ</em><sub>2</sub>, <em>σ</em><sub>2</sub>)</span>,
可以计算他们的KL散度如下: <span class="math display">$$
\begin{align}
\text{KL}\left(p_1 \| p_2\right) &amp; =\int_x p_1(x) \ln
\frac{p_1(x)}{q_1(x)} d x \\
&amp; =\int_x p_1(x) \left[\ln p_1(x) - \ln q_1(x)\right] d x \\
&amp; =\int_x p_1(x) \left[\ln\left( \frac{1}{\sqrt{2 \pi \sigma_1^2}}
e^{-\frac{(x-\mu_1)^2}{2 \sigma_1^2}}\right) - \ln\left(
\frac{1}{\sqrt{2 \pi \sigma_2^2}} e^{-\frac{(x-\mu_2)^2}{2
\sigma_2^2}}\right)\right] d x \\
&amp; =\int_x p_1(x) \left[-\ln \sigma_1 -\frac{(x-\mu_1)^2}{2
\sigma_1^2} + \ln\sigma_2 +\frac{(x-\mu_2)^2}{2 \sigma_2^2}\right] d x
\\
&amp; =\int_x p_1(x) \left[\ln \frac{\sigma_2}{\sigma_1}
-\frac{(x-\mu_1)^2}{2 \sigma_1^2} +\frac{(x-\mu_2)^2}{2
\sigma_2^2}\right] d x \\
&amp; =\log \frac{\sigma_2}{\sigma_1}+\underbrace{\int_x p_1(x)
\frac{\left(x-\mu_2\right)^2}{2 \sigma_2^2} d
x}_{\mathrm{B}}-\frac{1}{2}\\
\end{align}
$$</span></p>
<p>关注较为复杂的第二项, 即下标 <span class="math inline">B</span>
这一项。接下来要用的并不是带入 <span
class="math inline"><em>p</em><sub>1</sub>(<em>x</em>)</span>,
而是较为巧妙的使用 <span
class="math inline"><em>x</em> − <em>μ</em><sub>2</sub> = (<em>x</em> − <em>μ</em><sub>1</sub>) + (<em>μ</em><sub>1</sub> − <em>μ</em><sub>2</sub>)</span>,
重新使用常数、方差等性质。 <span class="math display">$$
\begin{align}
B &amp; =\frac{1}{2 \sigma_2^2}\int_x p_1(x)\left(x-\mu_2\right)^2 d x
\\
&amp; =\frac{1}{2 \sigma_2^2}\int_x
p_1(x)\left[\left(x-\mu_1\right)+\left(\mu_1-\mu_2\right)\right]^2 d x
\\
&amp; =\frac{1}{2 \sigma_2^2}\int_x p_1(x)\left(x-\mu_1\right)^2 d
x+\frac{2\left(\mu_1-\mu_2\right) }{2 \sigma_2^2}\int_x
p_1(x)\left(x-\mu_1\right) d x+\left(\mu_1-\mu_2\right)^2 \\
&amp; =\frac{1}{2
\sigma_2^2}\left[\sigma_1^2+0+\left(\mu_1-\mu_2\right)^2\right] \\
&amp; =\frac{1}{2
\sigma_2^2}\left[\sigma_1^2+\left(\mu_1-\mu_2\right)^2\right]
\end{align}
$$</span></p>
<p>综合以上结果, 我们有: <span class="math display">$$
\text{KL}\left(p_1 \| p_2\right)=\log
\frac{\sigma_2}{\sigma_1}+\frac{1}{2
\sigma_2^2}\left(\sigma_1^2+\left(\mu_1-\mu_2\right)^2\right)-\frac{1}{2}
$$</span></p>
<p>接下来, 回到VAE中, 由于我们将自由变量从标准正态分布中采样, 即 <span
class="math inline"><em>p</em><sub>2</sub> = 𝒩<sub>2</sub>(0, 1)</span>;
<span class="math display">$$
\text{KL}\left(p_1|| p_2\right)=-\frac{1}{2} \times\left[2 \log
\sigma_1+1-\sigma_1^2-\mu_1^2\right]
$$</span></p>
<p>大多数的VAE代码中间学习并不是 <span
class="math inline"><em>μ</em></span> 与 <span
class="math inline"><em>σ</em></span>, 而是 <span
class="math inline"><em>μ</em></span> 与<span
class="math inline"><em>l</em><em>o</em><em>g</em><em>v</em><em>a</em><em>r</em> = log <em>σ</em><sup>2</sup></span>，因此在代码中需要进行变换
。</p>
<p>代码如下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">KL = -<span class="number">0.5</span>*torch.<span class="built_in">sum</span>(logvar + <span class="number">1</span> - mu.<span class="built_in">pow</span>() - logvar.exp())</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Kullback Leibler Divergence</tag>
        <tag>KL Divergence</tag>
      </tags>
  </entry>
  <entry>
    <title>Ising formulations of many NP problems</title>
    <url>/2024/04/15/Math/Ising_formulations_of_many_NP_problems/Ising_formulations_of_many_NP_problems/</url>
    <content><![CDATA[<p>为许多NP-complete和NP-hard问题提供了Ising形式，包括<a
href="https://www.semanticscholar.org/topic/Karp%27s-21-NP-complete-problems/1887346">Karp的21个NP完全问题</a>中的所有问题。这收集并扩展了从分区、覆盖和可满足性到Ising模型的映射。在每种情况下，所需的自旋数最多是问题大小的三次方。这项工作可能对设计绝热量子优化算法有用。<font color='gree'>别的优化算法同样有用。</font></p>
<p>文献： * 本文<a href="https://arxiv.org/abs/1302.5843v3">Ising
formulations of many NP problems</a> * <a
href="https://iopscience.iop.org/article/10.1088/0305-4470/15/10/028">On
the computational complexity of Ising spin glass models</a> * <a
href="https://www.sciencedirect.com/science/article/pii/S0166218X01003419">E.
Boros and P.L. Hammer. “Pseudo-Boolean optimization”, Discrete Applied
Mathematics 123 155 (2002).</a> * <a
href="https://link.springer.com/chapter/10.1007/978-1-4684-2001-2_9">Reducibility
among Combinatorial Problems</a> * <a
href="https://academic.oup.com/book/6337?login=true">M. Me ́zard and A.
Montanari. Information, Physics and Computation (2009)</a> * <a
href="https://iopscience.iop.org/article/10.1088/0305-4470/19/9/033/meta">Y.
Fu and P.W. Anderson. “Application of statistical mechanics to
NP-complete problems in combinatorial optimisation”, Journal of Physics
A19 1605 (1986).</a></p>
<span id="more"></span>
<p>名词解释： * adiabatic quantum optimization (AQO) * NP problem
(Nondeterministic Polynomial)
NP问题是指可以在多项式时间内验证一个解的问题。换句话说，如果给定一个问题的解，存在一个多项式时间的算法来验证这个解是否正确。NP问题是决定性问题的复杂度类，它包含所有那些解可以在多项式时间内被验证的问题。
* NP-hard problem
NP-hard问题是指至少和NP中的最困难问题一样困难的问题。更严格地说，NP-hard问题是指不可能在多项式时间内解决的所有问题，除非P=NP（目前还未知是否成立）。所有的NP-hard问题也属于NP问题，因为你可以在多项式时间内验证一个假想的解（尽管找到解本身非常困难）。
* NP-complete
NP-complete问题既是NP问题又是NP-hard问题。这意味着NP-complete问题在NP中是“最难”的问题，并且很难找到这些问题的确切解，但是如果给出了解，可以在多项式时间内验证它。NP-complete问题在计算复杂性理论中非常重要，因为它们可能是P问题（能够在多项式时间内解决的问题）之外的最难问题类别。
* NP-complete decision problem NP-complete decision
problem是一类特殊的NP-complete问题，它们只要求回答“是”或“否”。也就是说，这类问题是决策问题，其解答为真或假，而不是提供一个具体的解。例如，在一个图中寻找哈密顿路径是一个NP-complete问题，但判断一个图是否含有哈密顿路径就是一个NP-complete
decision problem。 * Chimera graph Chimera
graph是量子计算中一个非常重要的概念，它通常与二次无约束二值优化问题（QUBOs）和量子退火算法相关联。在Chimera图中，节点代表目标函数的变量，边则表示变量之间的相互作用。这种图结构允许我们将问题的解映射到量子处理器上，其中节点映射为量子位，边映射为耦合器。通过这样的映射，可以解决如旅行商优化问题等复杂的组合优化问题。
* Complete graph 完全图</p>
<h1 id="introduction">Introduction</h1>
<p>众所周知<a
href="https://iopscience.iop.org/article/10.1088/0305-4470/15/10/028">Ising自旋模型是NP-hard模型</a>，因此自然相让其余的NP问题映射到Ising自旋模型上。从数学上讲，如果一个问题是NP完全的，意味着总是能在多项式时间内将其映射到一个Ising问题上。这个映射过程被重新定义为<a
href="https://www.sciencedirect.com/science/article/pii/S0166218X01003419">pseudo-Boolean
optimization problem</a>。</p>
<p>这篇文章主要目的是展示Ising模型Hamilton的一些精巧构造，这些构造如何帮助映射其它的NP问题。同时还将回顾一下，之前如何将覆盖、满足问题映射到Ising自旋模型上，特别是包括如何将所有的NP问题转化为Ising问题，但Ising模型的自旋数目将会是问题的多项式，并且不低于<span
class="math inline"><em>N</em><sup>3</sup></span>。</p>
<p>在量子计算中利用模拟退火算法的求解优化问题，是通过嵌合图（chimera
graph）完成，然而将完全图（complete
graph）转化为嵌合图是低效的，参考文献<a
href="https://arxiv.org/abs/0804.4884">1</a>和<a
href="https://arxiv.org/abs/1001.3116">2</a>。</p>
<h1 id="partitioning-problems">Partitioning Problems</h1>
<p>分割问题是将一个集合分为两个或者多个子集。这里回顾了分割问题到自旋玻璃的映射，并提出了一个新的基于类似思想（团问题）的映射。</p>
<h2 id="number-partitioning">Number Partitioning</h2>
<p>Number partitioning 问题描述如下：</p>
<p>在一个包含<span
class="math inline"><em>N</em></span>个正数的集合<span
class="math inline"><em>S</em> = {<em>n</em><sub>1</sub>, …, <em>n</em><sub><em>N</em></sub>}</span>，将该集合分为两个不互相包含的集合
<span class="math inline"><em>R</em></span> 和 <span
class="math inline"><em>S</em> − <em>R</em></span>。是否能够使得两子集元素之和相等？</p>
<p>该问题已经知道是一个<a
href="https://link.springer.com/chapter/10.1007/978-1-4684-2001-2_9">NP-complete问题</a>，将该问题转化为一个Ising模型：</p>
<p>令<span
class="math inline"><em>n</em><sub><em>i</em></sub>(<em>i</em> = 1, …, <em>N</em> = |<em>S</em>|)</span>
描述集合<span
class="math inline"><em>S</em></span>中的元素，然后定义Hamilton： <span
class="math display">$$\begin{align}
H=A\left(\sum_{i=1}^N n_i s_i\right)^2
\end{align}$$</span> 其中<span
class="math inline"><em>s</em><sub><em>i</em></sub> = ±1</span>
是自旋取值，其中<span
class="math inline"><em>A</em> &gt; 0</span>是正约束条件，通常会被取为1,但是保留这个参数可以区分不同能量尺度，<a
href="https://academic.oup.com/book/6337?login=true">参考文献</a>。</p>
<p>该Hamilton的描述是显然的，<span
class="math inline"><em>s</em></span>用于区分两类，<span
class="math inline"><em>n</em></span>表示值，如果两类值相等，那么<span
class="math inline"><em>H</em> = 0</span>。同时也说明，<span
class="math inline"><em>H</em></span>越低划分越好。</p>
<h2 id="graph-partitioning">Graph Partitioning</h2>
<p><a
href="https://iopscience.iop.org/article/10.1088/0305-4470/19/9/033/meta">图分割</a>是最初进行统计物理与NP问题关联的例子。问题描述如下：</p>
<p>考虑图<span
class="math inline"><em>G</em> = (<em>V</em>, <em>E</em>)</span>，有偶数个节点<span
class="math inline"><em>N</em> = |<em>V</em>|</span>，需要解决如何将集合<span
class="math inline"><em>V</em></span>分割成大小相等的两个子集，每个子集的大小为<span
class="math inline"><em>N</em>/2</span>，同时使得连接这两个子集的边数最小化？</p>
<p>将这个问题转化为Ising模型，放置自旋<span
class="math inline"><em>s</em><sub><em>v</em></sub> = ±1</span>在节点<span
class="math inline"><em>v</em> ∈ <em>V</em></span>，并且<span
class="math inline">±1</span>表示两类。设计Hamilton为：</p>
<p><span class="math display">$$\begin{align}
H=&amp;H_A+H_B \\
H_A=&amp;A\left(\sum_{n=1}^N s_i\right)^2 \\
H_B=&amp;B \sum_{(u v) \in E} \frac{1-s_u s_v}{2}
\end{align}$$</span></p>
<p>当<span class="math inline"><em>A</em> &gt; 0</span>时候，<span
class="math inline"><em>H</em><sub><em>A</em></sub></span>是对两个集合数量不相等惩罚项目，相差越大<span
class="math inline"><em>H</em><sub><em>A</em></sub></span>越大；当<span
class="math inline"><em>B</em> &gt; 0</span>时候，<span
class="math inline"><em>H</em><sub><em>B</em></sub></span>是对边数量的惩罚，连接数量越多<span
class="math inline"><em>H</em><sub><em>B</em></sub></span>越大。</p>
<h2 id="cliques">Cliques</h2>
<p>Cliques 问题描述如下：</p>
<p>在无向图<span
class="math inline"><em>G</em> = (<em>V</em>, <em>E</em>)</span>中，大小为<span
class="math inline"><em>K</em></span>的团簇（clique）是一个顶点子集<span
class="math inline"><em>W</em> ⊆ <em>V</em></span>，其大小为<span
class="math inline">|<em>W</em>| = <em>K</em></span>，使得子图<span
class="math inline">(<em>W</em>, <em>E</em><sub><em>W</em></sub>)</span>（其中<span
class="math inline"><em>E</em><sub><em>W</em></sub></span>是将边集<span
class="math inline"><em>E</em></span>限制在<span
class="math inline"><em>W</em></span>中节点之间的边上）是一个完全图。即图中所有可能的<span
class="math inline">$\frac{K(K -
1)}{2}$</span>条边都存在，因为团簇中的每个顶点都与其他每个顶点相连。简单描述为给定图<span
class="math inline"><em>G</em></span>和顶点数<span
class="math inline"><em>K</em></span>，能否从<span
class="math inline"><em>G</em></span>中选取<span
class="math inline"><em>K</em></span>个顶点构成完全图。</p>
<p>将这个问题转化为Ising模型，放置自旋<span
class="math inline"><em>s</em><sub><em>v</em></sub> = ±1</span>在节点<span
class="math inline"><em>v</em> ∈ <em>V</em></span>。通常将<span
class="math inline"><em>s</em><sub><em>α</em></sub></span>修改取值范围为<span
class="math inline"><em>x</em><sub><em>α</em></sub></span>： <span
class="math display">$$
x_\alpha \equiv \frac{s_\alpha+1}{2}
$$</span> 将Hamliton写为： <span class="math display">$$\begin{align}
H=A\left(K-\sum_v x_v\right)^2+B\left[\frac{K(K-1)}{2}-\sum_{(u v) \in
E} x_u x_v\right]
\end{align}$$</span> 其中<span
class="math inline"><em>A</em>, <em>B</em> &gt; 0</span>，在基态的时候<span
class="math inline"><em>H</em> = 0</span>。第一项要求满足节点数量为<span
class="math inline"><em>K</em></span>，第二项要求这些节点构成完全图。</p>
<p>对于<span
class="math inline"><em>H</em> ≠ 0</span>的其他情况，该问题可以转化为：
<span class="math display">$$\begin{align}
H_{\min }(n)=&amp;A(n-K)^2+B \frac{K(K-1)-n(n-1)}{2} \\
=&amp;(n-K)\left[A(n-K)-B \frac{n+K-1}{2}\right]
\end{align}$$</span></p>
<p>另一方面，需要寻找最大的
Cliques。同时原文中也提出了将该问题规模从<span
class="math inline"><em>N</em></span> 减少到 <span
class="math inline">log <em>N</em></span>的方案。</p>
<h1 id="binary-integer-linear-programming">Binary Integer Linear
Programming</h1>
<p>令<span
class="math inline"><em>x</em><sub>1</sub>, …, <em>x</em><sub><em>N</em></sub></span>
是 <span class="math inline"><em>N</em></span> 个二值变量，表示为<span
class="math inline"><strong>x</strong></span>。The binary integer linear
programming (ILP) 问题定义为： <span class="math display">$$
\begin{aligned}
&amp; \max _x \sum_{j=1}^n c_j x_j \\
&amp; \text { s.t. } \sum_{j=1}^n a_j x_j = b \\
&amp; x \in\{0,1\}^n \\
&amp;
\end{aligned}
$$</span></p>
<p>Ising Hamiltonian <span
class="math inline"><em>H</em> = <em>H</em><sub><em>A</em></sub> + <em>H</em><sub><em>B</em></sub></span>，可写为：
<span class="math display">$$\begin{align}
H_A=&amp;A \sum_{j=1}^m\left[b_j-\sum_{i=1}^N S_{j i} x_i\right]^2 \\
H_B=&amp;-B \sum_{i=1}^N c_i x_i
\end{align}$$</span> 其中 <span
class="math inline"><em>A</em> &gt; 0</span>，<span
class="math inline"><em>H</em><sub><em>A</em></sub></span>将约束转化为惩项；
<span
class="math inline">0 &lt; <em>B</em> ≪ <em>A</em></span>，为优化目标。</p>
<h1 id="covering-and-packing-problems">Covering and Packing
Problems</h1>
<h1 id="problems-with-inequalities">Problems with Inequalities</h1>
<h1 id="coloring-problems">Coloring Problems</h1>
<h1 id="hamiltonian-cycles">Hamiltonian Cycles</h1>
<h1 id="tree-problems">Tree Problems</h1>
<h1 id="graph-isomorphisms">Graph Isomorphisms</h1>
<h1 id="conclusions">Conclusions</h1>
<p>这篇文章聚焦于如何将 NP-complete/hard problems
转化为可以利用Ising类型Hamilton可表示的形式。</p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>NP problems</tag>
        <tag>Ising</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Network Diffusion</title>
    <url>/2024/03/04/DL/Neural_Network_Diffusion/Neural_Network_Diffusion/</url>
    <content><![CDATA[<h1 id="introduction">Introduction</h1>
<p>这篇文章的主要任务是，利用Diffusion
Model生成具备高表现能力的神经网络参数。</p>
<p>利用 autoencoder 和 laten diffusion model
两个主要部件，其中autoencoder将网络参数进行提取，diffusion
model再进行训练，然后再将训练好的模型进行解码。</p>
<span id="more"></span>
<p>将神经网络的训练与扩散逆过程进行对比： 1.
都是从随机到一个特定的分布。 2.
高质量的图像与具备很好表现的网络参数，可以退化为一个简单分布。</p>
<figure>
<img src="./fig1.png" alt="diffusion model VS model parameters" />
<figcaption aria-hidden="true">diffusion model VS model
parameters</figcaption>
</figure>
<p>本文提出parameter generation, 叫做 neural network diffusion (p-diff,
p stands for parameter)。</p>
<p>首先用已经训练好的模型参数，训练自编码器。 然后用标准latent diffusion
model 从随机噪声中合成编码后的表示。最后，利用训练完的diffusion
model，通过已经训练过的自编码器去生成新的参数。</p>
<h1 id="nerual-network-diffusion">Nerual Network Diffusion</h1>
<h2 id="preliminaries-of-diffusion-models">Preliminaries of diffusion
models</h2>
<figure>
<img src="./NND.png" alt="Train LDM" />
<figcaption aria-hidden="true">Train LDM</figcaption>
</figure>
<p>前向过程在不断的增加噪声，后向过程在猜测噪声分布。训练目标是计算分布的KL散度，尽可能的小。</p>
<p>上图中，LDM表示latent diffusion
model，训练编码后的表示。前向过程为：从已经表示的分布增加噪声，逐步成为随机噪声分布；后向过程为，从随机噪声出发逐步预测噪声进行去噪，得到原始的表达。</p>
<h2 id="parameter-autoencoder">Parameter autoencoder</h2>
<figure>
<img src="./autoencoder.png" alt="AutoEncoder" />
<figcaption aria-hidden="true">AutoEncoder</figcaption>
</figure>
<p>利用训练好的数据，进行编码训练。</p>
<h2 id="inference">Inference</h2>
<figure>
<img src="./inference.png" alt="Inference" />
<figcaption aria-hidden="true">Inference</figcaption>
</figure>
<p>使用DDPM进行优化。</p>
<h1 id="result">Result</h1>
<figure>
<img src="./result.png" alt="result" />
<figcaption aria-hidden="true">result</figcaption>
</figure>
<p>从结果来看部分数据集上最优，其它大部分也与最优表现持平。</p>
<p>该模型是否仅仅记住了训练好的模型参数，而非生成了新的模型参数？从结果来看，生成的模型要高于输入训练的模型，可见其是学习到了一些内容。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Diffusion Model</tag>
      </tags>
  </entry>
  <entry>
    <title>Boltzmann Machine</title>
    <url>/2022/11/30/Phys/Boltzmann_machine_phy/Boltzmann_machine_phy/</url>
    <content><![CDATA[<p>波尔兹曼机是物理模型，同时也是早期的机器学习模型。</p>
<p>Reference: * <a
href="https://campus.swarma.org/course/4543/study">神经网络的统计力学 –
受限制玻尔兹曼机的统计力学</a></p>
<span id="more"></span>
<h1 id="boltzmann-machine">Boltzmann Machine</h1>
<p><span id="fig1"/></p>
<figure>
<img src="./BMwithRBM.png" alt="BM with RBM" />
<figcaption aria-hidden="true">BM with RBM</figcaption>
</figure>
<p>上图(a)为玻尔兹曼机，类似于Ising模型，但是任意两个节点之间均含有连接。
<span class="math display">$$\begin{align}
E(\boldsymbol{\sigma})=-\sum_i h_i \sigma_i-\sum_{i&lt;j} w_{i j}
\sigma_i \sigma_j \label{Ising}
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>σ</em><sub><em>i</em></sub> = ±1</span>是节点<span
class="math inline"><em>i</em></span>的取值，<span
class="math inline"><em>w</em><sub><em>i</em><em>j</em></sub></span>是任意两个节点之间的链接权重，<span
class="math inline"><em>h</em><sub><em>i</em></sub></span>为外磁场项（可以理解为偏置项）。构型的波尔兹曼分布为：
<span class="math display">$$\begin{align}
p(\boldsymbol{\sigma})=\frac{1}{Z} e^{-\beta E(\sigma)}
\end{align}$$</span> 其中<span
class="math inline"><em>Z</em> = ∑<sub><em>σ</em></sub><em>e</em><sup>−<em>β</em><em>E</em>(<em>σ</em>)</sup></span>为配分函数。给定数据，数据表现形式通过<span
class="math inline"><em>σ</em></span>表现，目标是设计权重<span
class="math inline"><em>w</em><sub><em>i</em><em>j</em></sub></span>，使得对应构型的概率分布（波尔兹曼分布）较大；类似与Hopfield模型，可以通过输入部分构型，还原出整体的构型。这个任务本质上就是逆
Ising 问题（Inverse Ising Problem）。</p>
<p>为了讨论方便，这里假设 <span
class="math inline"><em>h</em><sub><em>i</em></sub> = 0  ∀<em>i</em></span>。当然，比较直接的权重设计方案是
Hebbian rule，但是这个并不是最优的结果。以下使用极大似然估计方法。</p>
<p><span class="math display">$$
\begin{align}
L(\boldsymbol{\theta} \mid\{\boldsymbol{\sigma}\}) &amp;
=\left\langle\log
\left(p_{\boldsymbol{\theta}}(\boldsymbol{\sigma})\right)\right\rangle_{\text
{data }} \\
&amp; =-\langle E(\boldsymbol{\sigma},
\boldsymbol{\theta})\rangle_{\text {data }}-\log Z(\boldsymbol{\theta})
\\
&amp; =\sum_{i=1}^N h_i\left\langle\sigma_i\right\rangle_{\text {data
}}+\sum_{i&lt;j} w_{i j}\left\langle\sigma_i
\sigma_j\right\rangle_{\text {data }}-\log Z(\boldsymbol{\theta})
\end{align}
$$</span></p>
<p>其中<span class="math inline">⟨…⟩<sub>data </sub></span>
表示对数据求平均，<span class="math inline"><strong>θ</strong></span>
表示模型参数 <span
class="math inline">{<strong>W</strong>, <strong>h</strong>}</span>，计算<span
class="math inline"><em>L</em>(<strong>θ</strong> ∣ {<strong>σ</strong>})</span>
梯度：</p>
<p><span class="math display">$$
\begin{align}
\frac{\partial L}{\partial h_i} &amp;
=\left\langle\sigma_i\right\rangle_{\text {data
}}-\left\langle\sigma_i\right\rangle_{\text {model }}  \\
\frac{\partial L}{\partial w_{i j}} &amp; =\left\langle\sigma_i
\sigma_j\right\rangle_{\text {data }}-\left\langle\sigma_i
\sigma_j\right\rangle_{\text {model }}
\end{align}
$$</span></p>
<p>其中<span class="math inline">⟨…⟩<sub>model </sub></span>
表示模型平均，表示的模型的热力学平均，是一个加权平均，穷举计算法耗时太长，直接计算是不现实的，可以认为是一个重要性抽样，从而使用蒙卡的方案计算。模型参数的梯度更新可以写为：</p>
<p><span class="math display">$$\begin{align}
\Delta h_i &amp; =\eta \frac{\partial L}{\partial
h_i}=\eta\left(\left\langle\sigma_i\right\rangle_{\text {data
}}-\left\langle\sigma_i\right\rangle_{\mathrm{model}}\right)  \\
\Delta w_{i j} &amp; =\eta \frac{\partial L}{\partial w_{i
j}}=\eta\left(\left\langle\sigma_i \sigma_j\right\rangle_{\text {data
}}-\left\langle\sigma_i \sigma_j\right\rangle_{\mathrm{model}}\right)
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>η</em></span>表示学习率。第一部分是数据的平均值，可以通过对数据进行平均计算；第二部分是模型的平均值，这部分计算如之前的描述，只能采用MCMC方法等重要性抽样方法才能计算（两点关联可以转化为点<span
class="math inline">$\frac{\partial\left\langle\sigma_i\right\rangle_{\text
{model }}}{\partial h_j}=\left\langle\sigma_i
\sigma_j\right\rangle_{\text {model
}}-\left\langle\sigma_i\right\rangle_{\text {model
}}\left\langle\sigma_j\right\rangle_{\text {model
}}$</span>），然而由于模型的<span
class="math inline"><em>h</em><sub><em>j</em></sub></span>与<span
class="math inline"><em>w</em><sub><em>i</em><em>j</em></sub></span>每一步都会发生变化，意味着每一次更新参数就需要重新进行一次重要性抽样，这显然是不能接受的。</p>
<p>如果存在更高关联，例如三阶：</p>
<p><span class="math display">$$\begin{align}
E(\boldsymbol{\sigma})=-\sum_i h_i \sigma_i-\sum_{i&lt;j} w_{i j}
\sigma_i \sigma_j-\sum_{i&lt;j&lt;k}\kappa_{ijk}\sigma_i\sigma_j\sigma_k
\end{align}$$</span></p>
<p>如何将这些关联以另一种形式展现，从而避免计算上的复杂性，这里引入一个隐变量<span
class="math inline"><em>s</em></span>。</p>
<p><span id="fig2"/></p>
<figure>
<img src="./hidden.png" alt="Hidden" />
<figcaption aria-hidden="true">Hidden</figcaption>
</figure>
<p><span class="math display">$$\begin{align}
E(\boldsymbol{\sigma},s)=-\sum_i h_i \sigma_i-\sum_{i&lt;j} w_{i j}
\sigma_i \sigma_j-s\sum_{i}\sigma_i k_i
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>k</em><sub><em>i</em></sub></span>表示<span
class="math inline"><em>σ</em><sub><em>i</em></sub></span>与隐变量之间的强度。可知其波尔兹曼分布为：</p>
<p><span class="math display">$$\begin{align}
P(\boldsymbol{\sigma},s) &amp;\sim
\exp\left(E(\boldsymbol{\sigma},s)\right) \\
P(\boldsymbol{\sigma})
&amp;=\sum_{s}\exp\left(E(\boldsymbol{\sigma},s)\right) \\
&amp;\sim \exp\left(-\sum_i h_i \sigma_i-\sum_{i&lt;j} w_{i j} \sigma_i
\sigma_j\right)2\cosh\left(\sum_i\sigma_i k_i\right) \\
&amp;= \exp\left(-\sum_i h_i \sigma_i-\sum_{i&lt;j} w_{i j} \sigma_i
\sigma_j\right)\exp\left( \ln\left[ 2\cosh\left(\sum_i\sigma_i
k_i\right) \right] \right)
\end{align}$$</span></p>
<p>可以将<span
class="math inline">ln [2cosh (∑<sub><em>i</em></sub><em>σ</em><sub><em>i</em></sub><em>k</em><sub><em>i</em></sub>)]</span>展开，从而得到高阶项:
<span class="math display">$$
\log (2 \cosh (x))=\log
   (2)+\frac{x^2}{2}-\frac{x^4}{12}+O\left(x^6\right)
$$</span></p>
<p>上面只是引入一个隐变量，更普遍的是具有更多隐变量<a
href="#fig1">图中(b)</a>，这样会使得模型表达能力提升。对应的能量为：</p>
<p><span class="math display">$$\begin{align}
E(\boldsymbol{\sigma},s)=-\sum_i h_i \sigma_i-\sum_{i&lt;j} w_{i j}
\sigma_i \sigma_j-\sum_{i,a}k_{ia}\sigma_i
s_a-\sum_{a&lt;b}\gamma_{ab}s_a s_b
\end{align}$$</span></p>
<p>需要学习的模型参数除了<span
class="math inline">$\eqref{Ising}$</span>中提到的<span
class="math inline"><em>w</em></span>与<span
class="math inline"><em>h</em></span>以外，还需要学习<span
class="math inline"><em>k</em><sub><em>a</em><em>b</em></sub></span>与<span
class="math inline"><em>γ</em><sub><em>a</em><em>b</em></sub></span>两个参数：</p>
<p><span class="math inline">$\begin{aligned}
&amp; \Delta k_{a b}=\eta\left(\left\langle\sigma_i
s_a\right\rangle_{\text {data }}-\left\langle\sigma_i
s_a\right\rangle_{\text {model }}\right) \\
&amp; \Delta \gamma_{a b}=\eta\left(\left\langle s_a
s_b\right\rangle_{\text {data }}-\left\langle s_a
s_b\right\rangle_{\text {model }}\right)
\end{aligned}$</span></p>
<p>由于<span
class="math inline"><em>s</em></span>是隐变量，并不能直接直接得到，但是可以得到其概率值，例如通过
Sigmoid 函数： <span class="math display">$$\begin{align}
P(s_a=1|\mathbf{\sigma})=\text{sigmoid}\left(
-\sum_{i,a}k_{ia}\sigma_i-\sum_{a&lt;b}\gamma_{ab}s_b \right)
\end{align}$$</span></p>
<p>可以看到上面的计算过程中使用到了其它隐变量<span
class="math inline"><em>s</em></span>，这里不得不进行近似，使用伯努力采样赋予<span
class="math inline"><em>s</em></span>值；依据这个过程将所有的隐变量概率以及具体的值计算出来。</p>
<p>可以看到在加入隐变量值后，计算CD散度需要更多的蒙卡模拟从而得到模型平均。这是一个十分费时的过程。</p>
<h1 id="restricted-boltzmann-machine">Restricted Boltzmann Machine</h1>
<p>为了解决计算模型平均花费很多时间，将一些关联项进行省略，包含可见变量之间的相互作用以及隐变量之间的相互作用项，将这个模型称之为受限玻尔兹曼机。</p>
<p><span id="fig3"/></p>
<figure>
<img src="./RBM.png" alt="BM with RBM" />
<figcaption aria-hidden="true">BM with RBM</figcaption>
</figure>
<p><span class="math display">$$\begin{align}
E(\boldsymbol{\sigma}, \boldsymbol{s})=-\sum_{i, a} \sigma_i w_{i a}
s_a-\sum_i \phi_i \sigma_i-\sum_a h_a s_a
\end{align}$$</span></p>
<p>其中<span class="math inline"><em>σ</em></span>与<span
class="math inline"><em>s</em></span>为可见变量与隐变量，其余参数为相互作用强度与偏置项。波尔兹曼分布为：</p>
<p><span class="math display">$$\begin{align}
p(\boldsymbol{\sigma}, \boldsymbol{s})=\frac{1}{Z} e^{-\beta
E(\boldsymbol{\sigma}, s)}
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>Z</em> = ∑<sub><em>σ</em>, <em>s</em></sub><em>e</em><sup>−<em>β</em><em>E</em>(<em>σ</em>, <em>s</em>)</sup></span>，并且假设<span
class="math inline"><em>β</em> = 1</span>，将其作用吸收进相互作用参数中。</p>
<p>将相同类型的节点之间的相互作用断开，好处在于可以实现近独立分布。只需要知道可见节点（隐藏节点）就可以推断出隐藏节点（可见节点）的概率分布，相同类型节点之间是独立的。</p>
<p><span class="math display">$$
\begin{align}
p\left(\sigma_i \mid \boldsymbol{s}\right) &amp;
=\frac{\sum_{\left\{\sigma_j: j \neq i\right\}} p(\boldsymbol{\sigma},
\boldsymbol{s})}{\sum_\sigma p(\boldsymbol{\sigma}, \boldsymbol{s})} \\
&amp; =\frac{e^{\sigma_i\left(\phi_i+\sum_a w_{i a}
s_a\right)}}{e^{\sigma_i\left(\phi_i+\sum_a w_{i a}
s_a\right)}+e^{-\sigma_i\left(\phi_i+\sum_a w_{i a} s_a\right)}} \\
&amp; =\frac{1}{1+e^{-2 \sigma_i\left(\phi_i+\sum_a w_{i a} s_a\right)}}
\\
p\left(s_a \mid \boldsymbol{\sigma}\right) &amp;
=\frac{\sum_{\left\{s_b: b \neq a\right\}} p(\boldsymbol{\sigma},
\boldsymbol{s})}{\sum_s p(\boldsymbol{\sigma}, \boldsymbol{s})} \\
&amp; =\frac{e^{s_a\left(h_a+\sum_i w_{i a}
\sigma_i\right)}}{e^{s_a\left(h_a+\sum_i w_{i a}
\sigma_i\right)}+e^{-s_a\left(h_a+\sum_i w_{i a} \sigma_i\right)}} \\
&amp; =\frac{1}{1+e^{-2 s_a\left(h_a+\sum_i w_{i a} \sigma_i\right)}}
\end{align}
$$</span></p>
<p>计算权重的方法与玻尔兹曼机类似，同样使用极大似然估计。</p>
<p><span class="math display">$$\begin{align}
\mathcal{L}(\boldsymbol{\theta} \mid\{\boldsymbol{\sigma}\}) &amp;
=\left\langle\log
\left(p_{\boldsymbol{\theta}}(\boldsymbol{\sigma})\right)\right\rangle_{\text
{data }} \\
&amp; =-\langle E(\boldsymbol{\sigma},
\boldsymbol{\theta})\rangle_{\text {data }}-\log
Z(\{\boldsymbol{\theta}\})
\end{align}$$</span></p>
<p><span class="math inline"><strong>θ</strong></span> 表示参数 <span
class="math inline">{<strong>W</strong>, <strong>ϕ</strong>, <strong>h</strong>}</span>，</p>
<p>参数的梯度为：</p>
<p><span class="math display">$$
\begin{align}
&amp; \frac{\partial \mathcal{L}\left(\left\{w_{i a}, \phi_i,
h_a\right\}\right)}{\partial w_{i a}}=\left\langle\sigma_i
s_a\right\rangle_{\text {data }}-\left\langle\sigma_i
s_a\right\rangle_{\text {model }} \\
&amp; \frac{\partial \mathcal{L}\left(\left\{w_{i a}, \phi_i,
h_a\right\}\right)}{\partial
\phi_i}=\left\langle\sigma_i\right\rangle_{\text {data
}}-\left\langle\sigma_i\right\rangle_{\text {model }} \\
&amp; \frac{\partial \mathcal{L}\left(\left\{w_{i a}, \phi_i,
h_a\right\}\right)}{\partial h_a}=\left\langle s_a\right\rangle_{\text
{data }}-\left\langle s_a\right\rangle_{\text {model }}
\end{align}
$$</span></p>
<h1 id="free-energy-calculation">Free Energy Calculation</h1>
<p><span class="math display">$$\begin{align}
p(\boldsymbol{\sigma}) &amp; =\sum_s p(\boldsymbol{\sigma},
\boldsymbol{s}) \\
&amp; =\frac{1}{Z} \sum_s e^{\sum_a\left(\sum_i \beta \sigma_i w_{i
a}+\beta h_a\right) s_a+\sum_i \beta \sigma_i \phi_i} \\
&amp; =\frac{1}{Z} e^{\sum_i \beta \sigma_i \phi_i} \sum_s \prod_a
e^{\left(\sum_i \beta \sigma_i w_{i a}+\beta h_a\right) s_a} \\
&amp; =\frac{1}{Z} \prod_i e^{\beta \sigma_i \phi_i} \prod_a \sum_{s_a}
e^{\left(\sum_i \beta \sigma_i w_{i a}+\beta h_a\right) s_a} \\
&amp; =\frac{1}{Z} \prod_i e^{\beta \sigma_i \phi_i} \prod_a\left[2
\cosh \left(\beta \boldsymbol{w}_a \boldsymbol{\sigma}+\beta
h_a\right)\right],
\end{align}$$</span></p>
<p><font color='green'>可见cavity
计算是将复杂计算转化为因子图迭代的过程，这是一种巧妙的近似，它的适用范围不限于计算自由能。</font></p>
<p>画出因子图： <img src="./cavity.png" alt="factor graph" /></p>
<p>factor node表示<span
class="math inline">2cosh (<em>β</em><strong>w</strong><sub><strong>a</strong></sub><strong>σ</strong> + <em>β</em><em>h</em><sub><em>a</em></sub>)</span>，variable
node 表示<span
class="math inline"><em>σ</em><sub><em>i</em></sub></span>。可以得到cavity迭代方程为：</p>
<p><span class="math display">$$
\begin{align}
&amp; P_{i \rightarrow a}\left(\sigma_i\right)=\frac{1}{Z_{i \rightarrow
a}} e^{\phi_i \sigma_i} \prod_{b \in \partial i \backslash a} \mu_{b
\rightarrow i}\left(\sigma_i\right) ; \\
&amp; \mu_{b \rightarrow i}\left(\sigma_i\right)=\sum_{\left\{\sigma_j
\mid j \in \partial b \backslash i\right\}} 2 \cosh
\left(\boldsymbol{w}_b \sigma+h_b\right) \prod_{j \in \partial b
\backslash i} P_{j \rightarrow b}\left(\sigma_j\right),
\end{align}
$$</span></p>
<p>其中 <span
class="math inline"><em>Z</em><sub><em>i</em> → <em>a</em></sub> = <em>e</em><sup><em>ϕ</em><sub><em>i</em></sub></sup>∏<sub><em>b</em> ∈ ∂<em>i</em> ∖ <em>a</em></sub><em>μ</em><sub><em>b</em> → <em>i</em></sub>(+1) + <em>e</em><sup>−<em>ϕ</em><sub><em>i</em></sub></sup>∏<sub><em>b</em> ∈ ∂<em>i</em> ∖ <em>a</em></sub><em>μ</em><sub><em>b</em> → <em>i</em></sub>(−1)</span>，虽然已经近似处理，但是求解这个迭代方程需要<span
class="math inline"><em>O</em>(2<sup><em>N</em> − 1</sup>)</span>的时间复杂度，因此还需要进一步近似。</p>
<p>定义 <span
class="math inline">𝒰<sub><em>b</em> → <em>i</em></sub> ≡ ∑<sub><em>j</em> ∈ ∂<em>b</em> ∖ <em>i</em></sub><em>w</em><sub><em>j</em><em>b</em></sub><em>σ</em><sub><em>j</em></sub></span>，根据中心极限定理（central
limit theorem (CLT)）在<span class="math inline"><em>N</em></span>
较大的情况下 <span
class="math inline">𝒰<sub><em>b</em> → <em>i</em></sub></span>应该服从高斯分布，因此<span
class="math inline">𝒰<sub><em>b</em> → <em>i</em></sub></span>的均值和方差写为：</p>
<p><span class="math display">$$
\begin{align}
G_{b \rightarrow i} &amp; =\left\langle\mathcal{U}_{b \rightarrow
i}\right\rangle_{\left\{\sigma_j \mid j \in \partial b \backslash
i\right\}}=\sum_{j \in \partial b \backslash i} w_{j b} m_{j \rightarrow
b}  \label{G}\\
\Xi_{b \rightarrow i}^2 &amp; =\left\langle\mathcal{U}_{b \rightarrow
i}^2\right\rangle_{\left\{\sigma_j \mid j \in \partial b \backslash
i\right\}}-\left\langle\mathcal{U}_{b \rightarrow
i}\right\rangle_{\left\{\sigma_j \mid j \in \partial b \backslash
i\right\}}^2 \\
&amp; \simeq \sum_{j \in \partial b \backslash i} w_{j b}^2\left(1-m_{j
\rightarrow b}^2\right)\\
m_{j \rightarrow b} &amp;\equiv \sum_{\sigma_j} \sigma_j P_{j
\rightarrow b}\left(\sigma_j\right)
\end{align}
$$</span></p>
<p>因此 <span
class="math inline"><em>μ</em><sub><em>b</em> → <em>i</em></sub>(<em>σ</em><sub><em>i</em></sub>)</span>
利用高斯积分可以近似写为：</p>
<p><span class="math display">$$
\begin{aligned}
\mu_{b \rightarrow i}\left(\sigma_i\right) &amp; =2 \int D t \cosh
\left(G_{b \rightarrow i}+\sqrt{\Xi_{b \rightarrow i}^2} t+h_b+w_{i b}
\sigma_i\right) \\
&amp; =2 e^{\frac{\Xi_{b \rightarrow i}^2}{2}} \cosh \left(G_{b
\rightarrow i}+h_b+w_{i b} \sigma_i\right),
\end{aligned}
$$</span></p>
<p>其中 <span class="math inline">$D t \equiv e^{-t^2 / 2} / \sqrt{2
\pi} d t$</span>。接下来计算<span
class="math inline"><em>m</em><sub><em>j</em> → <em>b</em></sub></span>即变量节点的概率：</p>
<p><span class="math display">$$
\begin{align}
m_{j \rightarrow b} &amp; =\sum_{\sigma_j} \sigma_j P_{j \rightarrow
b}\left(\sigma_j\right) \\
&amp; =\frac{\sum_{\sigma_i} \sigma_i e^{\phi_i \sigma_i} \prod_{b \in
\partial i \backslash a} \mu_{b \rightarrow
i}\left(\sigma_i\right)}{\sum_{\sigma_i} e^{\phi_i \sigma_i} \prod_{b
\in \partial i \backslash a} \mu_{b \rightarrow i}\left(\sigma_i\right)}
\label{10.15}\\
&amp; =\tanh \left(\phi_i+\sum_{b \in \partial i \backslash a} u_{b
\rightarrow i}\right) ; \\
u_{b \rightarrow i} &amp; =\frac{1}{2} \ln \frac{\mu_{b \rightarrow
i}(+1)}{\mu_{b \rightarrow i}(-1)}=\frac{1}{2} \ln \frac{\cosh
\left(h_b+G_{b \rightarrow i}+w_{i b}\right)}{\cosh \left(h_b+G_{b
\rightarrow i}-w_{i b}\right)}\label{u}
\end{align}
$$</span></p>
<p>从<span
class="math inline">$\eqref{G},\eqref{10.15},\eqref{u}$</span>得到自洽迭代方程。</p>
<p>对于自由能为：</p>
<p><span class="math display">$$
\begin{align}
F &amp; =\sum_i F_i-(N-1) \sum_a F_a \\
F_i &amp; =-\ln Z_i=-\ln \left(e^{\phi_i} \prod_{b \in \partial i}
\mu_{b \rightarrow i}(+1)+e^{-\phi_i} \prod_{b \in \partial i} \mu_{b
\rightarrow i}(-1)\right) \\
F_a &amp; =-\ln Z_a=-\ln \left(2 e^{\frac{\Xi_a^2}{2}} \cosh
\left(G_a+h_a\right)\right)
\end{align}
$$</span></p>
<p><font color='red'>end</font></p>
<p>where . Inserting this result into the cavity probability <span
class="math inline"><em>P</em><sub><em>i</em> → <em>a</em></sub>(<em>σ</em><sub><em>i</em></sub>)</span>,
we obtain the cavity magnetization</p>
<p>where <span
class="math inline"><em>u</em><sub><em>b</em> → <em>i</em></sub></span>
is the cavity bias (see Chap. 2). <span
class="math inline"><em>m</em><sub><em>i</em> → <em>a</em></sub></span>
represents the massage passing from variable node <span
class="math inline"><em>i</em></span> to factor node <span
class="math inline"><em>a</em></span>, and <span
class="math inline"><em>u</em><sub><em>b</em> → <em>i</em></sub></span>
denotes the massage passing from factor node <span
class="math inline"><em>b</em></span> to variable node <span
class="math inline"><em>i</em></span>. Iterating Eq. (10.15) can reach
the fixed point. Then, the Bethe free energy can be calculated as
follows:</p>
<p>where <span
class="math inline"><em>F</em><sub><em>i</em></sub></span> and <span
class="math inline"><em>F</em><sub><em>a</em></sub></span> are local
free energies of variable node <span
class="math inline"><em>i</em></span> and factor node <span
class="math inline"><em>a</em></span>, respectively, <span
class="math inline"><em>Ξ</em><sub><em>a</em></sub> = ∑<sub><em>j</em> ∈ ∂<em>a</em></sub><em>w</em><sub><em>j</em><em>a</em></sub><sup>2</sup>(1 − <em>m</em><sub><em>j</em> → <em>a</em></sub><sup>2</sup>)</span>,
and <span
class="math inline"><em>G</em><sub><em>a</em></sub> = ∑<sub><em>j</em> ∈ ∂<em>a</em></sub><em>w</em><sub><em>j</em><em>a</em></sub><em>m</em><sub><em>j</em> → <em>a</em></sub></span>.
The computation of <span
class="math inline"><em>F</em><sub><em>a</em></sub></span> is similar to
that of <span
class="math inline"><em>μ</em><sub><em>a</em> → <em>i</em></sub></span>.
Here, we show an experiment result of the free energy computation via
the Bethe approximation (Fig. 10.4).</p>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Boltzmann Machine</tag>
        <tag>Replica Method</tag>
        <tag>Energy-based Model</tag>
      </tags>
  </entry>
  <entry>
    <title>Boltzmann Machine Append</title>
    <url>/2022/11/30/Phys/Boltzmann_machine_phy/append/</url>
    <content><![CDATA[<h1 id="逆ising问题及其求解方法">逆Ising问题及其求解方法</h1>
<h2 id="引言">引言</h2>
<p>逆Ising问题涉及从观测数据中推断Ising模型的参数。Ising模型是一种简单的数学模型，用于描述磁性材料中的自旋系统。解决逆Ising问题的方法包括极大似然估计（MLE）、蒙特卡罗方法（MCMC）、平均场近似、伪似然估计（PLE）以及对比散度（CD）。</p>
<h2 id="极大似然估计mle">极大似然估计（MLE）</h2>
<h3 id="步骤">步骤</h3>
<ol type="1">
<li><p><strong>定义Hamiltonian</strong>: <span
class="math display"><em>H</em>(<strong>s</strong>) = −∑<sub><em>i</em> &lt; <em>j</em></sub><em>J</em><sub><em>i</em><em>j</em></sub><em>s</em><sub><em>i</em></sub><em>s</em><sub><em>j</em></sub></span></p></li>
<li><p><strong>定义配分函数</strong>: <span
class="math display"><em>Z</em> = ∑<sub><strong>s</strong></sub><em>e</em><sup>−<em>H</em>(<strong>s</strong>)</sup></span></p></li>
<li><p><strong>定义似然函数</strong>: <span class="math display">$$
L(J) = \prod_{k=1}^{N} P(\mathbf{s}^{(k)}|J) = \prod_{k=1}^{N}
\frac{e^{-H(\mathbf{s}^{(k)})}}{Z}
$$</span></p></li>
<li><p><strong>取对数似然函数</strong>: <span class="math display">$$
\log L(J) = -\sum_{k=1}^{N} H(\mathbf{s}^{(k)}) - N \log Z
$$</span></p></li>
<li><p><strong>优化对数似然函数</strong>:
使用数值优化方法（如梯度下降）最大化对数似然函数。</p></li>
</ol>
<h3 id="示例代码">示例代码</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观测数据</span></span><br><span class="line">data = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>], [-<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Hamiltonian</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hamiltonian</span>(<span class="params">J, s</span>):</span><br><span class="line">    <span class="keyword">return</span> -J * s[<span class="number">0</span>] * s[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义配分函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">partition_function</span>(<span class="params">J</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * (np.exp(J) + np.exp(-J))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义对数似然函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_likelihood</span>(<span class="params">J, data</span>):</span><br><span class="line">    logL = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> data:</span><br><span class="line">        logL += -hamiltonian(J, s)</span><br><span class="line">    logL -= <span class="built_in">len</span>(data) * np.log(partition_function(J))</span><br><span class="line">    <span class="keyword">return</span> -logL  <span class="comment"># 最小化负对数似然</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始猜测</span></span><br><span class="line">initial_J = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化参数</span></span><br><span class="line">result = minimize(log_likelihood, initial_J, args=(data,))</span><br><span class="line">optimal_J = result.x[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;MLE Estimated J: <span class="subst">&#123;optimal_J&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="蒙特卡罗方法mcmc">蒙特卡罗方法（MCMC）</h2>
<h3 id="步骤-1">步骤</h3>
<ol type="1">
<li><strong>初始化模型参数</strong>。</li>
<li><strong>生成样本</strong>：
使用MCMC方法（如Metropolis-Hastings或Gibbs采样）生成自旋配置样本。</li>
<li><strong>计算统计量</strong>：
基于生成的样本，计算系统的期望统计量。</li>
<li><strong>更新参数</strong>：
使用梯度方法或其他优化技术调整模型参数。</li>
<li><strong>重复步骤2-4</strong>，直到参数收敛。</li>
</ol>
<h3 id="示例代码-1">示例代码</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观测数据</span></span><br><span class="line">data = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>], [-<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">J = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 吉布斯采样函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gibbs_sample</span>(<span class="params">J, num_samples, burn_in</span>):</span><br><span class="line">    samples = []</span><br><span class="line">    s = np.random.choice([-<span class="number">1</span>, <span class="number">1</span>], size=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples + burn_in):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">            prob = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-<span class="number">2</span> * J * s[<span class="number">1</span> - i]))</span><br><span class="line">            s[i] = <span class="number">1</span> <span class="keyword">if</span> np.random.rand() &lt; prob <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> _ &gt;= burn_in:</span><br><span class="line">            samples.append(s.copy())</span><br><span class="line">    <span class="keyword">return</span> np.array(samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算期望统计量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_statistics</span>(<span class="params">samples</span>):</span><br><span class="line">    correlations = np.mean(samples[:, <span class="number">0</span>] * samples[:, <span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> correlations</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">J, samples, data</span>):</span><br><span class="line">    data_corr = np.mean(data[:, <span class="number">0</span>] * data[:, <span class="number">1</span>])</span><br><span class="line">    sample_corr = calculate_statistics(samples)</span><br><span class="line">    learning_rate = <span class="number">0.1</span></span><br><span class="line">    J += learning_rate * (data_corr - sample_corr)</span><br><span class="line">    <span class="keyword">return</span> J</span><br><span class="line"></span><br><span class="line"><span class="comment"># 蒙特卡罗迭代</span></span><br><span class="line">num_iterations = <span class="number">100</span></span><br><span class="line">num_samples = <span class="number">1000</span></span><br><span class="line">burn_in = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">    samples = gibbs_sample(J, num_samples, burn_in)</span><br><span class="line">    J = update_parameters(J, samples, data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Iteration <span class="subst">&#123;_&#125;</span>: J = <span class="subst">&#123;J&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Estimated J: <span class="subst">&#123;J&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="平均场近似mean-field-approximation">平均场近似（Mean Field
Approximation）</h2>
<h3 id="步骤-2">步骤</h3>
<ol type="1">
<li><strong>定义Hamiltonian</strong>。</li>
<li><strong>引入平均场近似</strong>：
每个自旋的影响被近似为一个平均场。</li>
<li><strong>计算自旋期望值</strong>： 根据平均场计算自旋的期望值。</li>
<li><strong>迭代求解</strong>：
通过迭代更新每个自旋的期望值和相互作用参数。</li>
<li><strong>更新相互作用参数</strong>：
调整相互作用参数使得模型生成的期望值与观测数据的期望值匹配。</li>
</ol>
<h3 id="示例代码-2">示例代码</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观测数据</span></span><br><span class="line">data = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>], [-<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">J = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算观测数据的期望值</span></span><br><span class="line">mean_s1 = np.mean(data[:, <span class="number">0</span>])</span><br><span class="line">mean_s2 = np.mean(data[:, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义迭代函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mean_field_iteration</span>(<span class="params">J, mean_s1, mean_s2, max_iter=<span class="number">100</span>, tol=<span class="number">1e-5</span></span>):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        h1 = J * mean_s2</span><br><span class="line">        h2 = J * mean_s1</span><br><span class="line">        new_mean_s1 = np.tanh(h1)</span><br><span class="line">        new_mean_s2 = np.tanh(h2)</span><br><span class="line">        <span class="keyword">if</span> np.<span class="built_in">abs</span>(new_mean_s1 - mean_s1) &lt; tol <span class="keyword">and</span> np.<span class="built_in">abs</span>(new_mean_s2 - mean_s2) &lt; tol:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        mean_s1, mean_s2 = new_mean_s1, new_mean_s2</span><br><span class="line">    <span class="keyword">return</span> mean_s1, mean_s2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代求解期望值</span></span><br><span class="line">mean_s1, mean_s2 = mean_field_iteration(J, mean_s1, mean_s2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新相互作用参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">J, mean_s1, mean_s2, data</span>):</span><br><span class="line">    observed_corr = np.mean(data[:, <span class="number">0</span>] * data[:, <span class="number">1</span>])</span><br><span class="line">    model_corr = mean_s1 * mean_s2</span><br><span class="line">    learning_rate = <span class="number">0.1</span></span><br><span class="line">    J += learning_rate * (observed_corr - model_corr)</span><br><span class="line">    <span class="keyword">return</span> J</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新参数</span></span><br><span class="line">J = update_parameters(J, mean_s1, mean_s2, data)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Estimated J: <span class="subst">&#123;J&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="伪似然估计ple">伪似然估计（PLE）</h2>
<h3 id="步骤-3">步骤</h3>
<ol type="1">
<li><strong>定义条件概率</strong>： <span
class="math display">$$\begin{align}
P(s_i | \mathbf{s}_{\setminus i}) = \frac{e^{s_i h_i}}{e^{h_i} +
e^{-h_i}}
\end{align}$$</span></li>
<li><strong>定义伪似然函数</strong>： <span
class="math display">$$\begin{align}
PL(J) = \prod_{i=1}^n \prod_{k=1}^N P(s_i^{(k)} | \mathbf{s}_{\setminus
i}^{(k)})
\end{align}$$</span></li>
<li><strong>取对数伪似然函数</strong>： <span
class="math display">$$\begin{align}
\log PL(J) = \sum_{i=1}^n \sum_{k=1}^N \log P(s_i^{(k)} |
\mathbf{s}_{\setminus i}^{(k)})
\end{align}$$</span></li>
<li><strong>优化对数伪似然函数</strong>：
使用数值优化方法（如梯度下降）最大化对数伪似然函数。</li>
</ol>
<h3 id="示例代码-3">示例代码</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观测数据</span></span><br><span class="line">data = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>], [-<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义条件概率</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conditional_probability</span>(<span class="params">s_i, s_j, J</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-<span class="number">2</span> * J * s_i * s_j))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义对数伪似然函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_pseudolikelihood</span>(<span class="params">J, data</span>):</span><br><span class="line">    logPL = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> data:</span><br><span class="line">        logPL += np.log(conditional_probability(s[<span class="number">0</span>], s[<span class="number">1</span>], J)) + np.log(conditional_probability(s[<span class="number">1</span>], s[<span class="number">0</span>], J))</span><br><span class="line">    <span class="keyword">return</span> -logPL  <span class="comment"># 最小化负对数伪似然</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始猜测</span></span><br><span class="line">initial_J = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化参数</span></span><br><span class="line">result = minimize(log_pseudolikelihood, initial_J, args=(data,))</span><br><span class="line">optimal_J = result.x[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Optimal J: <span class="subst">&#123;optimal_J&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="对比散度cd">对比散度（CD）</h2>
<h3 id="步骤-4">步骤</h3>
<ol type="1">
<li><strong>定义能量函数</strong>： <span
class="math display">$$\begin{align}
E(\mathbf{s}) = -\sum_{i&lt;j} J_{ij} s_i s_j
\end{align}$$</span></li>
<li><strong>初始化模型参数</strong>。</li>
<li><strong>正向采样（Positive Phase）</strong>：
从观测数据中采样，计算数据分布下的期望值。</li>
<li><strong>负向采样（Negative Phase）</strong>：
使用Gibbs采样从模型分布中生成样本，计算模型分布下的期望值。</li>
<li><strong>更新参数</strong>：
根据正向采样和负向采样的期望值差异，更新相互作用参数。</li>
<li><strong>迭代</strong>，直到参数收敛。</li>
</ol>
<h3 id="示例代码-4">示例代码</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观测数据</span></span><br><span class="line">data = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>], [-<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">J = <span class="number">0.1</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">num_iterations = <span class="number">100</span></span><br><span class="line">num_samples = <span class="number">1000</span></span><br><span class="line">burn_in = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算观测数据的期望值</span></span><br><span class="line">mean_s1s2_data = np.mean(data[:, <span class="number">0</span>] * data[:, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 吉布斯采样函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gibbs_sample</span>(<span class="params">J, num_samples, burn_in</span>):</span><br><span class="line">    samples = []</span><br><span class="line">    s = np.random.choice([-<span class="number">1</span>, <span class="number">1</span>], size=<span class="number">2</span>)  <span class="comment"># 初始自旋配置</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples + burn_in):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">            prob = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-<span class="number">2</span> * J * s[<span class="number">1</span> - i]))</span><br><span class="line">            s[i] = <span class="number">1</span> <span class="keyword">if</span> np.random.rand() &lt; prob <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> _ &gt;= burn_in:</span><br><span class="line">            samples.append(s.copy())</span><br><span class="line">    <span class="keyword">return</span> np.array(samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算模型分布的期望值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_model_expectation</span>(<span class="params">J, num_samples, burn_in</span>):</span><br><span class="line">    samples = gibbs_sample(J, num_samples, burn_in)</span><br><span class="line">    mean_s1s2_model = np.mean(samples[:, <span class="number">0</span>] * samples[:, <span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> mean_s1s2_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对比散度更新</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">    mean_s1s2_model = calculate_model_expectation(J, num_samples, burn_in)</span><br><span class="line">    J += learning_rate * (mean_s1s2_data - mean_s1s2_model)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Iteration <span class="subst">&#123;_&#125;</span>: J = <span class="subst">&#123;J&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;CD Estimated J: <span class="subst">&#123;J&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2>
<p>逆Ising问题涉及从观测数据中推断Ising模型的相互作用参数。本文介绍了五种求解方法：极大似然估计（MLE）、蒙特卡罗方法（MCMC）、平均场近似、伪似然估计（PLE）以及对比散度（CD）。每种方法都有其优缺点，选择合适的方法取决于具体问题的规模、数据特性和计算资源。</p>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Boltzmann Machine</tag>
        <tag>Replica Method</tag>
        <tag>Energy-based Model</tag>
      </tags>
  </entry>
  <entry>
    <title>Annealing approach to root finding</title>
    <url>/2024/08/27/Phys/Annealing_root_finding/Annealing_root_finding/</url>
    <content><![CDATA[<p>在数值分析和科学计算中，Newton-Raphson方法是一个非常重要的工具，它被广泛用于求解方程的根。然而，经典的Newton-Raphson方法在面对复杂的非线性方程和多个根的情况下，可能会出现收敛性差、振荡或发散的情况。为了解决这些问题，研究者们提出了一种基于物理学启发的新方法，该方法在保留Newton-Raphson方法优点的同时，通过引入一个新的参数<span
class="math inline"><em>β</em></span>，有效提升了算法的收敛速度和稳定性。</p>
<p>Link: * <a
href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.110.025305">Annealing
approach to root finding</a></p>
<span id="more"></span>
<p>这项改进的灵感来源于物理学中的退火过程。退火是指材料在高温下加热，然后缓慢冷却，以达到其最低能量状态的过程。通过这种逐渐降温的方法，材料可以在高温下探索更多的状态空间，而在低温下则集中于更稳定的状态。</p>
<p>这种物理过程被引入到数值计算中，特别是用于求解非线性方程的根。传统的Newton-Raphson方法可以看作是高温下的大幅跳跃，能够快速找到接近根的位置，但在接近根时可能会失去效率。通过引入一个类似“温度”的参数<span
class="math inline"><em>β</em></span>，算法在初期可以进行广泛的搜索，而在接近根时逐渐收敛，使得算法既能快速搜索，又能稳定逼近根。</p>
<p>在经典的Newton-Raphson方法中，新的迭代点是通过以下公式计算的：</p>
<p><span class="math display">$$
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
$$</span></p>
<p>这种方法在接近根时会快速收敛，但远离根时可能会跳过根或收敛到错误的解。为了改进这一点，本文提出了一个改进的更新公式：</p>
<p><span class="math display">$$\begin{aligned}
&amp;
\hat{x}_{n+1}=x_n-\frac{f\left(x_n\right)}{f^{\prime}\left(x_n\right)}
\\ &amp; x_{n+1}=\hat{x}_{n+1}-\beta
\frac{f\left(\hat{x}_{n+1}\right)}{f^{\prime}\left(x_n\right)}
\end{aligned}$$</span></p>
<p>其中，<span
class="math inline"><em>x</em><sub><em>n</em> + 1</sub><sup>(<em>N</em><em>R</em>)</sup></span>是经典Newton-Raphson方法计算的中间值。通过引入参数<span
class="math inline"><em>β</em></span>，算法能够在早期（类似高温状态）进行较大的跳跃搜索，而在后期（类似低温状态）逐步逼近根。这种方法与Adomian’s
method有一定关联。</p>
<p>这个改进方法的核心在于<span
class="math inline"><em>β</em></span>值的动态调整。通过调整<span
class="math inline"><em>β</em></span>，算法可以在迭代过程中逐步从“高温”过渡到“低温”，从而实现从全局搜索到局部收敛的平衡。当<span
class="math inline"><em>β</em> = 0</span>时，算法等同于经典的Newton-Raphson方法，具有快速的搜索能力；当<span
class="math inline"><em>β</em> = 1</span>时，算法更精确地逼近根，收敛速度显著提高。</p>
<figure>
<img src="./fig1.png" alt="Improved convergence to roots" />
<figcaption aria-hidden="true">Improved convergence to
roots</figcaption>
</figure>
<p>从上图中可以发现收敛性得到提升。尤其对于右图这种具有振荡性质函数，通过加入<span
class="math inline"><em>β</em></span>利用函数本身的性质进行收敛。</p>
<figure>
<img src="./fig2.png" alt="Improved convergence to roots" />
<figcaption aria-hidden="true">Improved convergence to
roots</figcaption>
</figure>
<p>提升迭代速度。</p>
<p>这里能否联系<a href="/2024/08/26/DL/LossPlasticity/LossPlasticity/" title="Loss of plasticity in deep continual learning">Loss of plasticity in deep continual learning</a>文章中修改反向传播的想法。通过<span
class="math inline"><em>β</em></span>的退火机制，通过对一些参数遗忘从而实现连续学习，一个简单的想法是当学习新的内容时，意味着温度升高，一些参数可以重新调整，然后学习的过程伴随着温度的下降，当完后学习之后，收敛到一个基态。</p>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Computer Physics</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine-learning-assisted Monte Carlo fails at sampling computationally hard problems</title>
    <url>/2024/04/19/Phys/BoostMC_fail/BoostMC_fail/</url>
    <content><![CDATA[<p>展示利用机器学习提升的MC在一些模型中的失败： *
为什么说失败？哪些指标说明失败？ *
在怎样的模型中？这种模型具有什么样的特点？ * 实验条件是什么？</p>
<p>参考文献： * <a
href="https://doi.org/10.1088/2632-2153/acbe91">Machine-learning-assisted
Monte Carlo fails at sampling computationally hard problems</a>
配套代码<a
href="https://zenodo.org/records/7567683">10.5281/zenodo.7567683</a> *
<a href="https://doi.org/10.3390/condmat7020038">Neural Annealing and
Visualization of Autoregressive Neural Networks in the Newman–Moore
Model</a> * <a
href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.60.5068">Glassy
dynamics and aging in an exactly solvable spin model</a> * <a
href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.108.174107">Boundary
conditions dependence of the phase transition in the quantum
Newman-Moore mode</a> * <a
href="https://arxiv.org/abs/1712.09913">Visualizing the Loss Landscape
of Neural Nets</a></p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>经典MC的问题在于使用细致平衡条件进行局部更新，无法处理临界慢化、关联长度长问题；一些改进的措施是将局部更新改为全局更新，但是这种方案与模型的结构直接相关。</p>
<p>最近利用机器学习诞生了一些高效的方案，通过近似分布<span
class="math inline"><em>P</em>(<em>σ</em>)</span>来获得目标分布，并且进行高效采样。</p>
<p>现在面临的困境本质与90年代修改MC方案是一样的，两种都是在进行采样。在当时已经提出了一些benchmark对不同方案进行检验。但是现在机器学习策略大多聚焦于解决MC之前的困境，并没有测试之前的benchmark。</p>
<p>接下来考虑一些难以采样的随机问题。</p>
<h1 id="fails-at-sampling-computationally-hard-problems">Fails at
sampling computationally hard problems</h1>
<p>在采样问题中通常会遇到mode-collapse in learning the auxiliary
model，这是由于在多峰分布情况下模型只学习到其中的一个峰分布。</p>
<p>模型是否准确学到分布，作者提出了三个重要的判断指标： *
其采样的接受概率是否足够高 <span class="math display">$$
  \operatorname{Acc}\left[\sigma_{\text {old }} \rightarrow
\sigma_{\text {new }}\right]=\min \left[1,
\frac{\mathrm{e}^{-(\beta+\delta \beta) H\left(\sigma_{\text {new
}}\right)} P_{\mathrm{AR}}\left(\sigma_{\text {old
}}\right)}{\mathrm{e}^{-(\beta+\delta \beta) H\left(\sigma_{\text {old
}}\right) }P_{\mathrm{AR}}\left(\sigma_{\text {new }}\right)}\right]
  $$</span> *
全局MCMC动态初始化于由AR模型生成的配置接近静止状态（即，像能量这样的单时间数量在时间上是恒定的，而像相关性这样的双时间数量只依赖于时间差）。<font color='red'>不一定吧？</font>
* 时间依赖的关联项消失。</p>
<p><font color='red'>这个作者提出的benchmark主要针对VAN这种直接生成下一步分布的模型，但是如果模型只是估计分布概率，这个benchmark并不有效。</font></p>
<h2 id="coloring">Coloring</h2>
<p><font color='yellow'>该模型本质就是随机图上的Potts Model。</font></p>
<p>关于这个模型有一些参考资料：</p>
<ul>
<li><a
href="https://iopscience.iop.org/article/10.1209/0295-5075/81/57005">random
first order transition universality class</a></li>
<li><a
href="https://www.tandfonline.com/doi/full/10.1080/00018732.2016.1211393">Statistical
physics of inference: thresholds and algorithms Adv. Phys</a></li>
</ul>
<p>The hard-to-sample coloring problem: <span
class="math inline"><em>N</em></span>个变量<span
class="math inline"><em>σ</em><sub><em>i</em></sub> ∈ {0, …, <em>q</em> − 1}</span>，每一个有<span
class="math inline"><em>q</em></span>概率染色，节点位于Erdős-Rényi随机图<span
class="math inline">𝒢</span>上，每条边以相同的概率进行采样，连通概率为<span
class="math inline"><em>c</em></span>。将模型的Hamiltonian 写为： <span
class="math display">$$\begin{align}
H(\sigma)=\sum_{\langle i, j\rangle \in \mathcal{G}} \delta_{\sigma_i,
\sigma_j}
\end{align}$$</span> 对于一个<span
class="math inline"><em>q</em></span>态的Potts模型，这是一反铁磁模型，在这样的随机图的基态下如何选取使得能量最低，等价于随机图着色问题。</p>
<p><img src="./A_4.png"
alt="Coloring—Phase diagram of the coloring of Erdös-Rényi random graphs" />
<span
class="math inline"><em>T</em><sub><em>d</em></sub></span>线是玻璃相转变，在其之下关联时间随尺寸变大指数增加，<span
class="math inline"><em>T</em><sub><em>k</em></sub></span>是凝聚线，在其之下不存在平坦的基态。其中小图表示在<span
class="math inline"><em>c</em> = 40</span>的情况，利用MCMC模拟不同体系尺寸的关联时间，横坐标为<span
class="math inline"><em>T</em> − <em>T</em><sub><em>d</em></sub></span>.关联函数为<span
class="math inline">$C(t, \tau)=\frac{1}{N} \sum_{i=1}^N
\delta_{\sigma_i(t), \sigma_i(t+\tau)}$</span>.</p>
<figure>
<img src="./A_5.png"
alt="模型实验结果，用于选取最好的模型进行下一步测试" />
<figcaption
aria-hidden="true">模型实验结果，用于选取最好的模型进行下一步测试</figcaption>
</figure>
<p>每幅图中两条水平的线表示MCMC采样的值（橙色），或者空腔采样的值（蓝色）。横坐标表示不同的模型。可以看到能量和熵在均值附近，考虑涨落影响是正常的。考虑到温度较高，不在基态也合理。接下来再看，随着模型的表现能力增强（网络复杂、regularization、dropout），能量下降熵也下降，作者认为这里发生这种因素的原因是过拟合<font color='red'>（非常主观的猜测，但这个猜测却是文章的重要转折点）</font>。接下来作者认为评判的参数维度是更低的能量于更高的熵，最好的模型是shallow
MADE (ColoredMADE)。</p>
<p>接下来将用表现最好的网络进行测试，演示其为什么失败。</p>
<p><img src="./A_6.png" alt="模型实验结果" /> 其中 variational 和
maximum likelihood
代表模型训练的两种方式，前者是wu等提出的方案，后者是基于前者方案加入极大似然估计（即进一步使用细致平衡挑选样本）。</p>
<p>a、b表明在高温下利用AR与传统算法没有区别<font color='red'>高温本来关联就弱</font>；在低温情况下利用AR系统转变的更快<font color='red'>这个算法能解决关联问题，不是很好么？</font>综上，AR失败的，因为不能在低温情况下采到能量更低的样本。以上讨论在<span
class="math inline"><em>T</em> &gt; 0.3</span>的情况下，更低传统方法也会失效。</p>
<p>在d图中可以看出 maximum likelihood
策略在低温情况下能量较高，温度升高逐渐接近MCMC采样值。从e图中能够发现，熵的值接近。但是从f图看熵-能量关系，可以立刻看出与传统方案的区别。</p>
<p>从e图中看 variational
方案，可以发现，熵在低温情况下很低，这是由于发生了mode-collapsed，模型聚集在其中的一个峰上。</p>
<p><img src="./A_7.png" alt="模型实验结果2" /> 其中the Boltzmann ratio
<span
class="math inline"><em>P</em><sub><em>B</em></sub>(<em>σ</em><sub>new
</sub>)/<em>P</em><sub><em>B</em></sub>(<em>σ</em><sub>old
</sub>)</span> and the model ratio <span
class="math inline"><em>P</em><sub><em>A</em><em>R</em></sub>(<em>σ</em><sub>old
</sub>)/<em>P</em><sub>AR </sub>(<em>σ</em><sub>new </sub>)</span>.
通过实验发现，接受率随着步数（与旧模型的相差程度）的增多，在剧烈下降，这意味着
maximum likelihood 方案在低温情况基本不能探索，能量无法下降。</p>
<p><img src="./A_9.png" alt="模型实验结果3" />
上图想要说明，因为在低温情况，传统采样关联性很强，而AR模型关联性下降很快，因此认为能量依旧很高。<font color='red'>为什么不直接放能量图呢？我怀疑是因为AR模型在几个能量相近局域最小值之间跳，传统方法陷在一个局域最小值点中了。</font></p>
<h1
id="neural-annealing-and-visualization-in-the-newmanmoore-model">Neural
Annealing and Visualization in the Newman–Moore Model</h1>
<p>The classical triangular plaquette model (TPM), introduced by Newman
and Moore, also named <a
href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.60.5068">Newman–Moore
Model.</a></p>
<p>The classical triangular plaquette model
是一个用于描述磁性系统中的自旋冰态（spin ice
state）的简化模型。在这个模型中，磁性离子位于三角形stop的顶点上，形成一个由三角形组成的格子（plaquette）。每个三角形代表一个“空位”，其中的磁性离子可以有向上或向下的磁矩。在自旋冰态中，由于几何限制和磁相互作用的特定规则，每个三角形内部的磁矩配置必须是两个向上，一个向下，或者两个向下，一个向上。这种配置被称为“two-in,
one-out”规则。</p>
<p><font color='red'>这篇文章有一个问题，其使用了RNN进行训练。而且并没有说明该网络在非阻措问题上的表现能力。</font></p>
<figure>
<img src="./B_2.png" alt="模型实验结果4" />
<figcaption aria-hidden="true">模型实验结果4</figcaption>
</figure>
<p>在Newman–Moore
Model上，可以观察到在小尺寸下符合的很好，但是随着晶格尺寸的变大，基态自由能突然发生改变<font color='red'>（我更怀疑是由于计算失误）</font>，有一个突然的上升，这可能由于陷入到了一种局域解中。</p>
<figure>
<img src="./B_5.png" alt="模型实验结果6" />
<figcaption aria-hidden="true">模型实验结果6</figcaption>
</figure>
<p>除了变分自由能的最小值现在已接近T0 =
10时的确切值。随着温度的降低，景观形状变得更加崎岖，出现了相当大的高能量平台和快速变化的障碍，最终导致局部最小值消失，退火结束时变成了完全混乱的景观。因此，从这个角度来看，很明显这里存在训练问题，阻碍了变分神经退火的成功应用。</p>
<p><font color='red'>这篇文章需要更多的实验</font></p>
<h1
id="a-method-for-quantifying-the-generalization-capabilities-of-generative-models-for-solving-ising-models">A
method for quantifying the generalization capabilities of generative
models for solving Ising models</h1>
<ul>
<li><a
href="https://www.cs.toronto.edu/~norouzi/research/papers/hdml.pdf">Hamming
Distance Metric Learning</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/458114525">漫谈-Distance
Metric Learning那些事儿</a></li>
</ul>
<p>Here we design a Hamming distance
(一种用于衡量字符串之间差距的距离判定方法，将字符编码为二进制，通过记数差异位数，得到距离)
regularizer in the framework of a class of generative models,
variational autoregressive networks (VANs), to quantify the
generalization capabilities of various network architectures combined
with VAN.</p>
<p><span class="math display">$$\begin{align}
\mathcal{L}=&amp;F_q+R_h \\
F_q=&amp;\sum_{\mathbf{s}}
q_\theta(\mathbf{s})\left[E(\mathbf{s})+\frac{1}{\beta} \ln
q_\theta(\mathbf{s})\right] \\
R_h=&amp;\sum_{\mathbf{s}}\left|h m_{\mathbf{g}}(\mathbf{s})-z\right| \\
\nabla_\theta \mathcal{L}=&amp;\mathbb{E}_{\mathbf{s} \sim
q_\theta(\mathbf{s})}\left\{\left[E(\mathbf{s})+\frac{1}{\beta} \ln
q_\theta(\mathbf{s})\right] \nabla_\theta \ln
q_\theta(\mathbf{s})\right\} \\
\end{align}$$</span> 其中<span
class="math inline"><em>h</em><em>m</em><sub><em>g</em></sub>(<em>s</em>)</span>衡量与基态的距离。
<font color='red'>最后一项目有问题，因为<span
class="math inline"><em>R</em><sub><em>h</em></sub></span>采样方式是通过<span
class="math inline"><em>q</em><sub><em>θ</em></sub></span>。不过不会对训练产生影响，毕竟反向传播是准确的。</font></p>
<figure>
<img src="./C_1.png" alt="模型实验结果6" />
<figcaption aria-hidden="true">模型实验结果6</figcaption>
</figure>
<p>上图演示了收敛到正确基态的过程。</p>
<p>该模型有效的原因就是因为加入了Hamming
distance，但是需要知道正确的基态，这个正确基态是这样来的： &gt; The
previous researches have illustrated that only by containing the
configurations in the training datasets that are close to the ground
state, measured by Hamming distance, to train the neural networks, may
we obtain the ground state after training [4–7]. Therefore, we design
this regularizer to explore the relationship between the Hamming
distance and the success rates of finding the ground state for different
network architectures combined with VAN.</p>
<p>不得不说，度量学习（distance metric
learning）是提升表现力的好手段。</p>
<h1
id="message-passing-variational-autoregressive-network-for-solving-intractable-ising-models">Message
Passing Variational Autoregressive Network for Solving Intractable Ising
Models</h1>
<p>通过加入消息传递机制，提升了模型的表现能力。</p>
<figure>
<img src="./D_1.png" alt="模型实验结果D_1" />
<figcaption aria-hidden="true">模型实验结果D_1</figcaption>
</figure>
<p>上图基于<a href="https://arxiv.org/abs/1906.00275">Wishart planted
ensemble
(WPE)</a>模型，分别对比了不同算法在基态时候能量差分布图。说明文章中所提到的算法是有其优越性的。</p>
<figure>
<img src="./D_2.png" alt="模型实验结果D_2" />
<figcaption aria-hidden="true">模型实验结果D_2</figcaption>
</figure>
<p>消息传递层，主要增加了不同节点之间相互作用的耦合。文章同时还论证了为什么增加这样的网络结构能够降低能量、自由能，从而提升模型能力。
<font color='red'>文章中如何从(21)得到(18)，我觉得这里有问题。同样的对于(22)的论证同样存在问题，非凸函数<span
class="math inline"><em>q</em><sub><em>θ</em></sub></span>最外层加一个<span
class="math inline">ln </span>并不会变成凸函数。</font></p>
<figure>
<img src="./D_6.png" alt="模型实验结果D_6" />
<figcaption aria-hidden="true">模型实验结果D_6</figcaption>
</figure>
<p>实验结果说明，在具有阻锉的结构下，文章所提到的方法确实能够得到更好的基态。</p>
<p><font color='blue'>这篇文章加入了消息传递层，使得最终能探索到更低的能量状态。消息传递的方案本身，是在处理具有弱阻锉情况下的模型自由能，而文章中用到的模型属于弱耦合的模型，是消息传递算法可以处理的。</font></p>
<h1 id="variational-neural-annealing">Variational Neural Annealing</h1>
<p>主要探讨在RNN的基础上，利用模拟退火方式采样，结合变分方式训练，最后在基态问题上的优秀表现。</p>
<p>文章中采用的变分退火公式： <span
class="math display"><em>F</em><sub><strong>λ</strong></sub>(<em>t</em>) = ⟨<em>H</em><sub>target
</sub>⟩<sub><strong>λ</strong></sub> − <em>T</em>(<em>t</em>)<em>S</em><sub>classical
</sub>(<em>p</em><sub><strong>λ</strong></sub>),</span></p>
<p><img src="./E_1.png" alt="VNA" />
从红色到蓝色代表了温度的下降，其中黄线是Boltamann
分布，红线和绿线代表模拟和变分的结果，可见最后变分的结果更靠近真实分布</p>
<figure>
<img src="./E_2.png" alt="VNA" />
<figcaption aria-hidden="true">VNA</figcaption>
</figure>
<p>模拟退火算法在经典和量子状态下的演示图。</p>
<p>接下来，文章展示了在random Ising chains、Edwards-Anderson
model、SherringtonKirkpatrick (SK) model、Wishart planted ensemble
(WPE)下的实验结果，均展示了该方案能够很好的探索到模型基态。</p>
<p><img src="./E_3.png" alt="Result1" /> <img src="./E_4.png"
alt="Result2" /> <img src="./E_5.png" alt="Result3" /></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Physics</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensor network Monte Carlo simulations for the two-dimensional random-bond Ising model</title>
    <url>/2024/09/18/Phys/TNMC/TNMC/</url>
    <content><![CDATA[<p>Tensor Network Monte Carlo (TNMC)
method将张量网络和蒙特卡洛模拟结合，是一种新的模拟方法。本文分为两个部分，介绍TNMC方法，以及其在随机二维Ising模型上的实验。</p>
<p>Link: * <a href="https://arxiv.org/abs/2409.06538">Tensor network
Monte Carlo simulations for the two-dimensional random-bond Ising
model</a> * <a href="https://arxiv.org/abs/1507.00767">Unbiased Monte
Carlo for the age of tensor networks</a></p>
<p>Code: * <a href="https://github.com/Fermichen99/TNMC">TNMC</a></p>
<span id="more"></span>
<h1 id="metropolis-hasting-method">Metropolis-Hasting method</h1>
<blockquote>
<p>In particular, the local moves can get trapped in local minima,
especially in disordered systems, because due to the nature of the
rugged energy landscape, the probability of moving to a higher-energy
state is low.</p>
</blockquote>
<p>对于传统的蒙卡，主要存在两个问题：临界慢化和阻锉模型基态问题，这两个问题是由完全不同的因素造成的。临界慢化的原因是在临界点会形成团簇，单个格点翻转接受概率低，这种阻碍也称为磁畴壁，有效的解决方案是从单一格点翻转改为集体翻转；基态问题是由于模型结构自身具有的阻锉引起的。</p>
<p>这两个问题也可以从自由能的角度看待。自由能是通过熵和能量的竞争得到的，临界慢化对应相变点，此时系统倾向于处于熵极大的构型，也就是在能量相同的情况下拥有尽可能多的构型，但是这样就会遇到采样的困难，目标构型是非常稀疏的；同时还有另一个问题，熵并不容易衡量，这就意味着在穷尽结果前并不知道自己选取的构型是否为目标构型。没有评价指标和稀疏采样，造成临界慢化的困难性。那么提升方案（Swendsen-wang、Wolff）为什么有效呢？它们并没有设计机制解决稀疏采样和无评价指标的问题，而是利用在临界点特性。这些算法敏锐的觉察到，这些目标构型之间存在关联，因此从一个目标构型出发可以快速采样到其它的目标构型。基于此，接受率成为一个很好的评价指标。</p>
<p>那接下来从自由能的角度分析基态。由于温度趋近于零，此时自由能等于能量，而基态的构型是由模型决定的，因此这是具有特定解的问题，同时评价构型优劣的指标为能量，能量越低是更倾向于选择的构型。此时的难点在于如何搜索。主要有两个方面，首先在穷举之前并不清楚目前低能量构型是最低能量构型；其次如何找到高效的搜索的方法。优化策略我认为有两种，首先是通过启发式的方法搜索，给出一个撒点的方式；另一方面是通过将构型的表示方法进行编码，在编码的空间进行搜索，将一些非凸的结构转化为凸性。</p>
<p><font color='red'>竟然没有在博客中写过这个内容…之后想写的话添加上引用。</font></p>
<p><span class="math display">$$\begin{align}
\frac{P\left(\mathbf{s}_b\right)}{P\left(\mathbf{s}_a\right)}&amp;=e^{\beta
E\left(\mathbf{s}_a\right)-\beta E\left(\mathbf{s}_b\right)} \\
P(\mathbf{s})&amp;=\prod_{i=1}^N P\left(s_i \mid
\mathbf{s}_{&lt;i}\right)\\
P\left(s_i \mid
\mathbf{s}_{&lt;\mathbf{i}}\right)&amp;=\frac{\sum_{\mathbf{s}_{&gt;i}}
e^{-\beta E\left(s_i, \mathbf{s}_{&lt;i}\right)}}{\sum_{s_i,
\mathbf{s}_{&gt;i}} e^{-\beta E\left(s_i,
\mathbf{s}_{&lt;i}\right)}}=\frac{Z\left(s_i,
\mathbf{s}_{&lt;i}\right)}{\sum_{s_i} Z\left(s_i,
\mathbf{s}_{&lt;i}\right)} \label{5}
\end{align}$$</span></p>
<h1 id="tensor-network-proposals">Tensor network proposals</h1>
<p>将Ising模型求解配分函数的过程转化为张量网络。首先将整个网格表示为：</p>
<figure>
<img src="./fig1a.png" alt="tensor network" />
<figcaption aria-hidden="true">tensor network</figcaption>
</figure>
<p>由节点<span class="math inline"><em>δ</em></span>和转移矩阵<span
class="math inline"><em>W</em></span>组成。其中节点<span
class="math inline"><em>δ</em></span>的具体表达式，根据其连接边的数目（腿）决定，例如<span
class="math inline"><em>δ</em><sub>1</sub></span>有2条腿、<span
class="math inline"><em>δ</em><sub>2</sub></span>有3条腿、<span
class="math inline"><em>δ</em><sub>5</sub></span>有4条腿。 <img
src="./tensor.png" alt="tensor" />
每一个节点，通过其腿的标号表示，例如<span
class="math inline"><em>δ</em><sub>2</sub> = <em>δ</em><sub><em>m</em><em>n</em><em>o</em></sub>, <em>δ</em><sub>6</sub> = <em>δ</em><sub><em>i</em><em>j</em><em>k</em><em>l</em></sub></span>，由于Ising模型只有<span
class="math inline">±1</span>，因此每条腿的选项只有<span
class="math inline">1, 2</span>两个，并且<span
class="math inline"><em>δ</em></span>的取值定为：</p>
<p><span class="math display">$$\begin{align}
\delta_{ijkl}= \begin{cases}1 &amp; i=j=k=l \\ 0 &amp; \text { else }
\quad(i, j, k, l=1,2)\end{cases}
\end{align}$$</span></p>
<p>每一条腿表示对应格点的自旋取值，因此同一个自旋外延出来的腿应该具有相同的取值。</p>
<p>然后定义转移矩阵：</p>
<p><span class="math display">$$\begin{align}
W_{i j}=\left(\begin{array}{cc}
e^{\beta J_{i j}} &amp; e^{-\beta J_{i j}} \\
e^{-\beta J_{i j}} &amp; e^{\beta J_{i j}}
\end{array}\right)
\end{align}$$</span></p>
<p>其中 <span
class="math inline"><em>W</em><sub>11</sub> = <em>W</em><sub>22</sub> = exp (<em>β</em><em>J</em><sub><em>i</em><em>j</em></sub>), <em>W</em><sub>12</sub> = <em>W</em><sub>21</sub> = exp (−<em>β</em><em>J</em><sub><em>i</em><em>j</em></sub>)</span>，<span
class="math inline"><em>J</em><sub><em>i</em><em>j</em></sub></span>表示相邻的相互作用系数（Ising模型中是相同的，在spin
glass）。</p>
<p>接下来需要进一步对矩阵进行收缩，将转移矩阵收缩进格点中，表示为 <img
src="./fig1b.png" alt="tensor" /></p>
<p>在之前的模型中，箭头表示收缩的方向，由下及上、由左及右。例如<span
class="math inline"><em>δ</em><sub>5</sub></span>会收缩两个方向的，因此将<span
class="math inline"><em>l</em>, <em>k</em></span>进行求和：</p>
<p><span class="math display">$$\begin{align}
T_5 = T_{ijqr}=\sum_l\sum_k \delta_{ijkl} W_{kq}W_{lr}
\end{align}$$</span></p>
<p><span class="math inline"><em>δ</em><sub>2</sub></span>同理：</p>
<p><span class="math display">$$\begin{align}
T_2 = T_{mis}=\sum_o\sum_n \delta_{nmo} W_{oi}W_{ns}
\end{align}$$</span></p>
<p>接下来对于<span
class="math inline"><em>T</em><sub>2</sub><em>T</em><sub>5</sub></span>的收缩，可以通过对<span
class="math inline"><em>i</em></span>的求和，表示为<span
class="math inline">∑<sub><em>i</em></sub><em>T</em><sub><em>m</em><em>i</em><em>s</em></sub><em>T</em><sub><em>i</em><em>j</em><em>q</em><em>r</em></sub></span>。</p>
<p>因此求配分函数，接下来就是通过指标的收缩求和。 <img src="./fig2.png"
alt="tensor network contraction process" /></p>
<p>精确求解这个张量网络会遇到维数增长的问题，可以通过singular-value-decomposition（SVD）近似的方法解决这个问题。</p>
<h1
id="computing-the-partition-function-using-tensor-networks">Computing
the partition function using tensor networks</h1>
<p>计算蒙卡的接受效率，需要通过计算<span
class="math inline">$\eqref{5}$</span>。因此，接下来描述如何利用张量网络结合采样，计算<span
class="math inline">$\eqref{5}$</span>。</p>
<p>以计算<span
class="math inline"><em>s</em><sub>4</sub></span>为例，那么已经提前知道采样<span
class="math inline"><em>s</em><sub>1</sub>, <em>s</em><sub>2</sub>, <em>s</em><sub>3</sub></span>的值。此时张量网络可以表示为：</p>
<figure>
<img src="./fig3.png" alt="sample tensor" />
<figcaption aria-hidden="true">sample tensor</figcaption>
</figure>
<p>当自旋确定，改变的是在自旋收缩的时候，不再是求和而是直接固定。上图展示了如何通过指标的收缩，表示<span
class="math inline">∑<sub><em>s</em><sub>4</sub></sub><em>Z</em>(<em>s</em><sub>4</sub>, <strong>s</strong><sub> &lt; 4</sub>)</span>。对于计算<span
class="math inline"><em>Z</em>(<em>s</em><sub>4</sub>, <strong>s</strong><sub> &lt; 4</sub>)</span>则需要将，<span
class="math inline"><em>δ</em><sub>4</sub></span>改为固定的<span
class="math inline"><em>s</em><sub>4</sub></span>：</p>
<figure>
<img src="./fig4.png" alt="sample tensor2" />
<figcaption aria-hidden="true">sample tensor2</figcaption>
</figure>
<p>其中<span
class="math inline"><em>f</em>(<em>s</em><sub>4</sub>) = <em>e</em><sup><em>β</em><em>J</em><sub>14</sub><em>s</em><sub>1</sub><em>s</em><sub>4</sub></sup></span>，张量两边为<span
class="math inline"><em>s</em><sub>4</sub></span>的取值。然后逐渐增加位置，直到完成整体的采样。</p>
<p>通过如上的计算方式，便可以计算<span
class="math inline">$\eqref{5}$</span>。其中，存在一些小技巧：为了加速，在计算的时候可以存储之前的计算结果，之后相似的构型可以直接查表。</p>
<p>然而这个计算方式也存在问题，看似解决了之前迭代速度慢的问题，但每次迭代比之前要花费更多的时间。这个算法真正有效的地方在于临界行为处，当处理复杂的能量面时候，这个算法能够更快速的迭代，而不是传统蒙卡被困于无穷关联长度中。</p>
<h1 id="result">Result</h1>
<p>接下来在随机二维Ising模型上进行实验，随机体现在自旋之间的相互作用<span
class="math inline"><em>J</em></span>正负是以<span
class="math inline"><em>p</em></span>和<span
class="math inline">1 − <em>p</em></span>的概率选取。</p>
<p>该算法的核心点在于接受率的提升，所以第一个数据展示在SVD不同维度的情况，以及不同温度、不同尺寸的情况下接受率的变化。</p>
<figure>
<img src="./fig6.png" alt="result" />
<figcaption aria-hidden="true">result</figcaption>
</figure>
<p>第二个实验展示了CPU用时和内存消耗。</p>
<figure>
<img src="./fig7.png" alt="result2" />
<figcaption aria-hidden="true">result2</figcaption>
</figure>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Monte Carlo</tag>
        <tag>Computer Physics</tag>
        <tag>Tensor Network</tag>
      </tags>
  </entry>
  <entry>
    <title>Sherrington-Kirkpatrick Model</title>
    <url>/2022/11/20/Phys/SK_model/SK_model/</url>
    <content><![CDATA[<p>利用复本方法计算 Sherrington-Kirkpatrick（SK）
模型。目的是掌握和熟悉复本方法的使用。</p>
<p>Reference: * Statistical Physics of Spin Glasses and Information
Processing. Nishimori * <a
href="https://hzshan.github.io/replica_method_in_SK_model.pdf">Replica
calculations for the SK model</a>笔误有点多</p>
<p>Link: * <a href="/2022/11/12/Phys/Hopfield/Hopfield/" title="Hopfield Model">Hopfield Model</a> * <a href="/2022/11/13/Phys/replica/replica/" title="Replica">Replica</a> * <a href="/2022/11/20/Phys/SK_model/SK_model1/" title="The Parisi solution of Sherrington-Kirkpatrick Model">The Parisi solution of Sherrington-Kirkpatrick Model</a></p>
<span id="more"></span>
<h1 id="sherrington-kirkpatrick-model">Sherrington-Kirkpatrick
Model</h1>
<p><span class="math display">$$\begin{align}
H=&amp;\sum_{i&lt;j} J_{i j} S_i S_j-h \sum_i S_i \label{1}\\
P\left(J_{i j}\right)=&amp;\frac{1}{J} \sqrt{\frac{N}{2 \pi}} \exp
\left(-\frac{N}{2 J^2}\left(J_{i j}-\frac{J_0}{N}\right)^2\right)\\
\end{align}$$</span></p>
<p><span class="math display">$$\begin{align}
\text{mean}(J)=&amp;\frac{J_0}{N} \\
\text{variance}(J)=&amp;\frac{J^2}{N}
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>S</em>, <em>J</em></span>都是变量。考虑是在quneched状态。热力学平均是指对<span
class="math inline"><em>S</em></span>的平均，记为<span
class="math inline">⟨⋅⟩</span>；构型平均是对相互作用参数<span
class="math inline"><em>J</em></span>的平均，记为<span
class="math inline">[⋅]</span>。自由能写为：</p>
<p><span class="math display">$$\begin{align}
[F(\boldsymbol{s})]=-\frac{1}{\beta}[\ln
Z(\boldsymbol{s})]=-\frac{1}{\beta}\left[\ln \sum_{\{\boldsymbol{s}\}}
\exp (-\beta H(\boldsymbol{s}))\right]
\end{align}$$</span></p>
<h1 id="calculating-the-free-energy-with-replica-method">Calculating the
free energy with replica method</h1>
<p>由复本对称知：</p>
<p><span class="math display">$$\begin{align}
[\ln Z(s)]=\lim_{n\to 0}\frac{[Z^n]-1}{n}\label{4}
\end{align}$$</span></p>
<p>结合<span class="math inline">$\eqref{1}$</span>，<span
class="math inline">[<em>Z</em><sup><em>n</em></sup>]</span>可写为：
<span class="math display">$$\begin{align}
\left[Z^n\right]=\int \prod_{i&lt;j} d J_{i j} P\left(J_{i j}\right)
\sum_{\left\{\boldsymbol{s}^\alpha, \boldsymbol{s}^\beta, \ldots,
\boldsymbol{s}^n\right\}} \exp \left(-\beta \sum_{i&lt;j} J_{i j}
\sum_{\alpha=1}^n S_i^\alpha S_j^\alpha+\beta h \sum_{i=1}^N
\sum_{\alpha=1}^n S_i^\alpha\right)
\end{align}$$</span></p>
<p>直接对<span class="math inline"><em>J</em></span>进行积分：</p>
<figure class="highlight mathematica"><table><tr><td class="code"><pre><span class="line"><span class="variable">P</span> <span class="operator">=</span> <span class="number">1</span><span class="operator">/</span><span class="variable">J</span> <span class="built_in">Sqrt</span><span class="punctuation">[</span><span class="variable">NN</span><span class="operator">/</span><span class="punctuation">(</span><span class="number">2</span> <span class="built_in">Pi</span><span class="punctuation">)</span><span class="punctuation">]</span> <span class="built_in">Exp</span><span class="punctuation">[</span><span class="operator">-</span><span class="variable">NN</span><span class="operator">/</span><span class="punctuation">(</span><span class="number">2</span> <span class="variable">j</span><span class="operator">^</span><span class="number">2</span><span class="punctuation">)</span> <span class="punctuation">(</span><span class="variable">jij</span> <span class="operator">-</span> <span class="variable">j0</span><span class="operator">/</span><span class="variable">NN</span><span class="punctuation">)</span><span class="operator">^</span><span class="number">2</span><span class="punctuation">]</span><span class="operator">;</span></span><br><span class="line"><span class="built_in">Integrate</span><span class="punctuation">[</span></span><br><span class="line">  <span class="variable">P</span> <span class="built_in">Exp</span><span class="punctuation">[</span><span class="number">1</span><span class="operator">/</span><span class="variable">NN</span> <span class="punctuation">(</span><span class="operator">-</span><span class="variable">\[Beta]</span> <span class="variable">jij</span> <span class="variable">si</span> <span class="variable">sj</span> <span class="operator">+</span> <span class="variable">\[Beta]</span> <span class="variable">h</span> <span class="variable">si</span><span class="punctuation">)</span><span class="punctuation">]</span><span class="operator">,</span> <span class="punctuation">&#123;</span><span class="variable">jij</span><span class="operator">,</span> <span class="operator">-</span><span class="built_in">Infinity</span><span class="operator">,</span> </span><br><span class="line">   <span class="built_in">Infinity</span><span class="punctuation">&#125;</span><span class="punctuation">]</span> <span class="operator">//</span> <span class="built_in">FullSimplify</span></span><br></pre></td></tr></table></figure>
<p><span class="math display">$$\begin{align}
\left[Z^n\right]=C_1 \sum_{\left\{\boldsymbol{s}^\alpha,
\boldsymbol{s}^\beta, \ldots, \boldsymbol{s}^n\right\}} \exp
\left\{\frac{1}{N} \sum_{i&lt;j}\left(\frac{1}{2} \beta^2 J^2
\sum_{\alpha, \beta} S_i^\alpha S_j^\alpha S_i^\beta S_j^\beta+\beta J_0
\sum_\alpha S_i^\alpha S_j^\alpha\right)+\beta h \sum_{i=1}^N
\sum_{\alpha=1}^n S_i^\alpha\right\} .
\end{align}$$</span></p>
<p>对其中一项进行拆分： <span class="math display">$$\begin{align}
\sum_{\alpha, \beta} S_i^\alpha S_j^\alpha S_i^\beta S_j^\beta=2
\sum_{\alpha&lt;\beta} S_i^\alpha S_j^\alpha S_i^\beta
S_j^\beta+\sum_\alpha\left(S_i^\alpha S_j^\alpha\right)^2=2
\sum_{\alpha&lt;\beta} S_i^\alpha S_j^\alpha S_i^\beta S_j^\beta+n
\end{align}$$</span></p>
<p>因此有：</p>
<p><span class="math display">$$\begin{align}
{\left[Z^n\right] } &amp; =C_1 \sum_{\left\{s^\alpha, s^\beta, \ldots,
s^n\right\}} \exp \left\{\frac{1}{N} \sum_{i&lt;j}\left(\frac{1}{2}
\beta^2 J^2\left(2 \sum_{\alpha&lt;\beta} S_i^\alpha S_j^\alpha
S_i^\beta S_j^\beta+n\right)+\beta J_0 \sum_\alpha S_i^\alpha
S_j^\alpha\right)+\beta h \sum_{i=1}^N \sum_{\alpha=1}^n
S_i^\alpha\right\} \\
&amp; =C_1 \sum_{\left\{s^\alpha, s^\beta, \ldots, s^n\right\}} \exp
\left\{\frac{1}{N} \sum_{i&lt;j}\left(\frac{1}{2} \beta^2 J^2
n+\frac{1}{2} \beta^2 J^2\left(2 \sum_{\alpha&lt;\beta} S_i^\alpha
S_j^\alpha S_i^\beta S_j^\beta\right)+\beta J_0 \sum_\alpha S_i^\alpha
S_j^\alpha\right)+\beta h \sum_{i=1}^N \sum_{\alpha=1}^n
S_i^\alpha\right\} \\
&amp; =C_1 \exp \left(\frac{(N-1) \beta^2 J^2 n}{4}\right)
\sum_{\left\{s^\alpha, s^\beta, \ldots, s^n\right\}} \exp
\left\{\frac{1}{N} \sum_{i&lt;j}\left(\beta^2 J^2 \sum_{\alpha&lt;\beta}
S_i^\alpha S_j^\alpha S_i^\beta S_j^\beta+\beta J_0 \sum_\alpha
S_i^\alpha S_j^\alpha\right)+\beta h \sum_{i=1}^N \sum_{\alpha=1}^n
S_i^\alpha\right\}\label{9}
\end{align}$$</span></p>
<p>采用近似<span class="math inline">$\exp \left(\frac{(N-1) \beta^2 J^2
n}{4}\right) \approx\exp \left(\frac{N \beta^2 J^2
n}{4}\right)$</span>，接下来关注<span
class="math inline">$\eqref{9}$</span>第二项中第一部分：</p>
<p><span class="math display">$$\begin{align}
\frac{1}{N} \sum_{i&lt;j} \beta^2 J^2 \sum_{\alpha&lt;\beta} S_i^\alpha
S_j^\alpha S_i^\beta S_j^\beta &amp; =\frac{\beta^2 J^2}{2
N}\left(\sum_{\alpha&lt;\beta}\left(\sum_i S_i^\alpha
S_i^\beta\right)^2-\sum_i \sum_{\alpha&lt;\beta} S_i^\alpha S_i^\alpha
S_i^\beta S_i^\beta\right) \\
&amp; =\frac{\beta^2 J^2}{2 N} \sum_{\alpha&lt;\beta}\left(\sum_i
S_i^\alpha S_i^\beta\right)^2-\frac{\beta^2 J^2}{2 N} \sum_i
\sum_{\alpha&lt;\beta} 1 \label{9_1}
\end{align}$$</span></p>
<p>将常数项加入前面的系数，接下来分析<span
class="math inline">$\eqref{9}$</span>第二项中第二部分：</p>
<p><span class="math display">$$\begin{align}
\frac{\beta J_0}{N} \sum_{i&lt;j} \sum_\alpha S_i^\alpha
S_j^\alpha=\frac{\beta J_0}{2 N} \sum_\alpha\left(\sum_i
S_i^\alpha\right)^2-\frac{\beta J_0}{2 N} \sum_i \sum_\alpha 1
\label{9_2}
\end{align}$$</span></p>
<p>同样处理常数项目，结合<span
class="math inline">$\eqref{9}\eqref{9_1}\eqref{9_2}$</span>，得到：</p>
<p><span class="math display">$$\begin{align}
\left[Z^n\right]=C_2 \exp \left(\frac{N \beta^2 J^2 n}{4}\right)
\sum_{\left\{s^\alpha, s^\beta, \ldots, s^n\right\}} \exp
\left\{\frac{\beta^2 J^2}{2 N} \sum_{\alpha&lt;\beta}\left(\sum_i
S_i^\alpha S_i^\beta\right)^2+\frac{\beta J_0}{2 N}
\sum_\alpha\left(\sum_i S_i^\alpha\right)^2+\beta h \sum_{i=1}^N
\sum_{\alpha=1}^n S_i^\alpha\right\} \label{10}
\end{align}$$</span></p>
<p>需要将<span
class="math inline">$\eqref{10}$</span>指数中的平方项转变为线性，使用Hubbard-Stratonovich
替换：</p>
<p><span class="math display">$$\begin{align}
\exp \left(\frac{y^2}{2}\right)=\int_{-\infty}^{\infty} \frac{d
x}{\sqrt{2 \pi}} \exp \left(-\frac{x^2}{2}\right) \exp (x y)\label{11}
\end{align}$$</span></p>
<p>可得：</p>
<p><span class="math display">$$
\begin{gather}
\exp \frac{\beta^2 J^2}{2 N}\left(\sum_i S_i^\alpha
S_i^\beta\right)^2=\int_{-\infty}^{\infty} \frac{N d q_{\alpha
\beta}}{\sqrt{2 \pi}} \exp \left(-\beta^2 J^2 N \frac{q_{\alpha
\beta}^2}{2}+\beta^2 J^2 q_{\alpha \beta} \sum_i S_i^\alpha
S_i^\beta\right) \\
\exp \frac{\beta J_0}{2 N}\left(\sum_i
S_i^\alpha\right)^2=\int_{-\infty}^{\infty} \frac{N d m_\alpha}{\sqrt{2
\pi}} \exp \left(-\beta J_0 N m_\alpha^2+\beta J_0 m_\alpha \sum_i
S_i^\alpha\right)
\end{gather}
$$</span></p>
<p>再将结果代入<span
class="math inline">$\eqref{10}$</span>，然后得到结果：</p>
<p><span class="math display">$$\begin{equation}
\begin{aligned}
&amp; {\left[Z^n\right]=} C_4 \exp \left(\frac{N \beta^2 J^2
n}{4}\right) \int_{-\infty}^{\infty} \prod_{\alpha&lt;\beta} d q_{\alpha
\beta} \prod_\alpha d m_\alpha \\
&amp; \exp \left(-\frac{\beta^2 J^2 N}{2} \sum_{\alpha&lt;\beta}
q_{\alpha \beta}^2-\frac{\beta J_0 N}{2} \sum_\alpha m_\alpha^2\right)
\\
&amp; \sum_{\left\{s^\alpha, s^\beta, \ldots, s^n\right\}} \exp
\left(\beta^2 J^2 \sum_{\alpha&lt;\beta} q_{\alpha \beta} \sum_i
S_i^\alpha S_i^\beta+\beta \sum_\alpha\left(J_0 m_\alpha+h\right) \sum_i
S_i^\alpha\right)
\end{aligned} \label{14}
\end{equation}$$</span></p>
<p>对于<span class="math inline">$\eqref{14}$</span>最后一项，改写为：
<span class="math display">$$
\prod_{i=1}^N \sum_{\left\{s_i^\alpha, s_i^\beta, \ldots, s_i^n\right)}
\exp \left(\beta^2 J^2 \sum_{\alpha&lt;\beta} q_{\alpha \beta}
S_i^\alpha S_i^\beta+\beta \sum_\alpha\left(J_0 m_\alpha+h\right)
S_i^\alpha\right)
$$</span></p>
<p>由于对于任意<span class="math inline"><em>i</em></span>，有<span
class="math inline"><em>s</em><sub><em>i</em></sub> = ±1</span>，这样对于不同的<span
class="math inline"><em>i</em></span>是没有差别的：</p>
<p><span class="math display">$$\begin{align}
\left\{\sum_{\left\{s_i^\alpha, s_i^\beta, \ldots, s_i^n\right)} \exp
\left(\beta^2 J^2 \sum_{\alpha&lt;\beta} q_{\alpha \beta} S^\alpha
S^\beta+\beta \sum_\alpha\left(J_0 m_\alpha+h\right)
S^\alpha\right)\right\}^N \equiv \exp \left\{N \log
\sum_{\left\{s_i^\alpha, s_i^\beta, \ldots, s_i^n\right)} \exp
\left(L\left(\left\{q_{\alpha \beta},
m_\alpha\right\}\right)\right)\right\} \label{18}
\end{align}$$</span></p>
<p>其中定义： <span class="math display">$$\begin{align}
L\left(\left\{q_{\alpha \beta}, m_\alpha\right\}\right):=\beta^2 J^2
\sum_{\alpha&lt;\beta} q_{\alpha \beta} S^\alpha S^\beta+\beta
\sum_\alpha\left(J_0 m_\alpha+h\right) S^\alpha \label{19}
\end{align}$$</span></p>
<p>将<span class="math inline">$\eqref{18}\eqref{19}$</span>代入<span
class="math inline">$\eqref{14}$</span>：</p>
<p><span class="math display">$$\begin{equation}
\begin{aligned}
\left[Z^n\right]=&amp; C_4 \exp \left(\frac{N \beta^2 J^2 n}{4}\right)
\int_{-\infty}^{\infty} \prod_{\alpha&lt;\beta} d q_{\alpha \beta}
\prod_\alpha d m_\alpha \\
&amp; \exp \left\{-\frac{\beta^2 J^2 N}{2} \sum_{\alpha&lt;\beta}
q_{\alpha \beta}^2-\frac{\beta J_0 N}{2} \sum_\alpha m_\alpha^2+N \log
\sum_{\left\{S^\alpha, S^\beta, \ldots, S^n\right)} \exp (L)\right\}
\end{aligned} \label{20}
\end{equation}$$</span></p>
<p>由于热力学极限有<span
class="math inline"><em>N</em> → ∞</span>，采用Laplace近似可知（进行该近似其是会出现一个常数项，这里没有写出，因为直接扔掉也对结果没有影响），结果取决于与指数上最大值的那个点：</p>
<p><span class="math display">$$
\begin{gather}
E:=-\frac{\beta^2 J^2 N}{2} \sum_{\alpha&lt;\beta} q_{\alpha
\beta}^2-\frac{\beta J_0 N}{2} \sum_\alpha m_\alpha^2+N \log
\sum_{\left\{S^\alpha, S^\beta, \ldots, S^n\right\}} \exp (L)
\label{22}\\
\left\{q_{\alpha \beta}^{\star}, m_\alpha^{\star}\right\}=\arg \max
_{\left\{q_{\alpha \beta}, m_\alpha\right\}} E
\end{gather}
$$</span></p>
<p>因此<span class="math inline">$\eqref{14}$</span>转变为：</p>
<p><span class="math display">$$\begin{align}
{\left[Z^n\right] } &amp; =C_4 \exp \left\{\frac{N \beta^2 J^2
n}{4}-\frac{\beta^2 J^2 N}{2} \sum_{\alpha&lt;\beta}\left(q_{\alpha
\beta}^{\star}\right)^2-\frac{\beta J_0 N}{2}
\sum_\alpha\left(m_\alpha^{\star}\right)^2+N \log \sum_{\left\{S^\alpha,
S^\beta, \ldots, S^n\right\}} \exp (L)\right\} \label{24}\\
&amp; =C_4 \exp \left\{N n\left(\frac{\beta^2 J^2}{4}-\frac{\beta^2
J^2}{2 n} \sum_{\alpha&lt;\beta}\left(q_{\alpha
\beta}^{\star}\right)^2-\frac{\beta J_0}{2 n}
\sum_\alpha\left(m_\alpha^{\star}\right)^2+\frac{1}{n} \log
\sum_{\left\{S^\alpha, S^\beta, \ldots, S^n\right\}} \exp
(L)\right)\right\} \label{25}
\end{align}$$</span></p>
<p><font color='blue'>在进行到<span
class="math inline">$\eqref{24}$</span>时候已经黔宾黩武，从定义<span
class="math inline">$\eqref{19}$</span>中可以看出<span
class="math inline"><em>L</em></span>包含<span
class="math inline"><em>q</em>, <em>m</em></span>。此时处理已经很麻烦了，需要近似。</font></p>
<p>同时采用复本极限<span
class="math inline"><em>n</em> → 0</span>，将<span
class="math inline">$\eqref{25}$</span>进行展开：</p>
<p><span class="math display">$$\begin{align}
\left[Z^n\right] \approx 1+N n\left\{\frac{\beta^2 J^2}{4}-\frac{\beta^2
J^2}{2 n} \sum_{\alpha&lt;\beta}\left(q_{\alpha
\beta}^{\star}\right)^2-\frac{\beta J_0}{2 n}
\sum_\alpha\left(m_\alpha^{\star}\right)^2+\frac{1}{n} \log
\sum_{\left\{S^\alpha, S^\beta, \ldots, S^n\right\}} \exp (L)\right\}
\label{26}
\end{align}$$</span></p>
<p>最后将<span class="math inline">$\eqref{26}$</span>代回复本技巧<span
class="math inline">$\eqref{4}$</span>中：</p>
<p><span class="math display">$$
\begin{align}
\frac{[\log Z]}{N} &amp; =\lim _{n \rightarrow 0}
\frac{\left[Z^n\right]-1}{N n} \\
&amp; =\lim _{n \rightarrow 0}\left\{\frac{\beta^2 J^2}{4}-\frac{\beta^2
J^2}{2 n} \sum_{\alpha&lt;\beta}\left(q_{\alpha
\beta}^{\star}\right)^2-\frac{\beta J_0}{2 n}
\sum_\alpha\left(m_\alpha^{\star}\right)^2+\frac{1}{n} \log
\sum_{\left\{S^\alpha, S^\beta, \ldots, S^n\right\}} \exp (L)\right\}
\label{28}
\end{align}
$$</span></p>
<p>最后需要确定<span
class="math inline">{<em>q</em><sub><em>α</em><em>β</em></sub><sup>⋆</sup>, <em>m</em><sub><em>α</em></sub><sup>⋆</sup>}</span>的表达式，使用<span
class="math inline">$\frac{\partial}{\partial q_{\alpha \beta}}
E=\frac{\partial}{\partial m_\alpha} E=0$</span>，从<span
class="math inline">$\eqref{22}$</span>中得到：</p>
<p><span class="math display">$$\begin{align}
q_{\alpha \beta}^{\star}=&amp; \frac{1}{\beta^2 J^2}
\frac{\partial}{\partial q_{\alpha \beta}} \log \sum_{\left\{S^\alpha,
S^\beta, \ldots, S^n\right\}} \exp (L)=\frac{1}{\beta^2 J^2}
\frac{\sum_{\left\{S^\alpha, S^\beta, \ldots, S^n\right\}} \exp (L)
\beta^2 J^2}{\sum_{\left\{S^\alpha, S^\beta, \ldots, S^n\right\}} \exp
(L)} S^\alpha S^\beta . \\
=&amp; \frac{\sum_{\left\{S^\alpha, S^\beta, \ldots, S^n\right\}}
S^\alpha S^\beta \exp (L)}{\sum_{\left\{S^\alpha, S^\beta, \ldots,
S^n\right\}} \exp (L)} \\
m_\alpha^{\star}=&amp;\frac{1}{\beta J_0} \frac{\partial}{\partial
m_\alpha} \log \sum_{\left\{s_i^\alpha, s_i^\beta, \ldots, s_i^n\right)}
\exp (L)=\frac{\sum_{\left\{S^\alpha, S^\beta, \ldots, S^n\right\}}
S^\alpha \exp (L)}{\sum_{\left\{S^\alpha, S^\beta, \ldots, S^n\right\}}
\exp (L)} .
\end{align}$$</span></p>
<h2 id="q_alpha-beta-and-m_alpha-as-order-parameters"><span
class="math inline"><em>q</em><sub><em>α</em><em>β</em></sub></span> and
<span class="math inline"><em>m</em><sub><em>α</em></sub></span> as
order parameters</h2>
<p>可以改写为（<font color='blue'>并不是直接通过上面的过程的得出，只是先把结论摆出来</font>）：
<span class="math display">$$\begin{align}
q_{\alpha \beta}=\left[\frac{\sum_{\left\{s^\alpha, s^s, \ldots,
s^n\right\}} S_i^\alpha S_i^\beta \exp \left(-\beta \sum_{\gamma=1}^n
H_\gamma\right)}{\sum_{\left\{s^\alpha, s^\beta, \ldots, s^n\right\}}
\exp \left(-\beta \sum_{\gamma=1}^n H_\gamma\right)}\right]
\equiv\left[\left\langle S_i^\alpha S_i^\beta\right\rangle\right]
\label{32}
\end{align}$$</span></p>
<p>其中： <span class="math display">$$\begin{align}
H_\gamma=&amp;\sum_{i&lt;j} J_{i j}^2\beta S_i^\gamma S_j^\gamma-h
\sum_i S_i^\gamma \label{33}\\
m_\alpha=&amp;\left[\left\langle S_i^\alpha\right\rangle\right]
\label{34}
\end{align}$$</span></p>
<h1 id="replica-symmetric-solution">Replica-symmetric solution</h1>
<p>对于<span
class="math inline">$\eqref{24}$</span>已经提到，需要处理里面的<span
class="math inline"><em>q</em><sub><em>α</em><em>β</em></sub></span>与<span
class="math inline"><em>m</em><sub><em>α</em></sub></span>的问题。现在直接简单粗暴的假设：
<span class="math display">$$\begin{align}
\forall\alpha,\beta\quad q_{\alpha\beta}=q,m_\alpha=m \label{RS}
\end{align}$$</span></p>
<p>将<span
class="math inline">$\eqref{28}$</span>近似之后的结果写为;</p>
<p><span class="math display">$$\begin{align}
\frac{[\log Z]}{N} &amp; =\lim _{n \rightarrow 0}\left\{\frac{\beta^2
J^2}{4}-\frac{\beta^2 J^2(n-1)}{4} q^2-\frac{\beta J_0}{2}
m^2+\frac{1}{n} \log \sum_{\left\{S^\alpha, S^\beta, \ldots,
S^n\right\}} \exp \left(L_{q_{\alpha \beta}=q,
m_\alpha=m}\right)\right\} \label{35} \\
&amp; =\frac{\beta^2 J^2}{4}\left(1+q^2\right)-\frac{\beta J_0}{2}
m^2+\lim _{n \rightarrow 0} \frac{1}{n} \log \sum_{\left\{S^\alpha,
S^\beta, \ldots, S^n\right\}} \exp \left(L_{q_\alpha \beta}=q,
m_\alpha=m\right) \label{36}
\end{align}$$</span></p>
<p>将其中<span class="math inline"><em>L</em></span>项具体写为：</p>
<p><span class="math display">$$\begin{align}
\frac{1}{n} \log \sum_{\left\{s_i^\alpha, s_i^\beta, \ldots,
s_i^n\right)} \exp \left(L_{q_{\alpha \beta}=q,
m_\alpha=m}\right)=\frac{1}{n}\log \sum_{\left\{S^\alpha, S^\beta,
\ldots, S^n\right\}} \exp \left(\beta^2 J^2 q \sum_{\alpha&lt;\beta}
S^\alpha S^\beta+\beta\left(J_0 m+h\right) \sum_\alpha S^\alpha\right)
\end{align}$$</span></p>
<p>引入变量<span class="math inline"><em>ẑ</em></span>，结合<span
class="math inline">$\eqref{11}$</span>将<span
class="math inline"><em>S</em><sup><em>α</em></sup><em>S</em><sup><em>β</em></sup></span>线性化：
<span class="math display">$$\begin{align}
p(\hat{z})=\sqrt{\frac{1}{2 \pi}} \exp \left(-\frac{\hat{z}^2}{2}\right)
\end{align}$$</span></p>
<p><span class="math display">$$
\begin{align}
&amp; \frac{1}{n} \log \sum_{\left\{s_i^\alpha, s_i^\beta, \ldots,
s_i^n\right)} \exp \left(\beta^2 J^2 \sum_{\alpha&lt;\beta} q_{\alpha
\beta} S^\alpha S^\beta+\beta \sum_\alpha\left(J_0 m_\alpha+h\right)
S^\alpha\right) \\
= &amp; \frac{1}{n} \log \sum_{\left\{s_i^\alpha, s_i^\beta, \ldots,
s_i^n\right)} \int d z p(z) \exp \left(\beta J \sqrt{q} \hat{z}
\sum_\alpha S^\alpha-\frac{n}{2} \beta^2 J^2 q+\beta\left(J_0 m+h\right)
\sum_\alpha S^\alpha\right) \\
= &amp; \frac{1}{n} \log\left[ \exp \left(-\frac{n}{2} \beta^2 J^2
q\right) \int d z p(\hat{z}) \sum_{\left\{S^\alpha, S^\beta, \ldots,
S^n\right\}} \exp \left(\sum_\alpha S^\alpha\left(\beta J \sqrt{q}
\hat{z}+\beta\left(J_0 m+h\right)\right)\right)\right] \\
= &amp; \frac{1}{n} \log\left[ \exp \left(-\frac{n}{2} \beta^2 J^2
q\right) \int D z \sum_{\left\{S^\alpha, S^\beta, \ldots, S^n\right\}}
\exp \left(\sum_\alpha S^\alpha\left(\beta J \sqrt{q} z+\beta\left(J_0
m+h\right)\right)\right)\right] \\
= &amp; \frac{1}{n} \log\left[ \exp \left(-\frac{n}{2} \beta^2 J^2
q\right) \int D z \prod_{\gamma=1}^n \sum_{S^\gamma= \pm 1} \exp
\left(S^\gamma\left(\beta J \sqrt{q} z+\beta\left(J_0
m+h\right)\right)\right)\right] \\
= &amp; \frac{1}{n} \log\left[ \exp \left(-\frac{n}{2} \beta^2 J^2
q\right) \int D z  \left\{2 \cosh \left(\beta J \sqrt{q}
z+\beta\left(J_0 m+h\right)\right)\right\}^n\right] \label{38_1} \\
= &amp; \frac{1}{n} \log\left[ \exp \left(-\frac{n}{2} \beta^2 J^2
q\right) \int D z \exp \left(n \log 2 \cosh \left(\beta J \sqrt{q}
z+\beta\left(J_0 m+h\right)\right)\right) \right] \\
= &amp;  \left(-\frac{1}{2} \beta^2 J^2 q\right)+\frac{1}{n} \log\left[
\int D z \exp \left(n \log 2 \cosh \left(\beta J \sqrt{q}
z+\beta\left(J_0 m+h\right)\right)\right) \right]\label{39} \\
\end{align}
$$</span> 对其中<span
class="math inline"><em>n</em></span>进行极小值展开，<font color='red'>这里其是存在一些问题更清晰的应该将求极限的操作放在后面进行，并且将另一个<span
class="math inline"><em>n</em></span>分离出来，不过问题不大只会造成理解困难。</font></p>
<p><span class="math display">$$\begin{align}
&amp;\approx \left(-\frac{1}{2} \beta^2 J^2 q\right)+\frac{1}{n} \log
\left\{1+n \int D z \log 2 \cosh \left(\beta J \sqrt{q} z+\beta\left(J_0
m+h\right)\right)\right\} \\
&amp;\approx \left(-\frac{1}{2} \beta^2 J^2 q\right)+\int D z \log 2
\cosh \left(\beta J \sqrt{q} z+\beta\left(J_0
m+h\right)\right)\label{41}
\end{align}$$</span></p>
<p>定义： <span class="math display">$$\begin{align}
\tilde{H}(z):=J \sqrt{q} z+J_0 m+h  \label{42}
\end{align}$$</span></p>
<p>结合<span class="math inline">$\eqref{41}\eqref{42}$</span>将<span
class="math inline">$\eqref{36}$</span>写为：</p>
<p><span class="math display">$$\begin{align}
\frac{[\log Z]}{N}=\frac{\beta^2 J^2}{4}(1-q)^2-\frac{\beta J_0}{2}
m^2+\int D z \log 2 \cosh (\beta \tilde{H}(z)) \label{44}
\end{align}$$</span></p>
<p>通过<span class="math inline">$\partial_q \frac{[\log
Z]}{N}=\partial_m \frac{[\log Z]}{N}=0$</span>获得极值点（计算<span
class="math inline"><em>q</em></span>的时候需要分布积分处理一下）：</p>
<figure class="highlight mathematica"><table><tr><td class="code"><pre><span class="line"><span class="variable">PP</span> <span class="operator">=</span> <span class="built_in">Sqrt</span><span class="punctuation">[</span><span class="variable">\[Beta]</span><span class="operator">^</span><span class="number">2</span> <span class="variable">j</span><span class="operator">^</span><span class="number">2</span> <span class="variable">q</span><span class="operator">/</span><span class="punctuation">(</span><span class="number">2</span> <span class="built_in">Pi</span><span class="punctuation">)</span><span class="punctuation">]</span> <span class="built_in">Exp</span><span class="punctuation">[</span><span class="operator">-</span><span class="variable">z</span><span class="operator">^</span><span class="number">2</span><span class="operator">/</span><span class="number">2</span> <span class="variable">\[Beta]</span><span class="operator">^</span><span class="number">2</span> <span class="variable">j</span><span class="operator">^</span><span class="number">2</span> <span class="variable">q</span><span class="punctuation">]</span><span class="operator">;</span></span><br><span class="line"></span><br><span class="line"><span class="variable">HH</span> <span class="operator">=</span> <span class="variable">j</span> <span class="built_in">Sqrt</span><span class="punctuation">[</span><span class="variable">q</span><span class="punctuation">]</span> <span class="variable">z</span> <span class="operator">+</span> <span class="variable">j0</span> <span class="variable">m</span> <span class="operator">+</span> <span class="variable">h</span><span class="operator">;</span></span><br><span class="line"><span class="variable">ZZ</span> <span class="operator">=</span> <span class="punctuation">(</span><span class="variable">\[Beta]</span><span class="operator">^</span><span class="number">2</span> <span class="variable">j</span><span class="operator">^</span><span class="number">2</span><span class="punctuation">)</span><span class="operator">/</span><span class="number">4</span> <span class="punctuation">(</span><span class="number">1</span> <span class="operator">-</span> <span class="variable">q</span><span class="punctuation">)</span><span class="operator">^</span><span class="number">2</span> <span class="operator">-</span> <span class="punctuation">(</span><span class="variable">\[Beta]</span> <span class="variable">j0</span><span class="punctuation">)</span><span class="operator">/</span><span class="number">2</span> <span class="variable">m</span><span class="operator">^</span><span class="number">2</span> <span class="operator">+</span> </span><br><span class="line">   <span class="built_in">Inactivate</span><span class="punctuation">[</span></span><br><span class="line">    <span class="built_in">Integrate</span><span class="punctuation">[</span><span class="variable">PP</span> <span class="built_in">Log</span><span class="punctuation">[</span><span class="number">2</span> <span class="built_in">Cosh</span><span class="punctuation">[</span><span class="variable">\[Beta]</span> <span class="variable">HH</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="operator">,</span> <span class="punctuation">&#123;</span><span class="variable">z</span><span class="operator">,</span> <span class="operator">-</span><span class="built_in">Infinity</span><span class="operator">,</span> <span class="built_in">Infinity</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="operator">;</span></span><br><span class="line"><span class="built_in">D</span><span class="punctuation">[</span><span class="variable">ZZ</span><span class="operator">,</span> <span class="variable">m</span><span class="punctuation">]</span> <span class="operator">//</span> <span class="built_in">TraditionalForm</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">D</span><span class="punctuation">[</span><span class="built_in">Tanh</span><span class="punctuation">[</span><span class="variable">\[Beta]</span> <span class="variable">HH</span><span class="punctuation">]</span><span class="operator">,</span> <span class="variable">z</span><span class="punctuation">]</span> <span class="operator">//</span> <span class="built_in">FullSimplify</span></span><br></pre></td></tr></table></figure>
<p><span class="math display">$$
\begin{gather}
m=\int D z \tanh \beta \tilde{H}(z) \label{45}\\
q=1-\int D z \operatorname{sech}^2 \beta \tilde{H}(z)=\int D z \tanh ^2
\beta \tilde{H}(z) \label{46}
\end{gather}
$$</span></p>
<h1 id="replica-symmetry-breaking-and-the-parisi-solution">Replica
symmetry breaking and the Parisi solution</h1>
<h2
id="problem-with-the-symmetric-results-negative-entropy-at-low-temperature">Problem
with the symmetric results: negative entropy at low temperature</h2>
<p>进行这样的假设<span
class="math inline"><em>J</em><sub>0</sub> = <em>h</em> = 0</span>，有
<span class="math inline">$\tilde{H}(z)=J \sqrt{q}
z$</span>；并且有零温极限<span
class="math inline"><em>β</em> → ∞</span>，此时<span
class="math inline"><em>q</em> → 1</span>。但还是需要更<span
class="math inline"><em>q</em></span>与<span
class="math inline"><em>β</em></span>的线性结果，将<span
class="math inline">$\eqref{46}$</span>在这个条件下进行零温近似：</p>
<p><span class="math display">$$\begin{align}
\lim _{\beta \rightarrow \infty} \int D z \operatorname{sech}^2 \beta
\tilde{H}(z)&amp;=\int D z \frac{2}{\beta J} \delta(\beta J z)\\
&amp;=\sqrt{\frac{2}{\pi}} \frac{T}{J}
\end{align}$$</span></p>
<p>考虑到<span class="math inline">$\int d x\operatorname{sech}^2 \beta
x=\frac{2}{\beta}$</span>，等价于<span class="math inline">$\int d
x\frac{2}{\beta}\delta(x)$</span>。</p>
<p>因此有： <span class="math display">$$\begin{align}
q=1-\frac{T}{J} \sqrt{\frac{2}{\pi}} \label{q_limit}
\end{align}$$</span></p>
<p>计算自由能： <span class="math display">$$\begin{align}
[f]\beta=-\frac{[\log Z]}{N}=-\frac{\beta^2 J^2}{4}(1-q)^2-\int D z \log
2 \cosh (\beta J \sqrt{q} z) \label{48}
\end{align}$$</span></p>
<p>其中<span
class="math inline">[<em>f</em>]</span>表示构型平均。将<span
class="math inline">$\eqref{q_limit}$</span>代入<span
class="math inline">$\eqref{48}$</span>最后一项：</p>
<p><span class="math display">$$\begin{align}
\int D z \log 2 \cosh (\beta J \sqrt{q} z) &amp; =2 \int_0^{\infty} D z
\log 2 \cosh (\beta J \sqrt{q} z) \\
&amp; =2 \int_0^{\infty} D z \log 2 \frac{\exp (-\beta J \sqrt{q}
z)+\exp (\beta J \sqrt{q} z)}{2} \\
&amp; \approx 2 \int_0^{\infty} D z \log 2 \frac{\exp (\beta J \sqrt{q}
z)}{2} \\
&amp;=\frac{2 \beta J \sqrt{q}}{\sqrt{2\pi}}
\end{align}$$</span></p>
<p><span class="math inline">$\sqrt{q}$</span>已经是<span
class="math inline">1</span>附近的小量，因此结合<span
class="math inline">$\eqref{q_limit}$</span>有<span
class="math inline">$\sqrt{q}\approx q=1-\frac{T}{J}
\sqrt{\frac{2}{\pi}}$</span>：</p>
<p><span class="math display">$$\begin{align}
\int D z \log 2 \cosh (\beta J \sqrt{q} z)
&amp;=\frac{2 \beta J}{\sqrt{2\pi}}\left(1-\frac{T}{J}
\sqrt{\frac{2}{\pi}}\right) \\
&amp;= \sqrt{\frac{2}{\pi}}\beta J-\frac{2}{\pi}
\end{align}$$</span></p>
<p>从<span class="math inline">$\eqref{48}$</span>自由能有：</p>
<p><span class="math display">$$\begin{align}
[f] \approx -\sqrt{\frac{2}{\pi}}J + \frac{2 T}{\pi}
\end{align}$$</span></p>
<p>利用关系<span class="math inline">$S=-\frac{\partial F}{\partial
T}$</span>，得到熵为： <span class="math display">$$\begin{align}
S=-\frac{2}{\pi}
\end{align}$$</span></p>
<p>显然负熵是没有含义的，这来源于假设的错误，需要引入复本对称破缺解决。</p>
<h2 id="stability-of-solutions">Stability of solutions</h2>
<p>利用一阶梯度为零的方法寻找极值，这存在一个问题：找到的解是否稳定？利用Hassian矩阵分析稳定性，并且假定<span
class="math inline"><em>h</em> = 0</span>。注记： <span
class="math display">$$\begin{align}
y^{\alpha \beta}:=\beta J q_{\alpha \beta}, x^\alpha=\sqrt{\beta J_0}
m_\alpha
\end{align}$$</span></p>
<p>改写<span class="math inline">$\eqref{28}$</span>：</p>
<p><span class="math display">$$\begin{align}
[f]=-\frac{\beta J^2}{4}-\lim _{n \rightarrow 0} \frac{1}{\beta
n}\left\{-\sum_{\alpha&lt;\beta} \frac{1}{2}\left(y^{\alpha
\beta}\right)^2-\sum_\alpha \frac{1}{2}\left(x^\alpha\right)^2+\log
\sum_{\left\{S^\alpha, S^\beta, \ldots, S^n\right\}} \exp \left(\beta J
\sum_{\alpha&lt;\beta} y^{\alpha \beta} S^\alpha S^\beta+\sqrt{\beta
J_0} \sum_\alpha x^\alpha S^\alpha\right)\right\}
\end{align}$$</span></p>
<p>结合之前的复本近似假设，认为不同的值是微扰之后的结果<span
class="math inline"><em>x</em><sup><em>α</em></sup> = <em>x</em> + <em>ϵ</em><sup><em>α</em></sup>, <em>y</em><sup><em>α</em><em>β</em></sup> = <em>y</em> + <em>η</em><sup><em>α</em><em>β</em></sup></span>，则<span
class="math inline">$\eqref{19}$</span>：</p>
<p><span class="math display">$$\begin{align}
&amp; L_0:=\beta J y \sum_{\alpha&lt;\beta} S^\alpha S^\beta+\sqrt{\beta
J_0} x \sum_\alpha S^\alpha \\
&amp; \langle f\rangle_{L^0}=\frac{\sum_{\left\{S^\alpha, S^\beta,
\ldots, S^n\right\}} e^{L_0\left(\left\{S^\alpha, S^\beta, \ldots,
S^n\right\}\right)} f}{\sum_{\left\{S^\alpha, S^\beta, \ldots,
S^n\right\}} e^{L_0\left(\left\{S^\alpha, S^\beta, \ldots,
S^n\right\}\right)} }
\end{align}$$</span></p>
<p><font color='red'>需要完善</font></p>
<h1 id="append">Append</h1>
<h2 id="hessian">Hessian</h2>
<p>Hessian矩阵在极值点的稳定性分析中起着关键作用，具体涉及以下几个方面：</p>
<h3 id="hessian矩阵的定义">Hessian矩阵的定义</h3>
<p>Hessian矩阵是由二阶偏导数组成的方阵，表示函数在某一点的二阶导数信息。对于一个多变量函数
<span
class="math inline"><em>f</em>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>n</em></sub>)</span>，Hessian矩阵
<span class="math inline"><em>H</em></span> 定义为：</p>
<p><span class="math display">$$
H = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial
x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1
\partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2
f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2
\partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2
f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2
f}{\partial x_n^2}
\end{bmatrix}
$$</span></p>
<h3 id="极值点与hessian矩阵">极值点与Hessian矩阵</h3>
<p>在某一点 <span
class="math inline"><em>x</em><sub>0</sub></span>，如果 <span
class="math inline">∇<em>f</em>(<em>x</em><sub>0</sub>) = 0</span>，则
<span class="math inline"><em>x</em><sub>0</sub></span>
是一个驻点，可能是极大值点、极小值点或鞍点。Hessian矩阵 <span
class="math inline"><em>H</em>(<em>x</em><sub>0</sub>)</span>
在这个驻点的性质可以帮助确定这个驻点的类型。</p>
<h3 id="hessian矩阵的正定性与负定性">Hessian矩阵的正定性与负定性</h3>
<ol type="1">
<li><strong>正定矩阵与局部极小值</strong>:
<ul>
<li>一个正定矩阵的所有特征值都是正的。</li>
<li>对于任何非零向量 <span class="math inline"><em>v</em></span>，有
<span
class="math inline"><em>v</em><sup><em>T</em></sup><em>H</em><em>v</em> &gt; 0</span>。</li>
<li>如果在驻点 <span
class="math inline"><em>x</em><sub>0</sub></span>，Hessian矩阵 <span
class="math inline"><em>H</em>(<em>x</em><sub>0</sub>)</span>
是正定的，那么在 <span class="math inline"><em>x</em><sub>0</sub></span>
附近，函数 <span class="math inline"><em>f</em></span>
的曲率向上，类似于一个碗的形状。对于任何微小的偏离 <span
class="math inline"><em>x</em><sub>0</sub></span> 的向量 <span
class="math inline"><em>v</em></span>，二阶泰勒展开式的二次项 <span
class="math inline">$\frac{1}{2} v^T H v$</span> 会贡献一个正值，因此
<span
class="math inline"><em>f</em>(<em>x</em><sub>0</sub> + <em>v</em>) &gt; <em>f</em>(<em>x</em><sub>0</sub>)</span>，即
<span class="math inline"><em>x</em><sub>0</sub></span>
是局部极小值点。</li>
</ul></li>
<li><strong>负定矩阵与局部极大值</strong>:
<ul>
<li>一个负定矩阵的所有特征值都是负的。</li>
<li>对于任何非零向量 <span class="math inline"><em>v</em></span>，有
<span
class="math inline"><em>v</em><sup><em>T</em></sup><em>H</em><em>v</em> &lt; 0</span>。</li>
<li>如果在驻点 <span
class="math inline"><em>x</em><sub>0</sub></span>，Hessian矩阵 <span
class="math inline"><em>H</em>(<em>x</em><sub>0</sub>)</span>
是负定的，那么在 <span class="math inline"><em>x</em><sub>0</sub></span>
附近，函数 <span class="math inline"><em>f</em></span>
的曲率向下，类似于一个倒置的碗的形状。对于任何微小的偏离 <span
class="math inline"><em>x</em><sub>0</sub></span> 的向量 <span
class="math inline"><em>v</em></span>，二阶泰勒展开式的二次项 <span
class="math inline">$\frac{1}{2} v^T H v$</span> 会贡献一个负值，因此
<span
class="math inline"><em>f</em>(<em>x</em><sub>0</sub> + <em>v</em>) &lt; <em>f</em>(<em>x</em><sub>0</sub>)</span>，即
<span class="math inline"><em>x</em><sub>0</sub></span>
是局部极大值点。</li>
</ul></li>
<li><strong>混合正负特征值与鞍点</strong>:
<ul>
<li>如果Hessian矩阵 <span
class="math inline"><em>H</em>(<em>x</em><sub>0</sub>)</span>
既有正特征值也有负特征值，那么它既不正定也不负定。</li>
<li>在这种情况下，存在一些方向 <span
class="math inline"><em>v</em><sub>1</sub></span> 使得 <span
class="math inline"><em>v</em><sub>1</sub><sup><em>T</em></sup><em>H</em><em>v</em><sub>1</sub> &gt; 0</span>，同时存在其他方向
<span class="math inline"><em>v</em><sub>2</sub></span> 使得 <span
class="math inline"><em>v</em><sub>2</sub><sup><em>T</em></sup><em>H</em><em>v</em><sub>2</sub> &lt; 0</span>。</li>
<li>如果在驻点 <span
class="math inline"><em>x</em><sub>0</sub></span>，Hessian矩阵 <span
class="math inline"><em>H</em>(<em>x</em><sub>0</sub>)</span>
既不正定也不负定，那么在 <span
class="math inline"><em>x</em><sub>0</sub></span> 附近，函数 <span
class="math inline"><em>f</em></span>
会在某些方向上向上弯曲（极小值行为），而在其他方向上向下弯曲（极大值行为）。这意味着
<span class="math inline"><em>x</em><sub>0</sub></span>
不是局部极值点，而是一个鞍点，类似于马鞍的形状，在某些方向上是极小值，在另一些方向上是极大值。</li>
</ul></li>
</ol>
<h3 id="实际应用">实际应用</h3>
<p>在优化问题中，Hessian矩阵用于评估驻点的稳定性和类型。在机器学习和深度学习中，Hessian矩阵用于分析损失函数的曲率和优化算法的收敛性。</p>
<p>总结来说，Hessian矩阵通过其正定性或负定性帮助我们区分驻点的类型，从而判断这些点是极大值点、极小值点还是鞍点，这对于理解函数的局部行为和稳定性非常重要。</p>
<h2 id="expleftfracpartialpartial-hright形式算子含义"><span
class="math inline">$\exp\left(\frac{\partial}{\partial
h}\right)$</span>形式算子含义</h2>
<p>(()) 是一个形式算子（formal
operator），它的作用可以理解为将函数沿着其变量 <span
class="math inline"><em>h</em></span>
方向移动一个单位距离。要理解这一点，我们需要结合泰勒级数展开和算子的作用进行说明。</p>
<h3 id="泰勒级数展开">泰勒级数展开</h3>
<p>考虑一个函数 <span
class="math inline"><em>f</em>(<em>h</em>)</span>，在 <span
class="math inline"><em>h</em></span> 处的泰勒展开可以表示为：</p>
<p><span class="math display">$$f(h + a) = \sum_{n=0}^{\infty}
\frac{a^n}{n!} \frac{\partial^n f(h)}{\partial h^n}$$</span></p>
<p>这个展开式表示的是函数 <span
class="math inline"><em>f</em>(<em>h</em>)</span> 在 <span
class="math inline"><em>h</em></span> 点处的 <span
class="math inline"><em>a</em></span> 位移。特别地，当 <span
class="math inline"><em>a</em> = 1</span> 时：</p>
<p><span class="math display">$$f(h + 1) = \sum_{n=0}^{\infty}
\frac{1^n}{n!} \frac{\partial^n f(h)}{\partial h^n} =
\sum_{n=0}^{\infty} \frac{1}{n!} \frac{\partial^n f(h)}{\partial
h^n}$$</span></p>
<h3 id="形式算子-expleftfracpartialpartial-hright">形式算子 <span
class="math inline">$\exp\left(\frac{\partial}{\partial
h}\right)$</span></h3>
<p>形式算子 <span
class="math inline">$\exp\left(\frac{\partial}{\partial
h}\right)$</span> 的定义基于它的泰勒级数展开：</p>
<p><span class="math display">$$\exp\left(\frac{\partial}{\partial
h}\right) = \sum_{n=0}^{\infty} \frac{1}{n!} \left(
\frac{\partial}{\partial h} \right)^n$$</span></p>
<p>当这个算子作用在函数 <span
class="math inline"><em>f</em>(<em>h</em>)</span>
上时，它会应用上述展开式：</p>
<p><span class="math display">$$\exp\left(\frac{\partial}{\partial
h}\right) f(h) = \sum_{n=0}^{\infty} \frac{1}{n!} \left(
\frac{\partial}{\partial h} \right)^n f(h)$$</span></p>
<p>这实际上与泰勒展开式中 <span
class="math inline"><em>a</em> = 1</span> 的情况一致：</p>
<p><span class="math display">$$\exp\left(\frac{\partial}{\partial
h}\right) f(h) = \sum_{n=0}^{\infty} \frac{1}{n!} \frac{\partial^n
f(h)}{\partial h^n} = f(h + 1)$$</span></p>
<h3 id="具体说明">具体说明</h3>
<ol type="1">
<li><p><strong>泰勒展开表示位移</strong>：泰勒级数展开表示函数在其定义点的某一位移。例如，<span
class="math inline"><em>f</em>(<em>h</em> + <em>a</em>)</span> 是函数
<span class="math inline"><em>f</em>(<em>h</em>)</span> 在 <span
class="math inline"><em>h</em></span> 点处的 <span
class="math inline"><em>a</em></span> 位移。</p></li>
<li><p><strong>算子的作用</strong>：<span
class="math inline">$\exp\left(\frac{\partial}{\partial
h}\right)$</span> 作为一个形式算子，其作用相当于将函数沿着其变量 <span
class="math inline"><em>h</em></span>
方向移动一个单位。因为它的作用与泰勒展开的结果一致，当 <span
class="math inline"><em>a</em> = 1</span>
时，正好表示向前移动一个单位距离。</p></li>
<li><p><strong>形式算子的求和性质</strong>：通过展开 <span
class="math inline">$\exp\left(\frac{\partial}{\partial
h}\right)$</span>
的泰勒级数，我们实际上是在累加所有阶数的导数项，这与泰勒级数本身的形式完全吻合，确保了我们得到的结果是函数在
<span class="math inline"><em>h</em></span> 处的一个单位位移。</p></li>
</ol>
<h3 id="总结">总结</h3>
<p><span class="math inline">$\exp\left(\frac{\partial}{\partial
h}\right)$</span> 形式算子的作用可以理解为利用泰勒级数展开的特性，将函数
<span class="math inline"><em>f</em>(<em>h</em>)</span> 沿变量 <span
class="math inline"><em>h</em></span>
的方向移动一个单位距离。这种理解源于泰勒展开中的每一项都对应于函数的一个导数项，而形式算子的展开正好与泰勒展开的累加形式一致，从而实现了位移的效果。</p>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Replica Method</tag>
        <tag>Sherrington-Kirkpatrick Model</tag>
      </tags>
  </entry>
  <entry>
    <title>The Parisi solution of Sherrington-Kirkpatrick Model</title>
    <url>/2022/11/20/Phys/SK_model/SK_model1/</url>
    <content><![CDATA[<p>利用复本方法计算 Sherrington-Kirkpatrick（SK） 模型。该章节主要介绍
Parisi 解。</p>
<p>Reference: * Statistical Physics of Spin Glasses and Information
Processing. Nishimori * <a
href="https://hzshan.github.io/replica_method_in_SK_model.pdf">Replica
calculations for the SK model</a>笔误有点多 * <a
href="https://www.sciencedirect.com/science/article/pii/S0378437199000242">RS
and RSB solutions for SK model with spin-S</a></p>
<p>Link: * <a href="/2022/11/12/Phys/Hopfield/Hopfield/" title="Hopfield Model">Hopfield Model</a> * <a href="/2022/11/13/Phys/replica/replica/" title="Replica">Replica</a> * <a href="/2022/11/20/Phys/SK_model/SK_model/" title="Sherrington-Kirkpatrick Model">Sherrington-Kirkpatrick Model</a></p>
<span id="more"></span>
<p>根据</p>
<p><span class="math display">$$\begin{align}
-\beta[f]= &amp; \lim _{n \rightarrow 0} \frac{\left[Z^n\right]-1}{n
N}=\lim _{n \rightarrow 0}\left\{-\frac{\beta^2 J^2}{4 n} \sum_{\alpha
\neq \beta} q_{\alpha \beta}^2-\frac{\beta J_0}{2 n} \sum_\alpha
m_\alpha^2+\frac{1}{4} \beta^2 J^2+\frac{1}{n} \log \operatorname{Tr}
\mathrm{e}^L\right\} \label{2.17}
\end{align}$$</span></p>
<h1 id="multi-step-replica-symmetry-breaking-rsb">Multi-step replica
symmetry breaking (RSB)</h1>
<p>在复本对称假设<span class="math inline">$\eqref{RS}$</span>中<span
class="math inline"><em>q</em><sub><em>α</em><em>β</em></sub></span>与<span
class="math inline"><em>m</em><sub><em>α</em><em>β</em></sub></span>并不依赖于<span
class="math inline"><em>α</em><em>β</em></span>，为了打破对称需要考虑<span
class="math inline"><em>q</em></span>矩阵的结构。现在考虑 n-RSB
的矩阵，首先对于 1-RSB有<span
class="math inline"><em>m</em><sub>1</sub> &lt; <em>n</em></span>，例如当<span
class="math inline"><em>n</em> = 6, <em>m</em><sub>1</sub> = 3</span>时候，<span
class="math inline"><em>q</em></span>矩阵有（对角线为零的原因在于，当<span
class="math inline"><em>α</em> = <em>β</em></span>时候没有定义）：</p>
<p><span class="math display">$$\begin{align}
\left(\begin{array}{ccc|ccc}
0 &amp; q_1 &amp; q_1 &amp; &amp; &amp; \\
q_1 &amp; 0 &amp; q_1 &amp; &amp; q_0 &amp; \\
q_1 &amp; q_1 &amp; 0 &amp; &amp; &amp; \\
\hline &amp; &amp; &amp; 0 &amp; q_1 &amp; q_1 \\
&amp; q_0 &amp; &amp; q_1 &amp; 0 &amp; q_1 \\
&amp; &amp; &amp; q_1 &amp; q_1 &amp; 0
\end{array}\right)\label{1-RSB}
\end{align}$$</span></p>
<p>可以执行相同的迭代操作，将粒度不断细化，例如再进行对称破缺<span
class="math inline">$\eqref{1-RSB}$</span>左上角的块变为： <span
class="math display">$$\begin{align}
\begin{array}{ccc|ccc}
0 &amp; q_2 &amp; q_2 &amp; &amp; &amp; \\
q_2 &amp; 0 &amp; q_2 &amp; &amp; q_1 &amp; \\
q_2 &amp; q_2 &amp; 0 &amp; &amp; &amp; \\
\hline &amp; &amp; &amp; 0 &amp; q_2 &amp; q_2 \\
&amp; q_1 &amp; &amp; q_2 &amp; 0 &amp; q_2 \\
&amp; &amp; &amp; q_2 &amp; q_2 &amp; 0
\end{array} \label{2-RSB}
\end{align}$$</span></p>
<p>同时应当满足条件：</p>
<p><span class="math display">$$\begin{align}
n \geq m_1 \geq m_2 \geq \ldots \geq 1
\end{align}$$</span></p>
<p>定义函数： <span class="math display">$$\begin{align}
q(x)=q_i \quad\left(m_{i+1} \leq x \leq m_i\right)
\end{align}$$</span></p>
<p>进行复本对称的极限变换<span
class="math inline"><em>n</em> → 0</span>，将不等关系进行任意的翻转<font color='red'>（令人费解的反号）</font>：</p>
<p><span class="math display">$$\begin{align}
0 \leq m_1 \leq m_2 . . \leq 1
\end{align}$$</span></p>
<h1 id="first-step-rsb">First step RSB</h1>
<p>再次回到计算<span
class="math inline">[<em>Z</em><sup><em>n</em></sup>]</span>，结合<span
class="math inline">$\eqref{19}$</span>，在<span
class="math inline"><em>J</em><sub>0</sub> = <em>h</em> = 0</span>的假设下，将<span
class="math inline">$\eqref{1-RSB}$</span>融入进去有：</p>
<p><span class="math display">$$\begin{align}
\sum_{\alpha&lt;\beta} q_{\alpha \beta} S^\alpha
S^\beta=\frac{1}{2}\left\{q_0\left(\sum_\alpha^n
S^\alpha\right)^2+\left(q_1-q_0\right) \sum_{b=1}^{n /
m_1}\left(\sum_{\alpha \in B_b}^{m_1} S^\alpha\right)^2-n q_1\right\}
\label{67}
\end{align}$$</span></p>
<p>其中<span class="math inline"><em>B</em><sub><em>b</em></sub></span>
表示 <span class="math inline"><em>b</em></span>-th
block。第一项是没有进行复本破缺的，第二项是表示一阶复本破缺，第三项将对角线的贡献排除。同样的，对于<span
class="math inline">$\eqref{28}$</span>有：</p>
<p><span class="math display">$$\begin{align}
\lim _{n \rightarrow 0} \frac{1}{n} \sum_{\alpha \neq \beta} q_{\alpha
\beta}^2=\lim _{n \rightarrow 0} \frac{1}{n}\left\{n^2
q_0^2+\frac{n}{m_1} m_1^2\left(q_1^2-q_0^2\right)-n
q_1^2\right\}=\left(m_1-1\right) q_1^2-m_1 q_0^2 \label{68}
\end{align}$$</span></p>
<p>此时已经将自由能中含有<span
class="math inline"><em>q</em><sub><em>α</em><em>β</em></sub></span>的地方进行替换，接下来分析<span
class="math inline">$\eqref{28}$</span>。首先将<span
class="math inline">$\eqref{68}$</span>代入：</p>
<p><span class="math display">$$\begin{align}
\beta\left[f_{1 R S B}\right]=\frac{\beta^2
J^2}{4}\left\{\left(m_1-1\right) q_1^2-m_1 q_0^2-1\right\}+\frac{\beta
J_0}{2} m^2-\frac{1}{n} \log \sum_{\left\{S^\alpha, S^3, \ldots,
S^n\right\}} \exp \left(L_{1 R S B}\right) \label{70}
\end{align}$$</span></p>
<p>再将<span class="math inline">$\eqref{67}$</span>代入<span
class="math inline">$\eqref{19}$</span>：</p>
<p><span class="math display">$$
\begin{gather}
L\left(\left\{q_{\alpha \beta}, m_\alpha\right\}\right):=\beta^2 J^2
\sum_{\alpha&lt;\beta} q_{\alpha \beta} S^\alpha S^\beta+\beta
\sum_\alpha\left(J_0 m_\alpha+h\right) S^\alpha \label{71} \\
\Rightarrow L_{1 R S B}=\frac{\beta^2
J^2}{2}\left\{q_0\left(\sum_\alpha^n
S^\alpha\right)^2+\left(q_1-q_0\right) \sum_{b=1}^{n /
m_1}\left(\sum_{\alpha \in B_b}^{m_1} S^\alpha\right)^2-n
q_1\right\}+\beta \sum_\alpha\left(J_0 m_\alpha+h\right) S^\alpha
\label{72}
\end{gather}
$$</span></p>
<p>由于里面有<span class="math inline">$\left(\sum_\alpha^n
S^\alpha\right)^2$</span>和<span class="math inline">$\left(\sum_{\alpha
\in B_b}^{m_1}
S^\alpha\right)^2$</span>，需要将平方项变为线性项，同样使用<span
class="math inline">$\eqref{11}$</span>的办法：</p>
<p><span class="math display">$$
\begin{align}
&amp; \frac{1}{n} \log \sum_{\left\{S^\alpha, S^\beta, \ldots,
S^n\right\}} \exp \left(L_{1 R S B}\right) \\
&amp; =\frac{1}{n} \log \sum_{\left\{S^\alpha, S^\beta, \ldots,
S^n\right)} \int D u \exp \left(\beta J \sqrt{q_0} u \sum_\alpha
S^\alpha-\frac{n}{2} \beta^2 J^2 q_1+\sum_\alpha S^\alpha \beta\left(J_0
m+h\right)+\left(q_1-q_0\right)\frac{\beta^2 J^2}{2} \sum_{b=1}^{n /
m_1}\left(\sum_{\alpha \in B_b}^{m_1} S^\alpha\right)^2\right) \\
&amp; =-\frac{1}{2} \beta^2 J^2 q_1+\underbrace{\frac{1}{n} \log
\sum_{\left\{S^\alpha, S^\beta, \ldots, S^n\right)} \int D u \exp
\left(\sum_\alpha S^\alpha\left(\beta J \sqrt{q_0} u+\beta\left(J_0
m+h\right)\right)+\left(q_1-q_0\right)\frac{\beta^2 J^2}{2}
\sum_{b=1}^{n / m_1}\left(\sum_{\alpha \in B_b}^{m_1}
S^\alpha\right)^2\right)}_{\Delta} \label{72_2}
\end{align}
$$</span></p>
<p>引入<span
class="math inline"><em>v</em><sub><em>b</em></sub></span>线性化<span
class="math inline">$\left(\sum_{\alpha \in B_b}^{m_1}
S^\alpha\right)^2$</span>：</p>
<p><span class="math display">$$
\begin{align}
&amp; \frac{1}{n} \log \sum_{\left\{S^\alpha, S^\beta, \ldots,\right\}}
\int D u \prod_{b=1}^{n / m_1}\left(\int D v_b\right) \exp
\left(\sum_\alpha S^\alpha \beta J \sqrt{q_0} u+\sum_\alpha S^\alpha
\beta\left(J_0 m+h\right)+\beta J \sqrt{q_1-q_0} \sum_{b=1}^{n / m_1}
v_b \sum_{\alpha \in B_b}^{m_1} S^\alpha\right) \\
= &amp; \frac{1}{n} \log \int D u \sum_{\left\{S^\alpha, S^\beta,
\ldots, S^n\right\}} \prod_{b=1}^{n / m_1}\left\{\int v e \exp
\left(\sum_\alpha S^\alpha \beta\left(J_0 m+h+\beta J \sqrt{q_0}
u\right)\right) \exp \left(\beta J \sqrt{q_1-q_0} v \sum_{\alpha \in
B_b}^{m_1} S^\alpha\right)\right\} \\
= &amp; \frac{1}{n} \log \int D u \prod_{b=1}^{n / m_1}\left\{\int D v_b
\sum_{\left\{S^\alpha\right\} \in B_b} \exp
\left(\sum_{\left\{S^\alpha\right\} \in B_b} S^\alpha\beta\left(J_0
m+h+\beta J \sqrt{q_0} u+J \sqrt{q_1-q_0} v_b\right)\right)\right\} \\
= &amp; \frac{1}{n} \log \int D u \prod_{b=1}^{n / m_1}\left\{\int D
v_b\left\{\sum_{S= \pm 1} \exp \left(S \beta\left(J_0 m+h+\beta J
\sqrt{q_0} u+J \sqrt{q_1-q_0} v_b\right)\right)\right\}^{m_1}\right\} \\
= &amp; \frac{1}{n} \log \int D u\left\{\int v\left\{\sum_{S= \pm 1}
\exp \left(S \beta\left(J_0 m+h+\beta J \sqrt{q_0} u+J \sqrt{q_1-q_0}
v\right)\right)\right\}^{m_1}\right\}^{n / m_1} \\
= &amp; \frac{1}{n} \log \int D u\left\{\int D v\left\{2 \cosh
\left(\beta\left(J_0 m+h+\beta J \sqrt{q_0} u+J \sqrt{q_1-q_0}
v\right)\right)\right\}^{m_1}\right\}^{n / m_1} \\
= &amp; \frac{1}{n} \log \int D u \exp \left\{\frac{n}{m_1} \log
\left\{\int D v\left\{2 \cosh \left(\beta\left(J_0 m+h+\beta J
\sqrt{q_0} u+J \sqrt{q_1-q_0}
v\right)\right)\right\}^{m_1}\right\}\right\} \\
\approx &amp; \frac{1}{n} \log \left\{1+\frac{n}{m_1} \int D u \log
\left\{\int D v\left\{2 \cosh \left(\beta\left(J_0 m+h+\beta J
\sqrt{q_0} u+J \sqrt{q_1-q_0}
v\right)\right)\right\}^{m_1}\right\}\right\} \text { expand exponent
around } 0 \\
\approx &amp; \frac{1}{n} \frac{n}{m_1} \int D u \log \left\{\int D
v\left\{2 \cosh \left(\beta\left(J_0 m+h+\beta J \sqrt{q_0} u+J
\sqrt{q_1-q_0} v\right)\right)\right\}^{m_1}\right\} \text { expand
exponent around } 1 \\
= &amp; \log 2+\frac{1}{m_1} \int D u \log \left\{\int D v\left\{\cosh
\left(\beta\left(J_0 m+h+\beta J \sqrt{q_0} u+J \sqrt{q_1-q_0}
v\right)\right)\right\}^{m_1}\right\} \text { pull the factor of } 2
\text { out } \label{82}
\end{align}
$$</span></p>
<p>令： <span class="math display">$$\begin{align}
\Xi=\beta\left(J_0 m+h+\beta J \sqrt{q_0} u+J \sqrt{q_1-q_0} v\right)
\label{86}
\end{align}$$</span></p>
<p>有： <span class="math display">$$\begin{align}
\Delta=\log 2+\frac{1}{m_1} \int D u \log \left\{\int D v\{\cosh
\Xi\}^{m_1}\right\} \label{87}
\end{align}$$</span></p>
<p>结合<span class="math inline">$\eqref{72_2}\eqref{87}$</span>将<span
class="math inline">$\eqref{70}$</span>写为： <span
class="math display">$$\begin{align}
\beta\left[f_{1 R S B}\right]=\frac{\beta^2
J^2}{4}\left\{\left(m_1-1\right) q_1^2-m_1 q_0^2+2
q_1-1\right\}+\frac{\beta J_0}{2} m^2-\log 2-\frac{1}{m_1} \int D u \log
\left\{\int D v\{\cosh \Xi\}^{m_1}\right\} .
\end{align}$$</span></p>
<p>得到极值点为： <span class="math display">$$\begin{gather}
m^*=\int D u \frac{\int D v\{\cosh \Xi\}^{m_1} \tanh \Xi}{\int D
v\{\cosh \Xi\}^{m_1}} \\
q_0^* =\int D u\left(\frac{\int D v\{\cosh \Xi\}^{m_1} \tanh \Xi}{\int D
v\{\cosh \Xi\}^{m_1}}\right)^2 \\
q_1^* =\int D u \frac{\int D v\{\cosh \Xi\}^{m_1}\{\tanh \Xi\}^2}{\int D
v\{\cosh \Xi\}^{m_1}}
\end{gather}$$</span></p>
<p><img src="./phase.png" alt="phase diagram" /> <a
href="./phase_diagram.nb">mathematica代码</a>与<a
href="./phase.py">python代码</a></p>
<h1 id="full-rsb-solution">Full RSB Solution</h1>
<p>由于熵还是负数，接下来进行更高阶的复本对称破缺（k-RSB）计算<span
class="math inline">$\eqref{28}$</span>。与一阶复本对称<span
class="math inline">$\eqref{67}$</span>类似:</p>
<p><span class="math display">$$\begin{align}
\sum_{\alpha \neq \beta} q_{\alpha \beta}^l &amp; =q_0^l
n^2+\left(q_1^l-q_0^l\right) m_1^2 \cdot
\frac{n}{m_1}+\left(q_2^l-q_1^l\right) m_2^2 \cdot \frac{m_1}{m_2} \cdot
\frac{n}{m_1}+\cdots-q_K^l \cdot n \\ &amp; =n
\sum_{j=0}^K\left(m_j-m_{j+1}\right) q_j^l \label{3.37}
\end{align}$$</span></p>
<p>其中<span class="math inline"><em>l</em></span>是任意整数，<span
class="math inline"><em>m</em><sub>0</sub> = <em>n</em>, <em>m</em><sub><em>K</em> + 1</sub> = 1</span>，在极限情况<span
class="math inline"><em>n</em> → 0</span>下，使用<span
class="math inline"><em>m</em><sub><em>j</em></sub> − <em>m</em><sub><em>j</em> + 1</sub> → −<em>d</em><em>x</em></span>，可以得到：</p>
<p><span class="math display">$$\begin{align}
\frac{1}{n} \sum_{\alpha \neq \beta} q_{\alpha \beta}^l
\rightarrow-\int_0^1 q^l(x) \mathrm{d} x \label{3.38}
\end{align}$$</span></p>
<p>在<span
class="math inline"><em>J</em><sub>0</sub> = 0, <em>h</em> = 0</span>的条件下，结合<span
class="math inline">$E=-\frac{\partial \log Z}{\partial
\beta}$</span>、<span class="math inline">$\chi=-\frac{\partial^2
F}{\partial h^2}$</span>与<span
class="math inline">$\eqref{28}$</span>，可得：</p>
<p><font color='red'>没有证明</font></p>
<p><span class="math display">$$\begin{gather}
E=-\frac{\beta J^2}{2}\left(1+\frac{2}{n} \sum_{\alpha&lt;\beta}
q_{\alpha \beta}^2\right) \rightarrow-\frac{\beta
J^2}{2}\left(1-\int_0^1 q^2(x) \mathrm{d} x\right) \\
\chi=\beta\left(1+\frac{1}{n} \sum_{\alpha \neq \beta} q_{\alpha
\beta}\right) \rightarrow \beta\left(1-\int_0^1 q(x) \mathrm{d} x\right)
\end{gather}$$</span></p>
<h2 id="parisi-equation">parisi equation</h2>
<p>接下来需要将高阶复本破缺技巧应用在自由能<span
class="math inline">$\eqref{2.17}$</span>，和计算一阶复本对称的操作一样，首先计算<span
class="math inline"><em>q</em><sub>0</sub></span>然后计算第一次分块<span
class="math inline"><em>q</em><sub>1</sub></span>的贡献并且排除<span
class="math inline"><em>q</em><sub>0</sub></span>的影响，以此类推，在主对角线上是<span
class="math inline"><em>q</em><sub><em>K</em></sub></span>。首先计算如下项：</p>
<p><span class="math display">$$
\begin{align}
G &amp; =\operatorname{Tr} \exp \left(\frac{1}{2} \sum_{\alpha,
\beta=1}^n q_{\alpha \beta} S^\alpha S^\beta+h \sum_\alpha^n
S^\alpha\right) \\
&amp; =\left.\exp \left(\frac{1}{2} \sum_{\alpha, \beta} q_{\alpha
\beta} \frac{\partial^2}{\partial h_\alpha \partial h_\beta}\right)
\prod_\alpha 2 \cosh h_\alpha\right|_{h_\alpha=h}  \label{B.1}
\end{align}
$$</span></p>
<p>这里将<span
class="math inline"><em>q</em><sub><em>α</em><em>β</em></sub></span>贡献的项目视为<span
class="math inline"><em>h</em></span>项的偏移，因此使用<span
class="math inline">$\exp(\frac{\partial^2}{\partial h_\alpha \partial
h_\beta})$</span>的形式表示这个偏移量。如果使用复本对称的假设，则：</p>
<p><span class="math display">$$\begin{align}
G=\exp \left(\frac{q}{2} \frac{\partial^2}{\partial h^2}\right)(2 \cosh
h)^n \label{B.2}
\end{align}$$</span></p>
<p>其中使用到：</p>
<p><span class="math display">$$
\left.\sum_\alpha \frac{\partial f\left(h_1, \ldots,
h_n\right)}{\partial h_\alpha}\right|_{h_\alpha=h}=\frac{\partial f(h,
\ldots, h)}{\partial h}
$$</span></p>
<p>从<span
class="math inline">$\eqref{67}$</span>中可以看出，当时的思想是从复本对称假设出发，然后将整个矩阵分为几部分（取决于<span
class="math inline">$\frac{n}{m}$</span>大小），在主对角线的块上为一阶复本对称<span
class="math inline"><em>q</em><sub>1</sub></span>；那么同样的二阶复本对称可以将每一个一阶复本对称的块分为几部分，然后将主对角线的块作为二阶复本<span
class="math inline"><em>q</em><sub>2</sub></span>；这个流程以此类推。现在可以反过来，从<span
class="math inline"><em>K</em></span>阶复本逐渐变为<span
class="math inline">0</span>阶级复本：</p>
<ol type="1">
<li>对于<span class="math inline"><em>K</em> − 1</span>阶复本有<span
class="math inline">(<em>q</em><sub><em>K</em></sub> − <em>q</em><sub><em>K</em> − 1</sub>)<em>I</em>(<em>m</em><sub><em>K</em></sub>)</span>，表示<span
class="math inline">(<em>m</em><sub><em>K</em></sub> × <em>m</em><sub><em>K</em></sub>)</span>大小。</li>
<li>生长出<span class="math inline">(<em>K</em> − 2)</span>复本，将<span
class="math inline"><em>K</em></span>阶复本通过<span
class="math inline">Diag<sub><em>K</em> − 1</sub></span>扩张<span
class="math inline">  (<em>q</em><sub><em>K</em></sub> − <em>q</em><sub><em>K</em> − 1</sub>)Diag<sub><em>K</em> − 1</sub>[<em>I</em>(<em>m</em><sub><em>K</em></sub>)]</span>，然后<span
class="math inline"><em>K</em> − 2</span>阶写为<span
class="math inline">(<em>q</em><sub><em>K</em> − 1</sub> − <em>q</em><sub><em>K</em> − 2</sub>)<em>I</em>(<em>m</em><sub><em>K</em> − 1</sub>)</span>矩阵的大小为<span
class="math inline">(<em>m</em><sub><em>K</em> − 1</sub> × <em>m</em><sub><em>K</em> − 1</sub>)</span>。整体为:
<span
class="math display">  (<em>q</em><sub><em>K</em></sub> − <em>q</em><sub><em>K</em> − 1</sub>)Diag<sub><em>K</em> − 1</sub>[<em>I</em>(<em>m</em><sub><em>K</em></sub>)] + (<em>q</em><sub><em>K</em> − 1</sub> − <em>q</em><sub><em>K</em> − 2</sub>)<em>I</em>(<em>m</em><sub><em>K</em> − 1</sub>)</span></li>
<li>重复步骤2，得到<span
class="math inline"><em>K</em> − 3</span>复本。矩阵的大小为<span
class="math inline">(<em>m</em><sub><em>K</em> − 2</sub> × <em>m</em><sub><em>K</em> − 2</sub>)</span>
<span
class="math display">  (<em>q</em><sub><em>K</em></sub> − <em>q</em><sub><em>K</em> − 1</sub>)Diag<sub><em>K</em> − 2</sub>[<em>I</em>(<em>m</em><sub><em>K</em></sub>)] + (<em>q</em><sub><em>K</em> − 1</sub> − <em>q</em><sub><em>K</em> − 2</sub>)Diag<sub><em>K</em> − 2</sub>[<em>I</em>(<em>m</em><sub><em>K</em> − 1</sub>)] + (<em>q</em><sub><em>K</em> − 2</sub> − <em>q</em><sub><em>K</em> − 3</sub>)<em>I</em>(<em>m</em><sub><em>K</em> − 2</sub>)</span></li>
</ol>
<p>在清楚以上步骤之后，接下来首先处理<span
class="math inline">(<em>m</em><sub><em>K</em></sub> × <em>m</em><sub><em>K</em></sub>)</span>大小矩阵<span
class="math inline">Tr </span>，并将结果标注为<span
class="math inline"><em>g</em>(<em>m</em><sub><em>K</em></sub>, <em>h</em>)</span>，由于这是复本破缺的最后一次，因此这个矩阵元是复本对称的，由<span
class="math inline">$\eqref{B.2}$</span>得到：</p>
<p><span class="math display">$$\begin{align}
g\left(m_K, h\right)=\exp \left\{\frac{1}{2}\left(q_K-q_{K-1}\right)
\frac{\partial^2}{\partial h^2}\right\}(2 \cosh h)^{m_K} \label{B.4}
\end{align}$$</span></p>
<p>对于<span class="math inline"><em>K</em> − 1</span>阶，是考虑将<span
class="math inline"><em>g</em>(<em>m</em><sub><em>K</em></sub>, <em>h</em>)</span>复制到对角部分，然后每一个元素相加<span
class="math inline"><em>q</em><sub><em>K</em> − 1</sub> − <em>q</em><sub><em>K</em> − 2</sub></span>，从<span
class="math inline">$\eqref{B.2}$</span>可以得到：</p>
<p><span class="math display">$$\begin{align}
g\left(m_{K-1}, h\right)=\exp
\left\{\frac{1}{2}\left(q_{K-1}-q_{K-2}\right)
\frac{\partial^2}{\partial h^2}\right\}\left[g\left(m_K,
h\right)\right]^{m_{K-1} / m_K} \label{B.5}
\end{align}$$</span></p>
<p>一直重复这个过程，直到复本对称项： <span
class="math display">$$\begin{align}
G=g(n, h)=\exp \left\{\frac{1}{2} q(0) \frac{\partial^2}{\partial
h^2}\right\}\left[g\left(m_1, h\right)\right]^{n / m_1}  \label{B.6}
\end{align}$$</span></p>
<p>考虑到极限<span class="math inline"><em>n</em> → 0</span>下有<span
class="math inline"><em>m</em><sub><em>j</em></sub> − <em>m</em><sub><em>j</em> − 1</sub> = −d<em>x</em></span>，将<span
class="math inline">$\eqref{B.5}$</span>写为：</p>
<p><span class="math display">$$\begin{align}
g(x+\mathrm{d} x, h)=\exp \left\{-\frac{1}{2} \mathrm{~d} q(x)
\frac{\partial^2}{\partial h^2}\right\} g(x, h)^{1+\mathrm{d} \log x}
\label{B.7}
\end{align}$$</span></p>
<p>对于<span class="math inline">$\eqref{B.4}$</span>，当<span
class="math inline"><em>K</em> → ∞</span>有<span
class="math inline"><em>m</em><sub><em>k</em></sub> → 1</span>，因为存在<span
class="math inline"><em>g</em>(1, <em>h</em>) = 2cosh <em>h</em></span>。将<span
class="math inline">$\eqref{B.7}$</span>转化为微分形式（对<span
class="math inline">d<em>q</em></span>与<span
class="math inline">d<em>x</em></span>进行展开）：</p>
<p><span class="math display">$$
\frac{\partial g}{\partial x}=-\frac{1}{2} \frac{\mathrm{d}
q}{\mathrm{~d} x} \frac{\partial^2 g}{\partial h^2}+\frac{1}{x} g \log g
$$</span></p>
<p>利用注记<span
class="math inline"><em>f</em><sub>0</sub>(<em>x</em>, <em>h</em>) = (1/<em>x</em>)log <em>g</em>(<em>x</em>, <em>h</em>)</span>改写为：</p>
<p><span class="math display">$$\begin{align}
\frac{\partial f_0}{\partial x}=-\frac{1}{2} \frac{\mathrm{d}
q}{\mathrm{~d} x}\left\{\frac{\partial^2 f_0}{\partial
h^2}+x\left(\frac{\partial f_0}{\partial h}\right)^2\right\} \label{B.8}
\end{align}$$</span></p>
<p>分析<span
class="math inline">$\eqref{B.6}$</span>的极限情况，根据之前的讨论知道，当<span
class="math inline"><em>n</em> → 0</span>的时候，<span
class="math inline"><em>m</em><sub>1</sub> → 0</span>：</p>
<p><span class="math display">$$\begin{align}
\frac{1}{n} \log \operatorname{Tr} \mathrm{e}^L &amp;= \left. \exp
\left(\frac{1}{2} q(0) \frac{\partial^2}{\partial h^2}\right)
\frac{1}{x} \log g(x, h)\right|_{x, h \rightarrow 0} \\
&amp; =\left.\exp \left(\frac{1}{2} q(0) \frac{\partial^2}{\partial
h^2}\right) f_0(0, h)\right|_{h \rightarrow 0} \\
&amp; =\int \mathrm{D} u f_0(0, \sqrt{q(0)} u) \label{B.10}
\end{align}$$</span></p>
<p>结合<span
class="math inline">$\eqref{3.38},\eqref{B.10}$</span>将<span
class="math inline">$\eqref{2.17}$</span>写为<font color='red'><span
class="math inline"><em>q</em>(1)</span>这一项怎么来的？</font>：</p>
<p><span class="math display">$$\begin{align}
\beta f=-\frac{\beta^2 J^2}{4}\left\{1+\int_0^1 q(x)^2 \mathrm{~d} x-2
q(1)\right\}-\int \mathrm{D} u f_0(0, \sqrt{q(0)} u) \label{3.41}
\end{align}$$</span></p>
<p>其中<span class="math inline"><em>f</em><sub>0</sub></span>应当满足
Parisi equation，即<span class="math inline">$\eqref{B.9}$</span>:</p>
<p><span class="math display">$$\begin{align}
\frac{\partial f_0(x, h)}{\partial x}=-\frac{J^2}{2} \frac{\mathrm{d}
q}{\mathrm{~d} x}\left\{\frac{\partial^2 f_0}{\partial
h^2}+x\left(\frac{\partial f_0}{\partial h}\right)^2\right\}
\label{3.42}
\end{align}$$</span></p>
<p>有 <span
class="math inline"><em>f</em><sub>0</sub>(1, <em>h</em>) = log 2cosh <em>β</em><em>h</em></span>。</p>
<h2 id="order-parameter-near-the-critical-point">Order parameter near
the critical point</h2>
<p>虽然求解<span
class="math inline">$\eqref{3.41}$</span>十分困难，但是在临界点附近的性质是可以研究的。</p>
<p>在<span
class="math inline"><em>J</em><sub>0</sub> = <em>h</em> = 0</span>的条件下，<span
class="math inline">$\eqref{2.17}$</span>展为4阶可以是：</p>
<p><font color='red'>未经计算，直接抄的，并且十分简略</font></p>
<p><span class="math display">$$
\begin{align}
\beta f= &amp; \lim _{n \rightarrow 0}
\frac{1}{n}\left\{\frac{1}{4}\left(\frac{T^2}{T_{\mathrm{f}}^2}-1\right)
\operatorname{Tr} Q^2-\frac{1}{6} \operatorname{Tr} Q^3 - \frac{1}{8}
\operatorname{Tr} Q^4+\frac{1}{4} \sum_{\alpha \neq \beta \neq \gamma}
Q_{\alpha \beta}^2 Q_{\alpha \gamma}^2-\frac{1}{12} \sum_{\alpha \neq
\beta} Q_{\alpha \beta}^4\right\},
\end{align}
$$</span></p>
<p>其中<span
class="math inline"><em>Q</em><sub><em>α</em><em>β</em></sub> = (<em>β</em><em>J</em>)<sup>2</sup><em>q</em><sub><em>α</em><em>β</em></sub></span>，令<span
class="math inline">$\theta=\frac{T_f-T}{T_f}$</span>并且考虑复本极限<span
class="math inline"><em>n</em> → 0</span>有：</p>
<p><span class="math display">$$
\beta f=\frac{1}{2} \int_0^1 \mathrm{~d} x\left\{|\theta|
q^2(x)-\frac{1}{3} x q^3(x)-q(x) \int_0^x q^2(y) \mathrm{d}
y+\frac{1}{6} q^4(x)\right\}
$$</span></p>
<p>对<span class="math inline"><em>q</em>(<em>x</em>)</span>求导：</p>
<p><span class="math display">$$
2|\theta| q(x)-x q^2(x)-\int_0^x q^2(y) \mathrm{d} y-2 q(x) \int_x^1
q(y) \mathrm{d} y+\frac{2}{3} q^3(x)=0
$$</span></p>
<p>持续求微分得到结果： <span
class="math display">|<em>θ</em>|−<em>x</em><em>q</em>(<em>x</em>) − ∫<sub><em>x</em></sub><sup>1</sup><em>q</em>(<em>y</em>)d<em>y</em> + <em>q</em><sup>2</sup>(<em>x</em>) = 0  
or   <em>q</em><sup>′</sup>(<em>x</em>) = 0</span></p>
<p><span class="math display">$$
q(x)=\frac{x}{2} \quad \text { or } \quad q^{\prime}(x)=0
$$</span></p>
<p>最后得到的解与<span class="math inline">|<em>θ</em>|</span>有关：
<span class="math display">$$
\begin{align}
&amp; q(x)=\frac{x}{2} \quad\left(0 \leq x \leq x_1=2 q(1)\right) \\
&amp; q(x)=q(1) \quad\left(x_1 \leq x \leq 1\right) \\
&amp; q(1)=|\theta|+\mathcal{O}\left(\theta^2\right)
\end{align}
$$</span></p>
<p>当接近相变点时，<span
class="math inline"><em>q</em>(<em>x</em> = 0)</span>。</p>
<figure>
<img src="./q_x.png" alt="q(x)" />
<figcaption aria-hidden="true"><span
class="math inline"><em>q</em>(<em>x</em>)</span></figcaption>
</figure>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Replica Method</tag>
        <tag>Sherrington-Kirkpatrick Model</tag>
        <tag>Parisi solution</tag>
        <tag>Replica Symmetry Breaking</tag>
      </tags>
  </entry>
  <entry>
    <title>Feynman path integrals</title>
    <url>/2024/05/26/Phys/Path_integrals/Path_integrals/</url>
    <content><![CDATA[<p>考虑到状态的演化，最直接的想法是通过经典力学通过拉格朗日量来构建经典的运动路径。如何处理的问题不是精确的状态，而是概率密度的分布，如何求概率密度分布的演化？路径积分则是十分自然的想法。将每一个小的状态，以任意的路径进行演化，最后得到最终转态的概率分布。这样处理的麻烦在于，不是每一条路径都是等价的，有一些路径发生的概率高一些，另一些则低一些。如何表示这样的概率分布，以及实际得到演化后的概率分布，则是本文要探讨的。</p>
<p>当然，这只是一个简单的版本，需要更多的讨论。</p>
<p>参考： * <a href="https://zhuanlan.zhihu.com/p/275827978">QM -
路径积分 (Path Integral) PT. 1 - 基本构架</a> * <a
href="https://zhuanlan.zhihu.com/p/270684456">QM - 路径积分 (Path
Integral) PT. 2 - 求解实例</a></p>
<span id="more"></span>
<h1 id="路径积分的基本构架">路径积分的基本构架</h1>
<p><font color='red'></p>
<p>本部分的目标： 1. 构建传播子 2. 相空间、位形空间的形式 3.
以自由粒子为例，展示两种形式</p>
<p></font></p>
<p>传播子是研究态随时间演化的方法，这样演化过程由基本方程确定，在量子力学中这样的方程是薛定谔方程。但是随时间演化的方程有很多，这些同样可以在传播子的框架下研究。</p>
<p>由于薛定谔方程过于出名，接下来的讨论在薛定谔方程下进行。</p>
<h2 id="传播子">传播子</h2>
<p>定义一个算符 <span
class="math inline"><em>U</em></span>，它的作用是给出演化结果:</p>
<p><span class="math display">$$\begin{align}
U\left(t-t_0\right)\left|\psi\left(t_0\right)\right\rangle=|\psi(t)\rangle
\end{align}$$</span></p>
<p>然后求解 <span class="math inline"><em>U</em></span>
的表达式，研究时间演化最熟悉的是薛定谔方程: <span
class="math display">$$\begin{align}
&amp; i \hbar \frac{\mathrm{d}}{\mathrm{d}
t}|\psi(t)\rangle=H|\psi(t)\rangle \\
\Rightarrow &amp; \frac{\mathrm{d}}{\mathrm{d} t}
U\left(t-t_0\right)\left|\psi\left(t_0\right)\right\rangle=-i
\frac{H}{\hbar}
U\left(t-t_0\right)\left|\psi\left(t_0\right)\right\rangle \\
\Rightarrow &amp; \frac{\mathrm{d}}{\mathrm{d} t} U\left(t-t_0\right)=-i
\frac{H}{\hbar} U\left(t-t_0\right) \\
\Rightarrow &amp; \mathrm{d} \ln U\left(t-t_0\right)=-i \frac{H}{\hbar}
\mathrm{d} t \\
\Rightarrow &amp; U\left(t-t_0\right)=e^{-i \frac{H}{\hbar} t} \cdot
\text { const }
\end{align}$$</span></p>
<p>如果时间不变自然不发生演化，以此作为初始条件，即 <span
class="math inline"><em>t</em> = <em>t</em><sub>0</sub> ⇒ <em>U</em> = <em>I</em></span>。确定系数之后得到了么正时间演化算符的表达式:
<span class="math display">$$\begin{align}
U\left(t-t_0\right)=e^{-i \frac{H}{\hbar}\left(t-t_0\right)}
\end{align}$$</span></p>
<p>在形式上地使用： <span class="math display">$$\begin{align}
|\psi(t)\rangle=U\left(t-t_0\right)\left|\psi\left(t_0\right)\right\rangle
\label{1_U}
\end{align}$$</span></p>
<p>接下来<span
class="math inline">$\eqref{1_U}$</span>式取坐标表象，即左乘 <span
class="math inline">⟨<em>r⃗</em>|</span>，再插入完备性关系式 <span
class="math inline"><em>I</em> = ∫|<em>r⃗</em><sub>0</sub>⟩⟨<em>r⃗</em><sub>0</sub>|d<em>r⃗</em><sub>0</sub></span>，可得：
<span class="math display">$$\begin{align}
\psi(\vec{r}, t)&amp;=\int\left\langle\vec{r}\left|U\left(t,
t_0\right)\right| \vec{r}_0\right\rangle\left\langle\vec{r}_0 \mid
\psi\left(t_0\right)\right\rangle \mathrm{d} \vec{r}_0 \\
&amp;=\int K\left(\vec{r}, t ; \vec{r}_0, t_0\right)
\psi\left(\vec{r}_0, t_0\right) \mathrm{d} \vec{r}_0 \\
K\left(\vec{r}, t ; \vec{r}_0, t_0\right) &amp;=
\left\langle\vec{r}\left|U\left(t, t_0\right)\right|
\vec{r}_0\right\rangle
\end{align}$$</span></p>
<p>其中 <span
class="math inline"><em>K</em>(<em>r⃗</em>, <em>t</em>; <em>r⃗</em><sub>0</sub>, <em>t</em><sub>0</sub>)</span>
称作传播子, 作用如上所示, 路径积分的最终目的就是得到其具体表达式。</p>
<h2 id="传播子的性质">传播子的性质</h2>
<p>设置时间节点（无特殊声明全文适用）：</p>
<p><span class="math display">$$\begin{align}
t_0&lt;t_n \leq t=t_N, n \in\{1,2,3, \cdots, N\}
\end{align}$$</span></p>
<p>注意不平均分割时间。由传播子的定义，我们不难发现它具有如下特性：</p>
<p><span class="math display">$$\begin{align}
K\left(\vec{r}, t ; \vec{r}_0, t_0\right) &amp;
=\left\langle\vec{r}\left|U\left(t, t_0\right)\right|
\vec{r}_0\right\rangle \\
&amp; =\left\langle\vec{r}\left|U\left(t, t_1\right) U\left(t_1,
t_0\right)\right| \vec{r}_0\right\rangle \\
&amp; =\int\left\langle\vec{r}\left|U\left(t, t_1\right)\right|
\vec{r}_1\right\rangle\left\langle\vec{r}_1\left|U\left(t_1,
t_0\right)\right| \vec{r}_0\right\rangle \mathrm{d} \vec{r}_1 \\
&amp; =\int\left\langle\vec{r}\left|U\left(t, t_2\right)\right|
\vec{r}_2\right\rangle\left\langle\vec{r}_2\left|U\left(t_2,
t_1\right)\right|
\vec{r}_1\right\rangle\left\langle\vec{r}_1\left|U\left(t_1,
t_0\right)\right| \vec{r}_0\right\rangle \mathrm{d} \vec{r}_1
\mathrm{~d} \vec{r}_2 \\
&amp; =\int K\left(\vec{r}, t ; \vec{r}_2, t_2\right) K\left(\vec{r}_2,
t_2 ; \vec{r}_1, t_1\right) K\left(\vec{r}_1, t_1 ; \vec{r}_0,
t_0\right) \mathrm{d} \vec{r}_1 \mathrm{~d} \vec{r}_2
\end{align}$$</span></p>
<p>注意我们要求上式中恒满足关系 <span
class="math inline"><em>r⃗</em><sub><em>N</em></sub> = <em>r⃗</em>, <em>t</em><sub><em>N</em></sub> = <em>t</em>, <em>a</em> &gt; <em>b</em> ⇔ <em>t</em><sub><em>a</em></sub> &gt; <em>t</em><sub><em>b</em></sub></span>。利用该性质进行无穷分割:</p>
<p><span class="math display">$$\begin{align}
K\left(\vec{r}, t ; \vec{r}_0, t_0\right)&amp;=\int\left[\prod_{n=1}^N
K\left(\vec{r}_n, t_n ; \vec{r}_{n-1}, t_{n-1}\right)\right] \mathrm{d}
\vec{r}_1 \cdots \mathrm{d} \vec{r}_{N-1} \\
\max \left\{t_n-t_{n-1}\right\} &amp;\rightarrow 0
\end{align}$$</span></p>
<p>将积分号里边儿的那个无穷小过程传播子换一个符号, 记为:</p>
<p><span class="math display">$$\begin{align}
K\left(\vec{r}, t ; \vec{r}_0, t_0\right)=\int\left[\prod_{n=1}^N
\mathcal{K}\left(\vec{r}_n, t_n ; \vec{r}_{n-1}, t_{n-1}\right)\right]
\mathrm{d} \vec{r}_1 \cdots \mathrm{d} \vec{r}_{N-1}{ }^{[2]}
\end{align}$$</span></p>
<h2 id="相空间的路径积分">相空间的路径积分</h2>
<p>要求 <span
class="math inline"><em>K</em>(<em>r⃗</em>, <em>t</em>; <em>r⃗</em><sub>0</sub>, <em>t</em><sub>0</sub>)</span>
，需先处理 <span
class="math inline">𝒦(<em>r⃗</em><sub><em>n</em></sub>, <em>t</em><sub><em>n</em></sub>; <em>r⃗</em><sub><em>n</em> − 1</sub>, <em>t</em><sub><em>n</em> − 1</sub>)</span>
。</p>
<p><span class="math display">$$\begin{align}
\mathcal{K}\left(\vec{r}_n, t_n ; \vec{r}_{n-1},
t_{n-1}\right)&amp;=\left\langle\vec{r}_n\left|U\left(t_n,
t_{n-1}\right)\right|
\vec{r}_{n-1}\right\rangle=\left\langle\vec{r}_n\left|e^{-\frac{i
H}{\hbar} \Delta t_n}\right| \vec{r}_{n-1}\right\rangle\\
e^{-\frac{i}{\hbar} H \Delta
t_n}&amp;=e^{-\frac{i}{\hbar}\left[\frac{p^2}{2 m}+V\left(\vec{R},
t_n\right)\right] \Delta t_n \Delta t_n \rightarrow 0}
e^{-\frac{i}{\hbar} V\left(\vec{R}, t_n\right) \Delta t_n}
e^{-\frac{i}{\hbar} \frac{p^2}{2 m} \Delta t_n}
\end{align}$$</span></p>
<p><span
class="math inline">𝒦</span>表示极短时间内从一个状态到另一个状态，在极限情况下式精确的；<span
class="math inline"><em>K</em></span>是一段时间的跳跃。因此有 <span
class="math inline"><em>K</em> ≠ 𝒦</span>，在一些情况下这两个是相等的例如自由粒子。</p>
<p>可以推知：</p>
<p><span class="math display">$$\begin{align}
\mathcal{K}\left(\vec{r}_n, t_n ; \vec{r}_{n-1}, t_{n-1}\right) &amp;
=\left\langle\vec{r}_n\left|e^{-\frac{i}{\hbar} V\left(\vec{R},
t_n\right) \Delta t_n} e^{-\frac{i}{\hbar} \frac{p^2}{2 m} \Delta
t_n}\right| \vec{r}_{n-1}\right\rangle \\
&amp; =e^{-\frac{i}{\hbar} V\left(\vec{r}_n, t_n\right) \Delta
t_n}\left\langle\vec{r}_n\left|e^{-\frac{i}{\hbar} \frac{P^2}{2 m}
\Delta t_n}\right| \vec{r}_{n-1}\right\rangle \\
&amp; =\int e^{-\frac{i}{\hbar} V\left(\vec{r}_n, t_n\right) \Delta
t_n}\left\langle\vec{r}_n \mid
\vec{p}_n\right\rangle\left\langle\vec{p}_n\left|e^{-\frac{i}{\hbar}
\frac{P^2}{2 m} \Delta t_n}\right| \vec{r}_{n-1}\right\rangle \mathrm{d}
\vec{p}_n \\
&amp; =\int e^{-\frac{i}{\hbar} V\left(\vec{r}_n, t_n\right) \Delta
t_n}\left\langle\vec{r}_n \mid \vec{p}_n\right\rangle
e^{-\frac{i}{\hbar} \frac{p_n^2}{2 m} \Delta t_n}\left\langle\vec{p}_n
\mid \vec{r}_{n-1}\right\rangle \mathrm{d} \vec{p}_n \\
&amp; =\frac{1}{(2 \pi \hbar)^3} \int e^{-\frac{i}{\hbar}
V\left(\vec{r}_n, t_n\right) \Delta t_n} e^{i \frac{\vec{p}_n}{\hbar}
\cdot \Delta \vec{r}_n} e^{-\frac{i}{\hbar} \frac{p_n^2}{2 m} \Delta
t_n} \mathrm{~d} \vec{p}_n \\
&amp; =\frac{1}{(2 \pi \hbar)^3} \int e^{\frac{i}{\hbar}\left[\vec{p}_n
\cdot \frac{\Delta \vec{r}_n}{\Delta t_n}-H\left(\vec{r}_n, \vec{p}_n,
t_n\right)\right] \Delta t_n} \mathrm{~d} \vec{p}_n \\
&amp; =\frac{1}{(2 \pi \hbar)^3} \int e^{\frac{i}{\hbar}\left[\vec{p}_n
\cdot \vec{r}_n-H\left(\vec{r}_n, \vec{p}_n, t_n\right)\right] \Delta
t_n} \mathrm{~d} \vec{p}_n \\
&amp; =\frac{1}{(2 \pi \hbar)^3} \int e^{i \frac{L\left(\vec{r}_n,
\vec{p}_n, t_n\right) \Delta t_n}{\hbar}} \mathrm{d} \vec{p}_n
\end{align}$$</span></p>
<p>将上式代回传播子的表达式就得到相空间的路径积分:</p>
<p><span class="math display">$$\begin{align}
K\left(\vec{r}, t ; \vec{r}_0, t_0\right) &amp; =\int\left[\prod_{n=1}^N
\mathcal{K}\left(\vec{r}_n, t_n ; \vec{r}_{n-1}, t_{n-1}\right)\right]
\mathrm{d} \vec{r}_1 \cdots \mathrm{d} \vec{r}_{N-1} \\
&amp; =\int \prod_{n=1}^N\left[\frac{1}{(2 \pi \hbar)^3} \int
e^{\frac{i}{\hbar} L\left(\vec{r}_n, \vec{p}_n, t_n\right) \Delta t_n}
\mathrm{~d} \vec{p}_n\right] \mathrm{d} \vec{r}_1 \cdots \mathrm{d}
\vec{r}_{N-1} \\
&amp; =\frac{1}{(2 \pi \hbar)^{3 N}} \int\left[\int e^{\frac{i}{\hbar}
\sum_{n=1}^N L\left(\vec{r}_n, \vec{p}_n, t_n\right) \Delta t_n}
\mathrm{~d} \vec{p}_1 \cdots \mathrm{d} \vec{p}_N\right] \mathrm{d}
\vec{r}_1 \cdots \mathrm{d} \vec{r}_{N-1} \\
&amp; \stackrel{\text { limit }}{=} \frac{1}{(2 \pi \hbar)^{3 N}} \int
e^{\frac{i}{\hbar} \int L(\vec{r}, \vec{p}, \tau) \mathrm{d} \tau}
\mathrm{d} \vec{r}_1 \cdots \mathrm{d} \vec{r}_{N-1} \mathrm{~d}
\vec{p}_1 \cdots \mathrm{d} \vec{p}_N \\
&amp; =\frac{1}{(2 \pi \hbar)^{3 N}} \int \mathcal{D} \vec{r}
\mathcal{D} \vec{p} e^{i \frac{S(\vec{r}, \vec{p}, t)}{\hbar}}
\end{align}$$</span></p>
<p>其中的 <span class="math inline">𝒟</span>
表征对各种怪异路径进行积分。注意位置积分比动量积分少一重，因为端点是固定的。</p>
<h2 id="位形空间的路径积分">位形空间的路径积分</h2>
<p>位形空间的路径积分，就是把动量先积掉：</p>
<p><span class="math display">$$\begin{align}
\mathcal{K}\left(\vec{r}_n, t_n ; \vec{r}_{n-1}, t_{n-1}\right) &amp;
=\frac{1}{(2 \pi \hbar)^3} \int e^{-\frac{i}{\hbar} V\left(\vec{r}_n,
t_n\right) \Delta t_n} e^{i \frac{\vec{p}_n}{\hbar} \cdot \Delta
\vec{r}_n} e^{-\frac{i}{\hbar} \frac{p_n^2}{2 m} \Delta t_n} \mathrm{~d}
\vec{p}_n \\
&amp; =\frac{1}{(2 \pi \hbar)^3} e^{-\frac{i}{\hbar} V\left(\vec{r}_n,
t_n\right) \Delta t_n} \int e^{i \frac{\vec{p}_n}{\hbar} \cdot \Delta
\vec{r}_n} e^{-\frac{i}{\hbar} \frac{p_n^2}{2 m} \Delta t_n} \mathrm{~d}
\vec{p}_n
\end{align}$$</span></p>
<p>显然后面的积分在自由粒子的格林函数中求解过, 这里直接而代入结论:</p>
<p><span class="math display">$$
\frac{1}{(2 \pi \hbar)^3} \int e^{i \frac{\vec{p}_n}{\hbar} \cdot \Delta
\vec{r}_n} e^{-\frac{i}{\hbar} \frac{p_n^2}{2 m} \Delta t_n} \mathrm{~d}
\vec{p}_n=\left(\frac{m}{2 \pi i \hbar \Delta t_n}\right)^{\frac{3}{2}}
\exp \left[i \frac{m\left(\Delta \vec{\Delta}_n\right)^2}{2 \hbar \Delta
t_n}\right]
$$</span></p>
<p>将上式代回无穷小间隔传播子表达式得:</p>
<p><span class="math display">$$\begin{align}
\mathcal{K}\left(\vec{r}_n, t_n ; \vec{r}_{n-1}, t_{n-1}\right) &amp;
=\frac{1}{(2 \pi \hbar)^3} \int e^{-\frac{i}{\hbar} V\left(\vec{r}_n,
t_n\right) \Delta t_n} e^{i \frac{\vec{p}_n}{\hbar} \cdot \Delta
\vec{r}_n} e^{-\frac{i}{\hbar} \frac{p_n^2}{2 m} \Delta t_n} \mathrm{~d}
\vec{p}_n \\
&amp; =\left(\frac{m}{2 \pi i \hbar \Delta t_n}\right)^{\frac{3}{2}}
e^{-\frac{i}{\hbar} V\left(\vec{r}_n, t_n\right) \Delta t_n} e^{i
\frac{m\left(\Delta \vec{F}_n\right)^2}{2 \hbar \Delta t_n}} \\
&amp; =\left(\frac{m}{2 \pi i \hbar \Delta t_n}\right)^{\frac{3}{2}}
e^{\frac{i}{\hbar}\left[\frac{1}{2} m \frac{\left(\Delta
\vec{r}_n\right)^2}{\left(\Delta t_n\right)^2}-V\left(\vec{r}_n,
t_n\right)\right] \Delta t_n} \\
&amp; =\left(\frac{m}{2 \pi i \hbar \Delta t_n}\right)^{\frac{3}{2}}
e^{\frac{i}{\hbar}\left[\frac{1}{2} m
\dot{\vec{r}}_n^2-V\left(\vec{r}_n, t_n\right)\right] \Delta t_n}
\end{align}$$</span></p>
<p>将上述结果代回传播子的表达式得到位形空间的路径积分:</p>
<p><span class="math display">$$\begin{align}
K\left(\vec{r}, t ; \vec{r}_0, t_0\right) &amp; =\int\left[\prod_{n=1}^N
\mathcal{K}\left(\vec{r}_n, t_n ; \vec{r}_{n-1}, t_{n-1}\right)\right]
\mathrm{d} \vec{r}_1 \cdots \mathrm{d} \vec{r}_{N-1} \\
&amp; =\int\left[\prod_{n=1}^N\left(\frac{m}{2 \pi i \hbar \Delta
t_n}\right)^{\frac{3}{2}} e^{\frac{i}{\hbar}\left[\frac{1}{2} m
\dot{\vec{r}}_n^2-V\left(\vec{r}_n, t_n\right)\right] \Delta t_n}\right]
\mathrm{d} \vec{r}_1 \cdots \mathrm{d} \vec{r}_{N-1} \\
&amp; =\prod_{n=1}^N\left(\frac{m}{2 \pi i \hbar \Delta
t_n}\right)^{\frac{3}{2}} \int e^{\frac{i}{\hbar}
\sum_{n=1}^N\left[\frac{1}{2} m \dot{\vec{r}}_n^2-V\left(\vec{r}_n,
t_n\right)\right] \Delta t_n} \mathrm{~d} \vec{r}_1 \cdots \mathrm{d}
\vec{r}_{N-1} \\
&amp; \stackrel{\text { limit }}{=} \prod_{n=1}^N\left(\frac{m}{2 \pi i
\hbar \Delta t_n}\right)^{\frac{3}{2}} \int e^{\frac{i}{\hbar}
\int_{t_0}^t \frac{1}{2} m \dot{\vec{r}}_n^2-V(\vec{r}, \tau) \mathrm{d}
\tau} \mathrm{d} \vec{r}_1 \cdots \mathrm{d} \vec{r}_{N-1} \\
&amp; =\prod_{n=1}^N\left(\frac{m}{2 \pi i \hbar \Delta
t_n}\right)^{\frac{3}{2}} \int \mathcal{D} \vec{r} e^{i
\frac{S\left(\vec{r}, \vec{r}_t, t\right)}{\hbar}}
\end{align}$$</span></p>
<h2 id="传播子的两类表达式">传播子的两类表达式</h2>
<p>综上所述, 路径积分中的传播子就表达为: <span
class="math display">$$\begin{align}
K\left(\vec{r}, t ; \vec{r}_0, t_0\right)= &amp; \frac{1}{(2 \pi
\hbar)^{3 N}} \int \mathcal{D} \vec{r} \mathcal{D} \vec{p} e^{i
\frac{S(\vec{r}, \vec{p}, t)}{\hbar}} \\
K\left(\vec{r}, t ; \vec{r}_0, t_0\right)= &amp;
\prod_{n=1}^N\left(\frac{m}{2 \pi i \hbar \Delta
t_n}\right)^{\frac{3}{2}} \int \mathcal{D} \vec{r} e^{i \frac{S(\vec{r},
\vec{r}, t)}{\hbar}} \label{1_weixin}
\end{align}$$</span></p>
<p>实际运算中需要用到:</p>
<p><span class="math display">$$
\left\{\begin{array}{l}
K\left(\vec{r}, t ; \vec{r}_0, t_0\right)=\frac{1}{(2 \pi \hbar)^{3 N}}
\int e^{\frac{i}{\hbar} \sum_{n=1}^N\left[\vec{p}_n \cdot
\dot{\vec{r}}_n-H\left(\vec{r}_n, \vec{p}_n, t_n\right)\right] \Delta
t_n} \mathrm{~d} \vec{r}_1 \cdots \mathrm{d} \vec{r}_{N-1} \mathrm{~d}
\vec{p}_1 \cdots \mathrm{d} \vec{p}_N \\
K\left(\vec{r}, t ; \vec{r}_0, t_0\right)=\prod_{n=1}^N\left(\frac{m}{2
\pi i \hbar \Delta t_n}\right)^{\frac{3}{2}} \int e^{\frac{i}{\hbar}
\sum_{n=1}^N\left[\frac{1}{2} m \dot{\vec{r}}_n^2-V\left(\vec{r}_n,
t_n\right)\right] \Delta t_n} \mathrm{~d} \vec{r}_1 \cdots \mathrm{d}
\vec{r}_{N-1}
\end{array}\right.
$$</span></p>
<p>其中 <span class="math inline">$\left\{\begin{array}{l}\Delta
t_n=t_n-t_{n-1} \\ \Delta \vec{r}_n=\vec{r}_n-\vec{r}_{n-1} \\
\dot{\vec{r}}_n=\Delta \vec{r}_n / \Delta t_n \\ n \in\{1,2,3, \cdots,
N\}\end{array} \quad\right.$</span> 且规定 <span
class="math inline">$\left\{\begin{array}{l}\vec{r}_N=\vec{r} \\ t_N=t
\\ t_0&lt;t_n \leq t \\ a&gt;b \Leftrightarrow t_a&gt;t_b \\ \min
\left\{t_n-t_{n-1}\right\} \rightarrow 0\end{array}\right.$</span>
根号下的虚数单位 <span class="math inline"><em>i</em></span> 定义为
<span class="math inline">exp [<em>i</em><em>π</em>/2]</span>。</p>
<h1 id="自由粒子---利用对角化求解路径积分">自由粒子 -
利用对角化求解路径积分</h1>
<p><font color='red'></p>
<p>已完成内容： 1. 路径积分的两种形式 2. 自由粒子的路径积分</p>
<p>本部分的目标： 1.
讨论自由粒子在位形空间的路径积分，得到传播子的具体形式 2.
讨论自由粒子在相空间的路径积分，得到传播子的具体形式所属</p>
<p></font></p>
<p>对时间采用平均分割的方案<span class="math inline">$\Delta
t_n=\frac{t-t_0}{N}=\varepsilon$</span>，在极限情况下讨论<span
class="math inline"><em>N</em> → ∞  or  <em>ε</em> → ∞</span>。由<span
class="math inline">$\eqref{1_weixin}$</span>可知自由粒子位形空间传播子为：</p>
<p><span class="math display">$$\begin{align}
K\left(x, t ; x_0, t_0\right)&amp;=\left(\frac{m}{2 \pi i \hbar
\varepsilon}\right)^{\frac{N}{2}} \int \mathrm{D} x e^{i
\frac{S}{\hbar}} \\
S&amp;=\int \frac{1}{2} m v^2 dt\\
&amp;=\sum_{n=1}^N \frac{1}{2} m \frac{\left(\Delta
x_n\right)^2}{\varepsilon^2} \varepsilon \\
&amp;=\frac{m}{2 \varepsilon} \sum_{n=1}^N\left(x_n-x_{n-1}\right)^2
\label{free_partial_S}
\end{align}$$</span></p>
<p>将位置<span
class="math inline"><em>x</em><sub><em>n</em></sub></span>表示成经典路径<span
class="math inline"><em>x</em><sub><em>n</em></sub><sup><em>c</em></sup></span>与路径扰动<span
class="math inline"><em>y</em><sub><em>n</em></sub></span>两个部分，其中下标表示时刻<span
class="math inline"><em>t</em><sub><em>n</em></sub></span>：</p>
<p><span class="math display">$$\begin{align}
x_n&amp;=x_n^c+y_n \\
x_n^{\mathrm{c}}&amp;=x^{\mathrm{c}}\left(t_n\right)=x^{\mathrm{c}}\left(t_0+n
\varepsilon\right) \\
\end{align}$$</span></p>
<p>并且有边界条件，在端点是固定不动的：</p>
<p><span class="math display">$$\begin{align}
\left\{
    \begin{array}
    {l}x_0^{\mathrm{c}}=x^{\mathrm{c}}\left(t_0\right)=x_0 \\
    x_N^{\mathrm{c}}=x^{\mathrm{c}}\left(t_N\right)=x_N=x
    \end{array}
\right.
\end{align}$$</span></p>
<p>以及关系：</p>
<p><span class="math display">$$\begin{aligned}
t_N&amp;=t_0+N \varepsilon \\
\Delta t_2+\Delta t_1&amp;=t_2-t_0 \Rightarrow \sum_{n=1}^N \Delta t_n=N
\varepsilon=t_N-t_0
\end{aligned}$$</span></p>
<p>代入<span class="math inline">$\eqref{free_partial_S}$</span>后：</p>
<p><span class="math display">$$\begin{align}
S&amp;=\frac{m}{2 \varepsilon} \sum_{n=1}^N\left(\Delta x_n\right)^2 \\
&amp;=\frac{m}{2 \varepsilon} \sum_{n=1}^N\left(\Delta
x_n^{\mathrm{c}}+\Delta y_n\right)^2\\
&amp;=\frac{m}{2 \varepsilon} \sum_{n=1}^N\left(\Delta
x_n^{\mathrm{c}}\right)^2+\frac{m}{2 \varepsilon}
\sum_{n=1}^N\left(\Delta y_n\right)^2+\frac{m}{\varepsilon} \sum_{n=1}^N
\Delta x_n^{\mathrm{c}} \Delta y_n
\end{align}$$</span></p>
<p>分为三个部分：</p>
<p><span class="math display">$$\begin{align}
S^{\mathrm{c}}&amp;= \frac{m}{2 \varepsilon} \sum_{n=1}^N\left(\Delta
x_n^{\mathrm{c}}\right)^2 \\
S^{\prime} &amp;= \frac{m}{2 \varepsilon} \sum_{n=1}^N\left(\Delta
y_n\right)^2 \\
S^{\times} &amp;= \frac{m}{\varepsilon} \sum_{n=1}^N \Delta
x_n^{\mathrm{c}} \Delta y_n
\end{align}$$</span></p>
<figure>
<img src="./path_integral.png" alt="Path Integral" />
<figcaption aria-hidden="true">Path Integral</figcaption>
</figure>
<p>如上图所示，表示在固定初始与终点的路径积分，每条线表示可能的路径，其中红线为经典路径。</p>
<p>针对交叉项：</p>
<p><span class="math display">$$\begin{align}
S^{\times} &amp;= \frac{m}{\varepsilon} \sum_{n=1}^N \Delta
x_n^{\mathrm{c}} \Delta y_n \\
&amp;=m \sum_{n=1}^N \dot{x}_n^{\mathrm{c}}\left(y_n-y_{n-1}\right) \\
&amp; =m\left(\sum_{n=1}^N \dot{x}_n^{\mathrm{c}} y_n-\sum_{n=1}^N
\dot{x}_n^{\mathrm{c}} y_{n-1}\right) \\
&amp; \stackrel{y_{N=0}}{=} m\left(\sum_{n=1}^{N-1}
\dot{x}_n^{\mathrm{c}} y_n-\sum_{n=0}^{N-1} \dot{x}_{n+1}^{\mathrm{c}}
y_n\right) \\
&amp; \stackrel{y_{0=0}}{=} m\left(\sum_{n=1}^{N-1}
\dot{x}_n^{\mathrm{c}} y_n-\sum_{n=1}^{N-1} \dot{x}_{n+1}^{\mathrm{c}}
y_n\right) \\
&amp;=-m \varepsilon \sum_{n=1}^{N-1} \frac{\Delta
\dot{x}_{n+1}^{\mathrm{c}}}{\varepsilon} y_n \\
&amp;=-m \varepsilon \sum_{n=1}^{N-1} \ddot{x}_{n+1}^{\mathrm{c}} y_n
\end{align}$$</span></p>
<p>考虑到自由粒子<span
class="math inline">$F=ma=m\ddot{x}^{\mathrm{c}}=0$</span>，因此交叉项<span
class="math inline"><em>S</em><sup>×</sup> = 0</span>。</p>
<p>对于经典路径项<span
class="math inline"><em>S</em><sup>c</sup></span>：</p>
<p><span class="math display">$$\begin{align}
S^{\mathrm{c}}&amp;=\frac{m}{2 \varepsilon} \sum_{n=1}^N\left(\Delta
x_n^{\mathrm{c}}\right)^2 \\
&amp;=\frac{m \varepsilon}{2} \sum_{n=1}^N \frac{\left(\Delta
x_n^{\mathrm{c}}\right)^2}{\varepsilon^2} \\
&amp;=\frac{m \varepsilon}{2}
\sum_{n=1}^N\left(\dot{x}_n^{\mathrm{c}}\right)^2 \\
&amp;\stackrel{\varepsilon \rightarrow 0}{=} \frac{m}{2}
\int_{t_0}^t\left(\dot{x}^{\mathrm{c}}\right)^2 \mathrm{~d} \tau
\end{align}$$</span></p>
<p>经典自由粒子解的 <span
class="math inline"><em>ẋ</em><sup><em>c</em></sup></span> 是一个常数,
且知道 <span
class="math inline">$\dot{x}^{\mathrm{c}}=\frac{x-x_0}{t-t_0}$</span>。所以：</p>
<p><span class="math display">$$\begin{align}
S^{\mathrm{c}}&amp;=\frac{m}{2}
\int_{t_0}^t\left(\dot{x}^{\mathrm{c}}\right)^2 \mathrm{~d} \tau \\
&amp;=\frac{m}{2}\left(\frac{x-x_0}{t-t_0}\right)^2 \int_{t_0}^t
\mathrm{~d} \tau\\
&amp;=\frac{m\left(x-x_0\right)^2}{2\left(t-t_0\right)}
\end{align}$$</span></p>
<p>端点给定，并且把经典路径分离出来，因此可以将传播子写为：</p>
<p><span class="math display">$$\begin{align}
K\left(x, t ; x_0, t_0\right)=e^{i
\frac{S^{\mathrm{c}}}{\hbar}}\left(\frac{m}{2 \pi i \hbar
\varepsilon}\right)^{\frac{N}{2}} \int e^{\frac{i}{\hbar} S^{\prime}}
\mathrm{d} y_1 \cdots \mathrm{d} y_{N-1}
\end{align}$$</span></p>
<p>接下来要化简的是<span
class="math inline"><em>S</em><sup>′</sup></span>，考虑到在两端点附近<span
class="math inline"><em>y</em><sub>0</sub> = <em>y</em><sub><em>N</em></sub> = 0</span>：</p>
<p><span class="math display">$$\begin{align}
S^{\prime} &amp; =\frac{m}{2 \varepsilon}
\sum_{n=1}^N\left(y_n-y_{n-1}\right)^2 \\
&amp; =\frac{m}{2
\varepsilon}\left[y_1^2+\left(y_2-y_1\right)^2+\cdots+\left(y_{N-1}-y_{N-2}\right)^2+y_{N-1}^2\right]
\\
&amp; =\frac{m}{2 \varepsilon}\left(2 \sum_{n=1}^{N-1} y_n^2-2
\sum_{n=1}^{N-1} y_n y_{n-1}\right) \\
&amp; =\frac{m}{2 \varepsilon} \sum_{n=1}^{N-1}\left(2 y_n^2-2 y_n
y_{n-1}\right) \\
&amp;=\frac{m}{2 \varepsilon} \sum_{n, m=1}^{N-1} y_m A_{m n}
y_n=\frac{m}{2 \varepsilon} y^{\mathrm{T}} A y
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>A</em></span>是一个实对称矩阵，<span
class="math inline"><em>A</em><sub><em>m</em><em>n</em></sub> = 2<em>δ</em><sub><em>m</em>, <em>n</em></sub> − <em>δ</em><sub><em>m</em>, <em>n</em> − 1</sub> − <em>δ</em><sub><em>m</em> − 1, <em>n</em></sub></span>，如果不去掉端点的话，就会造成不是一个实对称矩阵。既然已经写成对称矩阵的形式了，那接下来就是精确对角化，将其耦合项解耦，从而方便积分。</p>
<p><span class="math display">$$\begin{align}
R^{\mathrm{T}} A R=\Lambda=\operatorname{diag}\left(\lambda_1,
\lambda_2, \cdots, \lambda_{N-1}\right),
\end{align}$$</span></p>
<p>其中 <span class="math inline"><em>R</em></span> 是正交矩阵，满足
<span
class="math inline"><em>R</em><sup>T</sup><em>R</em> = |<em>R</em>| = 1</span>
同时注意么正变换不改变本征值:</p>
<p><span class="math display">$$\begin{align}
&amp;A \varphi=\lambda \varphi \\
&amp;\Rightarrow R^{\mathrm{T}} A R R^{\mathrm{T}} \varphi=\lambda
R^{\mathrm{T}} \varphi \\
&amp;\Rightarrow \Lambda \varphi^{\prime}=\lambda \varphi^{\prime}
\end{align}$$</span></p>
<p>令:</p>
<p><span class="math display">$$\begin{align}
y=R u \Rightarrow y^{\mathrm{T}} A y=u^{\mathrm{T}} R^{\mathrm{T}} A R
u=u^{\mathrm{T}} \Lambda u=\sum_{n=1}^{N-1} \lambda_n u_n^2
\end{align}$$</span></p>
<p>再进行变量代换:</p>
<p><span class="math display">$$\begin{align}
\mathrm{d} y_1 \cdots \mathrm{d} y_{N-1}&amp;=|\operatorname{det} R|
\mathrm{d} u_1 \cdots \mathrm{d} u_{N-1} \\
&amp;=\mathrm{d} u_1 \cdots \mathrm{d} u_{N-1}
\end{align}$$</span></p>
<p>因此传播子可以写为：</p>
<p><span class="math display">$$\begin{align}
K\left(x, t ; x_0, t_0\right) &amp; =e^{i
\frac{S^c}{\hbar}}\left(\frac{m}{2 \pi i \hbar
\varepsilon}\right)^{\frac{N}{2}} \int e^{i \frac{m}{2 \hbar
\varepsilon} \sum_{n=1}^{N-1} \lambda_n u_n^2} \mathrm{~d} u_1 \cdots
\mathrm{d} u_{N-1} \\
&amp; =e^{i \frac{S^c}{\hbar}}\left(\frac{m}{2 \pi i \hbar
\varepsilon}\right)^{\frac{N}{2}} \prod_{n=1}^{N-1} \int e^{-\frac{m
\lambda_n}{2 i \hbar \varepsilon} u_n^2} \mathrm{~d} u_n \\
&amp; =e^{i \frac{S^c}{\hbar}}\left(\frac{m}{2 \pi i \hbar
\varepsilon}\right)^{\frac{N}{2}} \prod_{n=1}^{N-1} \sqrt{\frac{2 \pi i
\hbar \varepsilon}{m \lambda_n}}\\
&amp;=e^{i \frac{s^c}{\hbar}} \sqrt{\frac{m}{2 \pi i \hbar
\varepsilon}}\left(\prod_{n=1}^{N-1} \lambda_n\right)^{-\frac{1}{2}}
\label{3_propagator}
\end{align}$$</span></p>
<p>从<span
class="math inline">$\eqref{3_propagator}$</span>可以看出，传播子最后只与特征值的连乘积有关，下面就是求解<span
class="math inline">$\prod_{n=1}^{N-1}
\lambda_n$</span>的值。求解本征的可通过本征多项式直接得到。根据<span
class="math inline"><em>A</em></span> 的本征多项式 <span
class="math inline">|<em>A</em> − <em>λ</em><em>I</em>| = <em>P</em><sub><em>N</em> − 1</sub>(<em>λ</em>)</span>，构造多项式：</p>
<p><span class="math display">$$\begin{align}
P_n(\lambda)=\left|\begin{array}{cccccc}
2-\lambda &amp; -1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
-1 &amp; 2-\lambda &amp; -1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; -1 &amp; 2-\lambda &amp; -1 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; -1 &amp; 2-\lambda &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots
\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 2-\lambda
\end{array}\right|
\end{align}$$</span></p>
<p>将<span
class="math inline"><em>P</em><sub><em>n</em></sub>(<em>λ</em>)</span>从第一行展开得到：</p>
<p><span class="math display">$$\begin{align}
&amp;P_n = (2-\lambda)P_{n-1}-P_{n-2} \\
&amp;P_n + (\lambda-2)P_{n+1}+P_{n+2} = 0 \label{2_P}
\end{align}$$</span></p>
<p>定义一个能提升多项式阶数的线性算子<span
class="math inline"><em>L</em> : <em>L</em><em>P</em><sub><em>n</em> − 1</sub> = <em>P</em><sub><em>n</em></sub></span>，然后将其代入递推关系<span
class="math inline">$\eqref{2_P}$</span>：</p>
<p><span class="math display">$$\begin{align}
0&amp;=[L^2+(\lambda-2)L+1]P_n \\
&amp;= (L-\alpha)(L-\beta)P_n \\
\alpha &amp;= \frac{2-\lambda+\sqrt{\lambda^2-4\lambda}}{2} \\
\beta &amp;= \frac{2-\lambda-\sqrt{\lambda^2-4\lambda}}{2} \\
\end{align}$$</span></p>
<p>接下来分别求解<span
class="math inline">(<em>L</em> − <em>α</em>)<em>P</em><sub><em>n</em></sub> = 0</span>与<span
class="math inline">(<em>L</em> − <em>β</em>)<em>P</em><sub><em>n</em></sub> = 0</span>：</p>
<p><span class="math display">$$\begin{align}
(L-\alpha)P_n&amp;=0 \\
LP_n&amp;=\alpha P_n \\
P_n&amp;=\alpha P_{n-1}=\alpha^2 P_{n-2} \\
&amp;=\alpha^n P_0^n = c_1\alpha^n
\end{align}$$</span></p>
<p>同理可得：</p>
<p><span class="math display">$$\begin{align}
P_n&amp;=c_2\beta^n
\end{align}$$</span></p>
<p>因此通解为：</p>
<p><span class="math display">$$\begin{align}
P_n&amp;=c_1\alpha^n + c_2\beta^n
\end{align}$$</span></p>
<p>根据多项式<span
class="math inline"><em>P</em><sub>1</sub> = 2 − <em>λ</em></span>与<span
class="math inline"><em>P</em><sub>2</sub> = (2 − <em>λ</em>)<sup>2</sup> − 1</span>可知<span
class="math inline"><em>P</em><sub>1</sub> = 1</span>，由此解得：</p>
<p><span class="math display">$$\begin{align}
c_1 &amp;= \frac{\alpha}{\alpha-\beta} \\
c_2 &amp;= \frac{-\beta}{\alpha-\beta} \\
P_n&amp;= \frac{\alpha^{n+1}-\beta^{n+1}}{\alpha-\beta} \\
&amp;=\alpha^n+\alpha^{n-1}\beta+\alpha^{n-2}\beta^2+\cdots+\alpha\beta^{n-1}+\beta^n
\end{align}$$</span></p>
<p>对于连乘积有<span class="math inline">$\prod_{n=1}^{N-1}
\lambda_n=|A|$</span>，根据<span
class="math inline">|<em>A</em> − <em>λ</em><em>I</em>| = <em>P</em><sub><em>N</em> − 1</sub>(<em>λ</em>)</span>可得到<span
class="math inline">|<em>A</em>| = <em>P</em><sub><em>N</em> − 1</sub>(0) = <em>N</em></span>，并将结论代入<span
class="math inline">$\eqref{3_propagator}$</span>：</p>
<p><span class="math display">$$\begin{align}
K\left(x, t ; x_0, t_0\right) &amp;=e^{i \frac{s^c}{\hbar}}
\sqrt{\frac{m}{2 \pi i \hbar \varepsilon}}\frac{1}{\sqrt{N}} \\
&amp;=e^{i \frac{s^c}{\hbar}} \sqrt{\frac{m}{2 \pi i \hbar (t-t_0)}} \\
&amp;=\sqrt{\frac{m}{2 \pi i \hbar (t-t_0)}}\exp{\left[i
\frac{m(x-x_0)^2}{2\hbar(t-t_0)}\right]}
\end{align}$$</span></p>
<h1 id="谐振子---利用对角化求解路径积分">谐振子 -
利用对角化求解路径积分</h1>
<p><font color='red'></p>
<p>已完成内容：</p>
<ol type="1">
<li>自由粒子的路径积分</li>
</ol>
<p>本部分的目标：</p>
<ol type="1">
<li>讨论谐振子的路径积分</li>
</ol>
<p></font></p>
<p>对于一维谐振子势<span
class="math inline">$V=\frac{1}{2}m\omega^2X^2$</span>，研究从<span
class="math inline"><em>t</em><sub>0</sub> → <em>t</em></span>的演化结果。位形空间传播子：</p>
<p><span class="math display">$$\begin{align}
K\left(x, t ; x_0, t_0\right)&amp;=\left(\frac{m}{2 \pi i \hbar
\varepsilon}\right)^{\frac{N}{2}} \int \mathrm{D} x e^{i
\frac{S}{\hbar}} \\
S&amp;=\int \frac{1}{2} m v^2 - \frac{1}{2}m\omega^2X^2 dt\\
&amp;=\sum_{n=1}^N \left[\frac{1}{2} m \frac{\left(\Delta
x_n\right)^2}{\varepsilon^2} - \frac{1}{2}m\omega^2x_n^2
\right]\varepsilon
\end{align}$$</span></p>
<p>同样进行代换，分离为经典路径和扰动项<span
class="math inline"><em>x</em><sub><em>n</em></sub> = <em>x</em><sub><em>n</em></sub><sup>c</sup> + <em>y</em><sub><em>n</em></sub></span>：</p>
<p><span class="math display">$$\begin{align}
S&amp;=\sum_{n=1}^N \left[\frac{1}{2} m \frac{\left(\Delta
x_n\right)^2}{\varepsilon^2} - \frac{1}{2}m\omega^2x_n^2
\right]\varepsilon \\
&amp;= \sum_{n=1}^N \left[\left(\Delta x_n\right)^2 - \omega^2x_n^2
\varepsilon^2\right]\frac{m}{2\varepsilon} \\
&amp;= \sum_{n=1}^N \left[\left(\Delta x_n^{\mathrm{c}}+\Delta
y_n\right)^2 - \omega^2(x_n^{\mathrm{c}}+y_n)^2
\varepsilon^2\right]\frac{m}{2\varepsilon} \\
&amp; \begin{aligned}
=&amp;\sum_{n=1}^N \left[\left(\Delta x_n^{\mathrm{c}}\right)^2 -
\omega^2(x_n^{\mathrm{c}})^2 \varepsilon^2\right]\frac{m}{2\varepsilon}
\\
&amp;+ \sum_{n=1}^N \left[\left(\Delta y_n\right)^2 - \omega^2(y_n)^2
\varepsilon^2\right]\frac{m} {2\varepsilon} \\
&amp;+ \sum_{n=1}^N \left[2\Delta x_n^{\mathrm{c}}\Delta y_n - 2\omega^2
x_n^{\mathrm{c}}y_n \varepsilon^2\right]\frac{m}{2\varepsilon}\\
\end{aligned}
\end{align}$$</span></p>
<p>同样分为三部分：</p>
<p><span class="math display">$$\begin{align}
&amp; S^{\mathrm{c}}=\frac{m}{2 \varepsilon}
\sum_{n=1}^N\left[\left(\Delta x_n^{\mathrm{c}}\right)^2-\varepsilon^2
\omega^2\left(x_n^{\mathrm{c}}\right)^2\right] \\
&amp; S^{\prime}=\frac{m}{2 \varepsilon} \sum_{n=1}^N\left[\left(\Delta
y_n\right)^2-\varepsilon^2 \omega^2 y_n^2\right] \\
&amp; S^{\times}=\frac{m}{2 \varepsilon} \sum_{n=1}^N\left(2 \Delta
x_n^{\mathrm{c}} \Delta y_n-\varepsilon^2 \omega^2 2 x_n^{\mathrm{c}}
y_n\right)
\end{align}$$</span></p>
<p>先利用<span
class="math inline"><em>y</em><sub><em>n</em></sub></span>的边界条件计算交叉项<span
class="math inline"><em>S</em><sup>×</sup></span>：</p>
<p><span class="math display">$$\begin{align}
S^{\times} &amp; =\frac{m}{2 \varepsilon} \sum_{n=1}^N\left(2 \Delta
x_n^{\mathrm{c}} \Delta y_n-\varepsilon^2 \omega^2 2 x_n^{\mathrm{c}}
y_n\right) \\
&amp; =m
\sum_{n=1}^N\left[\dot{x}_n^{\mathrm{c}}\left(y_n-y_{n-1}\right)-\varepsilon
\omega^2 x_n^{\mathrm{c}} y_n\right] \\
&amp; =m \sum_{n=1}^N \dot{x}_n^{\mathrm{c}} y_n-m \sum_{n=1}^N
\dot{x}_n^{\mathrm{c}} y_{n-1}-m \sum_{n=1}^N \varepsilon \omega^2
x_n^{\mathrm{c}} y_n \\
&amp; \stackrel{y_{N=0}}{=} m \sum_{n=1}^{N-1} \dot{x}_n^{\mathrm{c}}
y_n-m \sum_{n=0}^{N-1} \dot{x}_{n+1}^{\mathrm{c}} y_n-m \sum_{n=1}^{N-1}
\varepsilon \omega^2 x_n^{\mathrm{c}} y_n \\
&amp; \stackrel{y_{0=0}}{=} m \sum_{n=1}^{N-1} \dot{x}_n^{\mathrm{c}}
y_n-m \sum_{n=1}^{N-1} \dot{x}_{n+1}^{\mathrm{c}} y_n-m \sum_{n=1}^{N-1}
\varepsilon \omega^2 x_n^{\mathrm{c}} y_n \\
&amp; =-\left(m \varepsilon \sum_{n=1}^{N-1}
\frac{\dot{x}_{n+1}^{\mathrm{c}}-\dot{x}_n^{\mathrm{c}}}{\varepsilon}
y_n+\sum_{n=1}^{N-1} m \varepsilon \omega^2 x_n^{\mathrm{c}} y_n\right)
\\
&amp; =-\varepsilon \sum_{n=1}^{N-1}\left(m
\ddot{x}_{n+1}^{\mathrm{c}}+m \omega^2 x_n^{\mathrm{c}}\right) y_n
\end{align}$$</span></p>
<p>根据经典方程<span
class="math inline">$m\ddot{x}^{\mathrm{c}}=-\frac{\partial V}{\partial
x^{\mathrm{c}}}=-m\omega^2x^\mathrm{c}$</span>，则有<span
class="math inline"><em>S</em><sup>×</sup> = 0</span>。</p>
<p>再计算经典路径<span
class="math inline"><em>S</em><sup>c</sup></span>：</p>
<p><span class="math display">$$\begin{align}
S^{\mathrm{c}} &amp; =\frac{m}{2 \varepsilon}
\sum_{n=1}^N\left[\left(\Delta x_n^{\mathrm{c}}\right)^2-\varepsilon^2
\omega^2\left(x_n^{\mathrm{c}}\right)^2\right] \\
&amp; =\frac{m \varepsilon}{2}
\sum_{n=1}^N\left[\left(\dot{x}_n^{\mathrm{c}}\right)^2-\omega^2\left(x_n^{\mathrm{c}}\right)^2\right]
\\
&amp;=\frac{m}{2}
\int_{t_0}^t\left[\left(\dot{x}^{\mathrm{c}}\right)^2-\omega^2\left(x^{\mathrm{c}}\right)^2\right]
\mathrm{d} \tau \label{3_classical}
\end{align}$$</span></p>
<p>接下来计算经典路径的形式：</p>
<p><span class="math display">$$\begin{align}
m\ddot{x}^{\mathrm{c}}&amp;=-m\omega^2 x^{\mathrm{c}} \\
x^{\mathrm{c}}(t_0) &amp;= x_0 \\
x^{\mathrm{c}}(t) &amp;= x
\end{align}$$</span></p>
<figure class="highlight mathematica"><table><tr><td class="code"><pre><span class="line"><span class="built_in">DSolve</span><span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="variable">m</span> <span class="built_in">D</span><span class="punctuation">[</span><span class="variable">x</span><span class="punctuation">[</span><span class="variable">t</span><span class="punctuation">]</span><span class="operator">,</span> <span class="punctuation">&#123;</span><span class="variable">t</span><span class="operator">,</span> <span class="number">2</span><span class="punctuation">&#125;</span><span class="punctuation">]</span> <span class="operator">==</span> <span class="operator">-</span><span class="variable">m</span> <span class="variable">\[Omega]</span><span class="operator">^</span><span class="number">2</span> <span class="variable">x</span><span class="punctuation">[</span><span class="variable">t</span><span class="punctuation">]</span><span class="operator">,</span> <span class="variable">x</span><span class="punctuation">[</span><span class="variable">t0</span><span class="punctuation">]</span> <span class="operator">==</span> <span class="variable">x0</span><span class="operator">,</span> </span><br><span class="line">   <span class="variable">x</span><span class="punctuation">[</span><span class="variable">tend</span><span class="punctuation">]</span> <span class="operator">==</span> <span class="variable">xend</span><span class="punctuation">&#125;</span><span class="operator">,</span> <span class="variable">x</span><span class="punctuation">[</span><span class="variable">t</span><span class="punctuation">]</span><span class="operator">,</span> <span class="variable">t</span><span class="punctuation">]</span> <span class="operator">//</span> <span class="built_in">Simplify</span></span><br></pre></td></tr></table></figure>
<p>得到结果：</p>
<p><span class="math display">$$\begin{align}
x^{\mathrm{c}} &amp; =\frac{x \sin
\left[\omega\left(\tau-t_0\right)\right]-x_0 \sin [\omega(\tau-t)]}{\sin
\left[\omega\left(t-t_0\right)\right]} \label{classical}
\end{align}$$</span></p>
<p>将经典路径的表达形式<span
class="math inline">$\eqref{classical}$</span>代入经典路径的路径积分中<span
class="math inline">$\eqref{3_classical}$</span>，可以得到路径积分的具体表达式。</p>
<figure class="highlight mathematica"><table><tr><td class="code"><pre><span class="line"><span class="variable">x</span><span class="punctuation">[</span><span class="variable">t</span><span class="punctuation">]</span> <span class="operator">=</span> <span class="operator">-</span><span class="built_in">Csc</span><span class="punctuation">[</span><span class="punctuation">(</span><span class="variable">t0</span> <span class="operator">-</span> <span class="variable">tend</span><span class="punctuation">)</span> <span class="variable">\[Omega]</span><span class="punctuation">]</span> <span class="punctuation">(</span><span class="variable">xend</span> <span class="built_in">Sin</span><span class="punctuation">[</span><span class="punctuation">(</span><span class="variable">t</span> <span class="operator">-</span> <span class="variable">t0</span><span class="punctuation">)</span> <span class="variable">\[Omega]</span><span class="punctuation">]</span> <span class="operator">-</span> </span><br><span class="line">     <span class="variable">x0</span> <span class="built_in">Sin</span><span class="punctuation">[</span><span class="punctuation">(</span><span class="variable">t</span> <span class="operator">-</span> <span class="variable">tend</span><span class="punctuation">)</span> <span class="variable">\[Omega]</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="operator">;</span></span><br><span class="line"><span class="built_in">Integrate</span><span class="punctuation">[</span></span><br><span class="line">  <span class="variable">m</span><span class="operator">/</span><span class="number">2</span> <span class="punctuation">(</span><span class="built_in">D</span><span class="punctuation">[</span><span class="variable">x</span><span class="punctuation">[</span><span class="variable">t</span><span class="punctuation">]</span><span class="operator">,</span> <span class="punctuation">&#123;</span><span class="variable">t</span><span class="operator">,</span> <span class="number">1</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="operator">^</span><span class="number">2</span> <span class="operator">-</span> <span class="variable">\[Omega]</span><span class="operator">^</span><span class="number">2</span> <span class="variable">x</span><span class="punctuation">[</span><span class="variable">t</span><span class="punctuation">]</span><span class="operator">^</span><span class="number">2</span><span class="punctuation">)</span><span class="operator">,</span> <span class="punctuation">&#123;</span><span class="variable">t</span><span class="operator">,</span> <span class="variable">t0</span><span class="operator">,</span> </span><br><span class="line">   <span class="variable">tend</span><span class="punctuation">&#125;</span><span class="punctuation">]</span> <span class="operator">//</span> <span class="built_in">FullSimplify</span></span><br></pre></td></tr></table></figure>
<p><span class="math display">$$\begin{align}
S^{\mathrm{C}}&amp;=\frac{m \omega}{2}\left\{\left(x^2+x_0^2\right) \cot
\left[\omega\left(t-t_0\right)\right]-2 x x_0 \csc
\left[\omega\left(t-t_0\right)\right]\right\} \\
K\left(x, t ; x_0, t\right)&amp;=\left(\frac{m}{2 \pi i \hbar
\varepsilon}\right)^{\frac{N}{2}} \int e^{i
\frac{\left(S^c+S^{\prime}+S^{\times}\right)}{\hbar}} \mathrm{d} x_1
\cdots \mathrm{d} x_{N-1} \\
&amp; =e^{i \frac{S^c}{\hbar}}\left(\frac{m}{2 \pi i \hbar
\varepsilon}\right)^{\frac{N}{2}} \int e^{i \frac{S^{\prime}}{\hbar}}
\mathrm{d} y_1 \cdots \mathrm{d} y_{N-1}
\end{align}$$</span></p>
<p>路径 <span
class="math inline"><em>x</em><sub><em>n</em></sub><sup>c</sup> = <em>x</em><sup>c</sup>(<em>t</em><sub><em>n</em></sub>)</span>
都是给定的,故: <span
class="math display">d<em>x</em><sub><em>n</em></sub><sup>c</sup> = 0 ⇒ d<em>x</em><sub><em>n</em></sub> = d(<em>x</em><sub><em>n</em></sub><sup>c</sup> + <em>y</em><sub><em>n</em></sub>) = d<em>y</em><sub><em>n</em></sub></span></p>
<p>接下来需要解决的是：</p>
<p><span class="math display">$$\begin{align}
S^{\prime}=\frac{m}{2 \varepsilon} \sum_{n=1}^N\left[\left(\Delta
y_n\right)^2-\varepsilon^2 \omega^2 y_n^2\right] \label{3_Sprime}
\end{align}$$</span></p>
<p>首先将<span
class="math inline"><em>Δ</em><em>y</em><sub><em>n</em></sub></span>利用差分的办法改写为矩阵形式：</p>
<p><span class="math display">$$\begin{align}
\sum_{n=1}^N\left(\Delta y_n\right)^2 &amp;
=\sum_{n=1}^N\left(y_n-y_{n-1}\right)^2 \\
&amp; =\sum_{n=1}^N y_n^2+\sum_{n=1}^N y_{n-1}^2-2 \sum_{n=1}^N y_n
y_{n-1} \\
&amp; \stackrel{y_{N=0}}{=} \sum_{n=1}^{N-1} y_n^2+\sum_{n=0}^{N-1}
y_n^2-2 \sum_{n=1}^{N-1} y_n y_{n-1} \\
&amp; \stackrel{y_0=0}{=} \sum_{n=1}^{N-1} y_n^2+\sum_{n=1}^{N-1}
y_n^2-2 \sum_{n=1}^{N-1} y_n y_{n-1}\\
&amp; =\sum_{n=1}^{N-1}\left(2 y_n^2-2 y_n y_{n-1}\right)=y^{\mathrm{T}}
A y \\
A_{mn}&amp;= 2\delta_{m,n}-\delta_{m-1,n}-\delta_{m,n-1}
\end{align}$$</span></p>
<p>与自由粒子一样，因为是实正定矩阵，可以将其对角化。</p>
<p><span class="math display">$$\begin{align}
K\left(x, t ; x_0, t\right) = &amp; e^{i
\frac{S^c}{\hbar}}\left(\frac{m}{2 \pi i \hbar
\varepsilon}\right)^{\frac{N}{2}} \int e^{i \frac{m}{2 \hbar
\varepsilon}\left(y^{\mathrm{T}} A y-\varepsilon^2 \omega^2
y^{\mathrm{T}} y\right)} \mathrm{d} y_1 \cdots \mathrm{d} y_{N-1} \\
= &amp; e^{i \frac{S^c}{\hbar}}\left(\frac{m}{2 \pi i \hbar
\varepsilon}\right)^{\frac{N}{2}} \int e^{i \frac{m}{2 \hbar
\varepsilon} \sum_{n=1}^{N-1}\left(\lambda_n-\varepsilon^2
\omega^2\right) u_n^2} \mathrm{~d} u_1 \cdots \mathrm{d} u_{N-1} \\
= &amp; e^{i \frac{S^c}{\hbar}}\left(\frac{m}{2 \pi i \hbar
\varepsilon}\right)^{\frac{N}{2}} \prod_{n=1}^{N-1} \int
e^{-\frac{m\left(\lambda_n-\varepsilon^2 \omega^2\right)}{2 i \hbar
\varepsilon} u_n^2} \mathrm{~d} u_n \\
= &amp; e^{i \frac{S^c}{\hbar}}\left(\frac{m}{2 \pi i \hbar
\varepsilon}\right)^{\frac{N}{2}} \prod_{n=1}^{N-1} \sqrt{\frac{2 \pi i
\hbar \varepsilon}{m\left(\lambda_n-\varepsilon^2 \omega^2\right)}}\\
=&amp;e^{i \frac{S^c}{\hbar}} \sqrt{\frac{m}{2 \pi i \hbar
\varepsilon}}\left[\prod_{n=1}^{N-1}\left(\lambda_n-\varepsilon^2
\omega^2\right)\right]^{-\frac{1}{2}}
\end{align}$$</span></p>
<p>接下来解决连乘积<span
class="math inline">$\prod_{n=1}^{N-1}\left(\lambda_n-\varepsilon^2
\omega^2\right)$</span>，对<span
class="math inline">$\eqref{3_Sprime}$</span>的本征多项式入手，与自由粒子的区别在于对角线上会多出来一项：</p>
<p><span class="math display">$$\begin{align}
P_n(\lambda)=\left|\begin{array}{cccccc}
2-\varepsilon^2 \omega^2-\lambda &amp; -1 &amp; 0 &amp; 0 &amp; \cdots
&amp; 0 \\
-1 &amp; 2-\varepsilon^2 \omega^2-\lambda &amp; -1 &amp; 0 &amp; \cdots
&amp; 0 \\
0 &amp; -1 &amp; 2-\varepsilon^2 \omega^2-\lambda &amp; -1 &amp; \cdots
&amp; 0 \\
0 &amp; 0 &amp; -1 &amp; 2-\varepsilon^2 \omega^2-\lambda &amp; \cdots
&amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots
\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 2-\varepsilon^2
\omega^2-\lambda
\end{array}\right|
\end{align}$$</span></p>
<p>对第一行进行展开，可以得到递推关系：</p>
<p><span class="math display">$$\begin{align}
&amp; P_n(\lambda) = (2-\varepsilon^2
\omega^2-\lambda)P_{n-1}(\lambda)-P_{n-2}(\lambda) \\
&amp; P_{n+2}(\lambda)+ P_{n}(\lambda) + (\varepsilon^2
\omega^2+\lambda-2)P_{n+1}(\lambda) = 0 \label{3_ditui}
\end{align}$$</span></p>
<p>定义线性算子<span
class="math inline"><em>L</em><em>P</em><sub><em>n</em></sub>(<em>λ</em>) = <em>P</em><sub><em>n</em> + 1</sub>(<em>λ</em>)</span>，并将其代入递推关系可以得到：</p>
<p><span class="math display">$$\begin{align}
&amp;\left[L^2+ (\varepsilon^2
\omega^2+\lambda-2)L+1\right]P_{n}(\lambda) = 0
\end{align}$$</span></p>
<p>进行分解：</p>
<p><span class="math display">$$\begin{align}
(L-\alpha)(L-\beta)P_{n}(\lambda) &amp;= \left[L^2+ (\varepsilon^2
\omega^2+\lambda-2)L+1\right]P_{n}(\lambda) \\
\alpha &amp;= \frac{1}{2} \left(-\lambda +\sqrt{\left(\lambda +\omega ^2
\varepsilon ^2-2\right)^2-4}-\omega ^2 \varepsilon ^2+2\right) \\
\beta &amp;= \frac{1}{2} \left(-\lambda -\sqrt{\left(\lambda +\omega ^2
\varepsilon ^2-2\right)^2-4}-\omega ^2 \varepsilon ^2+2\right)
\end{align}$$</span></p>
<p>接下来分别求解<span
class="math inline">(<em>L</em> − <em>β</em>)<em>P</em><sub><em>n</em></sub> = 0</span>与<span
class="math inline">(<em>L</em> − <em>α</em>)<em>P</em><sub><em>n</em></sub> = 0</span>:</p>
<p><span class="math display">$$\begin{aligned}
(L-\alpha)P_{n}&amp;=0 \\
LP_{n} &amp;= \alpha P_{n}\\
P_{n+1} &amp;=\alpha P_{n} \\
P_{n} &amp;= \alpha P_{n-1} = \alpha^n P_0(\alpha) \\
P_{n} &amp;= \beta P_{n-1} = \beta^n P_0(\beta) \\
\end{aligned}$$</span></p>
<p>因此<span class="math inline"><em>P</em></span>通项写为：</p>
<p><span
class="math display"><em>P</em><sub><em>n</em></sub> = <em>c</em><sub>1</sub><em>α</em><sup><em>n</em></sup> + <em>c</em><sub>2</sub><em>β</em><sup><em>n</em></sup></span></p>
<p>代入初始条件：</p>
<p><span class="math display">$$\begin{align}
P_1(\lambda) &amp;= 2-\varepsilon^2 \omega^2-\lambda \\
P_2(\lambda) &amp;= (2-\varepsilon^2 \omega^2-\lambda)^2 - 1 \\
\end{align}$$</span></p>
<p>由递推关系<span
class="math inline">$\eqref{3_ditui}$</span>可以得到<span
class="math inline"><em>P</em><sub>0</sub> = 1</span>，<span
class="math inline"><em>P</em><sub><em>n</em></sub></span>的表达式：</p>
<p><span class="math display">$$\begin{align}
c_1 &amp;= -\frac{\beta +\lambda +\omega ^2 \varepsilon ^2-2}{\alpha
-\beta }\\
c_2 &amp;= \frac{\alpha +\lambda +\omega ^2 \varepsilon ^2-2}{\alpha
-\beta } \\
P_{n}(\lambda) &amp;= c_1 \alpha^n+ c_2 \beta^n \\
&amp;= \frac{\beta ^n \left(\alpha -1\right)-\alpha ^n \left(\beta
-1\right)}{\alpha-\beta }
\end{align}$$</span></p>
<p>通过以上讨论已经成功将特征多项<span
class="math inline">$\eqref{3_ditui}$</span>通项表达形式写出来。接下来需要具体写出
<span class="math inline"><em>α</em></span> 与 <span
class="math inline"><em>β</em></span>
的表达形式。结合需要具体求解的问题<span
class="math inline">$\prod_{n=1}^{N-1}\left(\lambda_n-\varepsilon^2
\omega^2\right)=|P_{N-1}(\lambda=0)|$</span>，可知<span
class="math inline"><em>λ</em> = 0</span>:</p>
<p><span class="math display">$$\begin{align}
\lambda &amp;= 0 \\
\alpha &amp;= \frac{1}{2} \left(\sqrt{\left(\omega ^2 \varepsilon
^2-2\right)^2-4}-\omega ^2 \varepsilon ^2+2\right) \\
&amp;\stackrel{\varepsilon \to 0}{=} \frac{1}{2}
\left(\omega  \varepsilon\sqrt{-4}+2\right)\\
\beta &amp;= \frac{1}{2} \left( -\sqrt{\left(\omega ^2 \varepsilon
^2-2\right)^2-4}-\omega ^2 \varepsilon ^2+2\right) \\
&amp;\stackrel{\varepsilon \to 0}{=} \frac{1}{2} \left(-\omega
\varepsilon\sqrt{-4}+2\right)\\
\end{align}$$</span></p>
<p>可以得到：</p>
<p><span class="math display">$$\begin{align}
\prod_{n=1}^{N-1}\left(\lambda_n-\varepsilon^2
\omega^2\right)&amp;=P_{N-1}(\lambda=0) \\
&amp;= \frac{(1+i \omega \varepsilon)^N-(1-i \omega \varepsilon)^N}{1+i
\omega \varepsilon-1+i \omega \varepsilon} \\
&amp;= \frac{1}{2 i \omega \varepsilon}\left\{\left[1+\frac{i
\omega\left(t-t_0\right)}{N}\right]^N-\left[1-\frac{i
\omega\left(t-t_0\right)}{N}\right]^N\right\} \\
&amp; \stackrel{N \rightarrow \infty}{=} \frac{1}{2 i \omega
\varepsilon}\left[e^{i \omega\left(t-t_0\right)}-e^{-i
\omega\left(t-t_0\right)}\right] \\
&amp;=\frac{\sin \left[\omega\left(t-t_0\right)\right]}{\omega
\varepsilon}
\end{align}$$</span></p>
<p>所以谐振子的传播子写为：</p>
<p><span class="math display">$$\begin{align}
K\left(x, t ; x_0, t_0\right)&amp;=e^{i \frac{S^c}{\hbar}} \sqrt{\frac{m
\omega}{2 \pi i \hbar \sin \left[\omega\left(t-t_0\right)\right]}} \\
S^{\mathrm{c}}&amp;=\frac{m \omega}{2}\left\{\left(x^2+x_0^2\right) \cot
\left[\omega\left(t-t_0\right)\right]-2 x x_0 \csc
\left[\omega\left(t-t_0\right)\right]\right\}
\end{align}$$</span></p>
<p><font color='red'>应该考虑相位了，但现在还没出现问题，之再说吧。详细讨论看上面引用的内容。</font></p>
<h1 id="路径积分蒙特卡洛">路径积分蒙特卡洛</h1>
<p>参考： * <a
href="https://zhuanlan.zhihu.com/p/509118112">路径积分蒙特卡洛
PIMC</a></p>
<p>考虑量子力学中的哈密顿量以及波函数：</p>
<p><span class="math display">$$\begin{align}
H&amp;=-\hbar^2 \sum_i \frac{1}{2 m_i} \frac{\partial^2}{\partial
\mathbf{r}_i^2}+V(\mathcal{R}) \\
\psi(\mathcal{R}, t)&amp;=\left\langle\mathcal{R}\left|e^{-i t H /
\hbar}\right| \psi\right\rangle
\end{align}$$</span></p>
<p>在使用蒙卡数值计算的过程中存在两个问题： *
指数是复数，不能用使用正实数概率分布表示 * <span
class="math inline"><em>H</em></span>是量子力学算符，动能与势能不对易</p>
<h2 id="continuing-to-imaginary-time">Continuing to Imaginary Time</h2>
<p>解决复数问题。</p>
<p>通过引入虚时<span
class="math inline"><em>τ</em> = −<em>i</em><em>t</em></span>，将复数变为一个实数处理，这种做法也出现在扩散蒙特卡洛（diffusion
Monte Carlo, DMC）。通过定义配分函数，产生与统计力学的联系：</p>
<p><span class="math display">$$\begin{align}
Z(\beta)&amp;=\int d^{N d}
\mathcal{R}\left\langle\mathcal{R}\left|e^{-\tau H / \hbar}\right|
\mathcal{R}\right\rangle=\operatorname{Tr} e^{-\beta H}, \quad
\beta=\frac{\tau}{\hbar} \\
k_{\mathrm{B}} T&amp;=1 / \beta=\hbar / \tau
\end{align}$$</span></p>
<h2 id="discretizing-the-time-dimension">Discretizing the Time
Dimension</h2>
<p>解决不对易的问题。</p>
<p>将时间<span class="math inline"><em>t</em></span>放在<span
class="math inline"><em>M</em></span>的时间格点上，<span
class="math inline"><em>Δ</em><em>τ</em> = <em>τ</em>/<em>M</em></span>，这样有<span
class="math inline"><em>M</em> − 1</span>个中间时间。在每一个中间的时间步骤中都有完整的本征态：</p>
<p><span class="math display">$$\begin{align}
\mathbf{1}=\int d^{N d}
\mathcal{R}_i\left|\mathcal{R}_i\right\rangle\left\langle\mathcal{R}_i\right|,
\quad i=1, \ldots, M-1
\end{align}$$</span></p>
<p>然后沿着时间的顺序将配分函数分解：</p>
<p><span class="math display">$$\begin{align}
&amp; Z(\beta)= \int d^{N d}
\mathcal{R}\left\langle\mathcal{R}\left|e^{-\tau H / \hbar}\right|
\mathcal{R}\right\rangle \\
&amp;=\int d \mathcal{R}_0 \int d \mathcal{R}_1 \ldots \int d
\mathcal{R}_{M-1}\left\langle\mathcal{R}_0\left|e^{-\Delta \tau H /
\hbar}\right| \mathcal{R}_{M-1}\right\rangle\times
\ldots\times\left\langle\mathcal{R}_2\left|e^{-\Delta \tau H /
\hbar}\right|
\mathcal{R}_1\right\rangle\left\langle\mathcal{R}_1\left|e^{-\Delta \tau
H / \hbar}\right| \mathcal{R}_0\right\rangle
\end{align}$$</span></p>
<p>根据时间分割，将一段时间的演化过程，转变为几个“瞬间”的演化过程<span
class="math inline"><em>Δ</em><em>τ</em> → 0</span>。这其实就是路径积分的思想。</p>
<p>接下来通过这样的小量，解决不对易的关系。根据 Baker-Campbell-Hausdorff
(BCH)定理：</p>
<p><span class="math display">$$\begin{align}
e^{\mathcal{A}} e^{\mathcal{B}}&amp;=e^{\mathcal{C}} \\
\mathcal{C}&amp;=\mathcal{A}+\mathcal{B}+\frac{1}{2}[\mathcal{A},
\mathcal{B}]+\cdots
\end{align}$$</span></p>
<p>把这个关系用在短时演化的过程中：</p>
<p><span class="math display">$$\begin{align}
\mathcal{C}&amp;=-\Delta \tau H / \hbar\\
\mathcal{B}&amp;=-\Delta \tau V / \hbar\\
\mathcal{A}&amp;=-\Delta \tau K / \hbar \\
K&amp;=-\hbar^2 \sum_i \frac{1}{2 m_i} \frac{\partial^2}{\partial
\mathbf{r}_i^2}
\end{align}$$</span></p>
<p>通过BCH定理可以知道，将<span
class="math inline"><em>H</em></span>分为两项，需要计算交叉项。但是这个交叉项是<span
class="math inline"><em>Δ</em><em>τ</em></span>的高阶小量可以直接忽略。</p>
<p><span class="math display">$$\begin{align}
\left\langle\mathcal{R}_1\left|e^{-\Delta \tau H / \hbar}\right|
\mathcal{R}_0\right\rangle
&amp;\simeq\left\langle\mathcal{R}_1\left|e^{-\Delta \tau K / \hbar}
e^{-\Delta \tau V / \hbar}\right|
\mathcal{R}_0\right\rangle=\left\langle\mathcal{R}_1\left|e^{-\Delta
\tau K / \hbar}\right| \mathcal{R}_0\right\rangle e^{-\Delta \tau
V\left(\mathcal{R}_0\right) / \hbar} \\
\left\langle\mathcal{R}_1\left|e^{-\Delta \tau K / \hbar}\right|
\mathcal{R}_0\right\rangle &amp; =\int d \mathcal{P}_0 \int d
\mathcal{P}_1\left\langle\mathcal{R}_1 \mid
\mathcal{P}_1\right\rangle\left\langle\mathcal{P}_1\left|e^{-\Delta \tau
K / \hbar}\right| \mathcal{P}_0\right\rangle\left\langle\mathcal{P}_0
\mid \mathcal{R}_0\right\rangle \\
&amp; =\left(\frac{m}{2 \pi \hbar \Delta \tau}\right)^{N d / 2} \exp
\left[-\frac{m \Delta \tau}{2 \hbar} \sum_i\left(\frac{\mathbf{r}_{i
1}-\mathbf{r}_{i 0}}{\Delta \tau}\right)^2\right]
\end{align}$$</span></p>
<h2 id="path-integral-monte-carlo-algorithm">Path Integral Monte Carlo
Algorithm</h2>
<p>配分函数的最终表达式为：</p>
<p><span class="math display">$$\begin{align}
Z(\beta) \simeq\left(\frac{m}{2 \pi \hbar \Delta \tau}\right)^{M N d /
2} \int d \mathcal{R}_0 \int d \mathcal{R}_1 \ldots \int d
\mathcal{R}_{M-1} \quad \times \exp \left\{-\frac{\Delta \tau}{\hbar}
\sum_{j=0}^{M-1}\left[\frac{m}{2}\left(\frac{\mathcal{R}_{j+1}-\mathcal{R}_j}{\Delta
\tau}\right)^2+V\left(\mathcal{R}_j\right)\right]\right\}
\end{align}$$</span></p>
<p>看这个形式，对比统计晶格中的形式。具有一定的对应关系，路径积分等价于晶格所有构型的求和，因此真正需要关心的是“能量项”：</p>
<p><span class="math display">$$\begin{align}
\sum_{j=0}^{M-1}\left[\frac{m}{2}\left(\frac{\mathcal{R}_{j+1}-\mathcal{R}_j}{\Delta
\tau}\right)^2+V\left(\mathcal{R}_j\right)\right]
\end{align}$$</span></p>
<p>将每一格运动区间进行计算，从而得出最终的“能量”，更准确应该是作用量。然后利用
Metropolis algorithm 进行重要性抽样，将不同权重的路径进行加权运算。</p>
<h2 id="code">Code</h2>
<p>以下是一个对于一维谐振子的实现，核心部分在<code>sample</code>模块中。这个实现，每次只对前一次路径中一步进行修改，然后接受概率设为<code>exp(-DeltaE * Delta_t / hbar)</code>，这里作者将<span
class="math inline">ℏ = 1</span>进行无量纲化了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PathIntegral</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, nt=<span class="number">100</span>, tmax = <span class="number">2</span>*np.pi</span>):</span><br><span class="line">        <span class="variable language_">self</span>.nt = nt</span><br><span class="line">        <span class="variable language_">self</span>.tmax = tmax</span><br><span class="line">        <span class="variable language_">self</span>.t = np.linspace(<span class="number">0</span>, <span class="variable language_">self</span>.tmax, nt)</span><br><span class="line">        <span class="variable language_">self</span>.sample_idx = []</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, steps=<span class="number">100000</span></span>):</span><br><span class="line">        <span class="comment"># 1. initialize the path</span></span><br><span class="line">        x = np.zeros_like(<span class="variable language_">self</span>.t)</span><br><span class="line">        dt = <span class="variable language_">self</span>.t[<span class="number">1</span>] - <span class="variable language_">self</span>.t[<span class="number">0</span>]</span><br><span class="line">        ilist = np.random.choice(<span class="built_in">range</span>(<span class="number">1</span>, <span class="variable language_">self</span>.nt-<span class="number">1</span>), size=steps)</span><br><span class="line">        dxlist = np.random.uniform(-<span class="number">1</span>, <span class="number">1</span>, size=steps)</span><br><span class="line">        sampled = []           </span><br><span class="line">        <span class="comment"># randomly pick x[i]</span></span><br><span class="line">        <span class="comment"># do random walk x[i] -&gt; x[i] + r </span></span><br><span class="line">        <span class="comment"># if DeltaE &lt; 0: accept</span></span><br><span class="line">        <span class="comment"># elif DeltaE &gt; 0: accept with probability exp(-DeltaE * Delta_t / hbar)</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(steps)):</span><br><span class="line">            i = ilist[n]</span><br><span class="line">            dx = dxlist[n]</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#if x[i] + dx &lt;= 0.0: continue</span></span><br><span class="line">            ds = (<span class="variable language_">self</span>.kinetic_energy(x[i-<span class="number">1</span>], x[i]+dx, dt) -</span><br><span class="line">                  <span class="variable language_">self</span>.kinetic_energy(x[i-<span class="number">1</span>], x[i], dt) +</span><br><span class="line">                  <span class="variable language_">self</span>.kinetic_energy(x[i]+dx, x[i+<span class="number">1</span>], dt) -</span><br><span class="line">                  <span class="variable language_">self</span>.kinetic_energy(x[i], x[i+<span class="number">1</span>], dt) + </span><br><span class="line">                  <span class="variable language_">self</span>.potential(x[i]+dx) -</span><br><span class="line">                  <span class="variable language_">self</span>.potential(x[i])</span><br><span class="line">                 ) * dt</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> ds &lt; <span class="number">0</span>: </span><br><span class="line">                x[i] = x[i] + dx</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                r = np.random.rand()</span><br><span class="line">                <span class="keyword">if</span> r &lt; np.exp(-ds):</span><br><span class="line">                    x[i] = x[i] + dx</span><br><span class="line">            sampled.append(x[i])</span><br><span class="line">            </span><br><span class="line">            <span class="variable language_">self</span>.sample_idx.append(i)</span><br><span class="line">        <span class="keyword">return</span> sampled, <span class="variable language_">self</span>.sample_idx</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kinetic_energy</span>(<span class="params">self, x1, x2, dt</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span> * (x2 - x1)**<span class="number">2</span> / dt**<span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">potential</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span> * x**<span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">plot</span>(<span class="params">self</span>):</span><br><span class="line">        tmax = <span class="variable language_">self</span>.tmax</span><br><span class="line">        plt.vlines(<span class="variable language_">self</span>.t, ymin=-tmax, ymax=tmax, color=<span class="string">&#x27;k&#x27;</span>, linestyles=<span class="string">&#x27;dotted&#x27;</span>, alpha=<span class="number">0.9</span>)</span><br><span class="line">        dt = <span class="variable language_">self</span>.t[<span class="number">1</span>] - <span class="variable language_">self</span>.t[<span class="number">0</span>]</span><br><span class="line">        xi = np.random.uniform(-<span class="number">1</span>, <span class="number">1</span>, <span class="variable language_">self</span>.nt)</span><br><span class="line">        xi[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        xi = np.cumsum(xi)</span><br><span class="line">        xi[-<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">        plt.plot(<span class="variable language_">self</span>.t, xi)</span><br><span class="line">        plt.ylabel(<span class="string">r&#x27;x $\rightarrow$&#x27;</span>, loc=<span class="string">&#x27;center&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">r&#x27;t $\rightarrow$&#x27;</span>, loc=<span class="string">&#x27;center&#x27;</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Quantum Field Theory</tag>
        <tag>Path Integrals</tag>
        <tag>Path Integrals Monte Carlo</tag>
      </tags>
  </entry>
  <entry>
    <title>Hopfield Model</title>
    <url>/2022/11/12/Phys/Hopfield/Hopfield/</url>
    <content><![CDATA[<p>讨论Hopfield Model
的相关内容，包含其本身的一些性值以及其在组合优化问题中的应用。</p>
<p>Reference: * <a
href="https://www.pnas.org/doi/10.1073/pnas.79.8.2554">Neural networks
and physical systems with emergent collective computational
abilities</a> * <a
href="https://link.springer.com/article/10.1007/BF00339943">“Neural”
Computation of Decisions in Optimization Problems</a> * <a
href="https://www.labxing.com/lab/666/news/677">神经网络的统计力学</a> *
<a
href="https://www.researchgate.net/publication/220669035_Neural_Networks_for_Combinatorial_Optimization_A_Review_of_More_Than_a_Decade_of_Research?enrichId=rgreq-c2245af59a8dd59271293a87e4cf98fc-XXX&amp;enrichSource=Y292ZXJQYWdlOzIyMDY2OTAzNTtBUzo5OTQ4NzkzODc3NzExM0AxNDAwNzMxMTgxOTMy&amp;el=1_x_3&amp;_esc=publicationCoverPdf">Neural
Networks for Combinatorial Optimization: A Review of More Than a Decade
of Research</a></p>
<p>Link: * <a href="/2022/11/13/Phys/Hopfield/Hopfield1/" title="Hopfield Model 自由能">Hopfield 自由能求解</a></p>
<span id="more"></span>
<h1 id="hopfield-model">Hopfield Model</h1>
<p><img src="./Hopfield.png" alt="Hopfield Model" />
由上图所示网络，格点<span
class="math inline"><em>s</em></span>取分立的<span
class="math inline">−1, +1</span>两个值，任意两个格点之间都有连接权重为<span
class="math inline"><em>ω</em></span>. 假定一种二进制的状态<span
class="math inline"><em>ξ</em></span>，利用Hebbian rule设定权重<span
class="math inline"><em>ω</em></span>为：</p>
<p><span class="math display">$$\begin{equation}
\begin{aligned}
\omega_{ij}&amp;=\frac{1}{N}\xi_i\xi_j \quad i\neq j\\
\omega_{ii}&amp;=0
\end{aligned} \label{omega}
\end{equation}
$$</span></p>
<p>其中<span
class="math inline"><em>N</em></span>总格点数。这种状态<span
class="math inline"><em>ξ</em></span>就是一个吸引子，是能量最低点（由于对称性，反号也是最低点）。也可以设定多个状态<span
class="math inline"><em>ξ</em></span>，通过平均将其结合在一起：</p>
<p><span class="math display">$$\begin{align}
\omega_{ij}&amp;=\frac{1}{N}\sum_{l}^{P}\xi_i^{(l)}\xi_j^{(l)} \quad
i\neq j\\
\omega_{ii}&amp;=0
\end{align}$$</span></p>
<p>其中<span class="math inline"><em>M</em></span>吸引子的数量。</p>
<p>如何从任意初始状态转变到吸引子的状态呢？接下来设计动力学过程：</p>
<p><span class="math display">$$\begin{align}
S_i(t+1) \leftarrow \operatorname{sgn}\left(\sum_j w_{i j}
S_j(t)-\theta_i\right)
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>θ</em><sub><em>i</em></sub></span>是一个偏置项。这个过程本质就是一个零温的蒙特卡洛模拟过程。</p>
<figure>
<img src="./landscape.png" alt="landscape" />
<figcaption aria-hidden="true">landscape</figcaption>
</figure>
<p>Hamiltonian写为： <span class="math display">$$\begin{equation}
H=-\frac{1}{2} \sum_{i, j}^N w_{i j} S_i S_j \label{hamiltonian}
\end{equation}$$</span></p>
<p>接下来的讨论假设每一个模式都是完全随机选取的，即<span
class="math inline"><em>P</em>(<em>ξ</em> = ±1) = 1/2</span>。<font color='blue'>则是一个很强的设定。</font></p>
<h1 id="组合优化问题中的应用">组合优化问题中的应用</h1>
<p>旅行商问题是给出一些地点，以及这些地点两两之间的距离，从一个点出发走完所有点，要求花费的路层最短。</p>
<p>难点在于如何将旅行商问题转化为 hopfield
形式。以五个城市ABCDE为例，将每一个城市由五位二进制编码，例如B对应<span
class="math inline">01000</span>、D对应<span
class="math inline">00010</span>。这样履行路径可以表示为： <span
class="math display">$$
\begin{array}{l|lllll}
&amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\
\hline A &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
B &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
C &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
D &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
E &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0
\end{array}
$$</span></p>
<p>一个由<span
class="math inline"><em>n</em></span>个城市组成的问题，可以利用<span
class="math inline"><em>n</em><sup>2</sup></span>个节点组成的Hopfield网络表示。</p>
<p>首先这样的表示有一些约束，约束的能量项表示为： <span
class="math display">$$\begin{align}
E_{c}=  A / 2 \sum_X \sum_i \sum_{j \neq i} V_{X i} V_{X j}+B / 2 \sum_i
\sum_X \sum_{X \neq Y} V_{X i} V_{Y i}+C / 2\left(\sum_X \sum_i V_{X
i}-n\right)^2
\end{align}$$</span> 其中<span
class="math inline"><em>V</em><sub><em>X</em><em>i</em></sub></span>表示Hopfield网络中节点，代表城市<span
class="math inline"><em>X</em></span>以旅行位置<span
class="math inline"><em>i</em></span>。在这个能量项中，第一项约束在旅行中每次只能访问一个城市，第二项表示每个城市只能被访问一次，第三项表示每个城市都应该被访问一次。如果所有的约束条件都满足<span
class="math inline"><em>E</em><sub><em>c</em></sub> = 0</span>。</p>
<p>用于计算路程的项写为：</p>
<p><span class="math display">$$\begin{align}
E_l = \frac{1}{2} D \sum_X \sum_{Y \neq X} \sum_i d_{X Y} V_{X
i}\left(V_{Y, i+1}+V_{Y, i-1}\right)
\end{align}$$</span></p>
<p>总能量项目为: <span class="math display">$$\begin{align}
E=E_c+E_l
\end{align}$$</span></p>
<p>至此已经将旅行商问题转化为Hopfield网络上，只需要迭代求解即可。<font color='red'>此时的参数其实十分关键，并且作者对动力学更新方程有新的参数加入。</font></p>
<p><font color='red'>这样就构建了一个桥梁，将组合优化问题转移到Spin
Glass中，怎么更加清晰的分析这个问题就十分有趣了。</font></p>
<figure>
<img src="./TSP1.png" alt="TSP1" />
<figcaption aria-hidden="true">TSP1</figcaption>
</figure>
<a href="/2022/11/13/Phys/Hopfield/Hopfield1/" title="Hopfield Model 自由能">Hopfield 自由能求解</a>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Combinatorial Optimization Methods</tag>
        <tag>Hopfield Model</tag>
      </tags>
  </entry>
  <entry>
    <title>Hopfield Model 自由能</title>
    <url>/2022/11/13/Phys/Hopfield/Hopfield1/</url>
    <content><![CDATA[<p>Link: * <a href="/2022/11/14/Phys/Hopfield/Hopfield2/" title="Hopfield Model">Hopfield 相图</a> * <a href="/2022/11/13/Phys/replica/replica/" title="Replica">Replica</a></p>
<p>根据前文已知： <span class="math display">$$\begin{equation}
\begin{aligned}
\omega_{ij}&amp;=\frac{1}{N}\xi_i\xi_j \quad i\neq j\\
\omega_{ii}&amp;=0
\end{aligned} \label{omega}
\end{equation} $$</span> <span class="math display">$$\begin{equation}
H=-\frac{1}{2} \sum_{i, j}^N w_{i j} S_i S_j \label{hamiltonian}
\end{equation}$$</span></p>
<h1 id="free-energy">Free Energy</h1>
<p>假定能够存储<span
class="math inline"><em>P</em> = <em>α</em><em>N</em></span>个随机模式。结合<span
class="math inline">$\eqref{omega}$</span>与<span
class="math inline">$\eqref{hamiltonian}$</span>有：</p>
<p><span class="math display">$$\begin{aligned}
\left\langle Z^n\right\rangle &amp; =\left\langle\operatorname{Tr} \exp
\left[\frac{\beta}{2 N} \sum_{i, j}^N \sum_{\mu=1}^P \sum_{\rho=1}^n
\xi_i^\mu \xi_j^\mu S_i^\rho S_j^\rho\right]\right\rangle \\
&amp; =\left\langle\operatorname{Tr} \exp \left[\frac{\beta}{2 N}
\sum_{\rho, \mu}\left(\sum_i \xi_i^\mu S_i^\rho\right)\left(\sum_j
\xi_j^\mu S_j^\rho\right)\right]\right\rangle \\
&amp; =\left\langle\operatorname{Tr} \exp \left[\frac{\beta N}{2}
\sum_{\rho, \mu}\left(\frac{1}{N} \sum_i \xi_i^\mu
S_i^\rho\right)^2\right]\right\rangle \\
&amp; =\left\langle\operatorname{Tr} \prod_{\rho, \mu} \exp
\left[\frac{\beta N}{2}\left(\frac{1}{N} \sum_i \xi_i^\mu
S_i^\rho\right)^2\right]\right\rangle,
\end{aligned}$$</span></p>
<p>其中<span class="math inline">Tr </span>表示对所有构型<span
class="math inline"><strong>S</strong></span>求和，<span
class="math inline">⟨⋅⟩</span>表示淬火无序平均。使用Hubbard-Stratonovich
transformation技巧，将其中指数中的二次项转化为高斯积分：</p>
<p><span class="math display">$$\begin{align}
e^{a b^2}=\sqrt{\frac{a}{\pi}} \int e^{-a x^2+2 a b x} d x
\end{align}$$</span></p>
<p>设定这些参数为：</p>
<p><span class="math display">$$\begin{align}
\left\{\begin{array}{l}
b \rightarrow \frac{1}{N} \sum_i \xi_i^\mu S_i^\rho \\
x \rightarrow m_\rho^\mu \\
a \rightarrow \frac{\beta N}{2}
\end{array} \right.
\end{align}$$</span></p>
<p>通过对<span
class="math inline"><em>m</em><sub><em>ρ</em></sub><sup><em>μ</em></sup></span>积分得到：
<span class="math display">$$\begin{align}
\left\langle Z^n\right\rangle &amp; =\left\langle\operatorname{Tr} \int
\prod_{\rho, \mu} \sqrt{\frac{\beta N}{2 \pi}} d m_\rho^\mu \exp
\left[-\frac{\beta N}{2}\left(m_\rho^\mu\right)^2+\beta m_\rho^\mu
\sum_i \xi_i^\mu S_i^\rho\right]\right\rangle \\
&amp; =\left\langle\operatorname{Tr} \int \prod_{\rho', \mu'}
\sqrt{\frac{\beta N}{2 \pi}} d m_{\rho'}^{\mu'} \exp \left[-\frac{\beta
N}{2} \sum_{\rho, \mu}\left(m_\rho^\mu\right)^2+\beta \sum_{\rho, \mu}
m_\rho^\mu \sum_i \xi_i^\mu S_i^\rho\right]\right\rangle \\
&amp; =\left\langle\operatorname{Tr} \int \prod_{\rho', \mu'}
\sqrt{\frac{\beta N}{2 \pi}} d m_{\rho'}^{\mu'} \exp \left[-\frac{\beta
N}{2} \sum_{\mu \geq 2}
\sum_\rho\left(m_\rho^\mu\right)^2+\right.\left.\beta \sum_{\mu \geq 2}
\sum_\rho m_\rho^\mu \sum_i \xi_i^\mu S_i^\rho-\frac{\beta N}{2}
\sum_\rho\left(m_\rho^1\right)^2+\beta \sum_\rho m_\rho^1 \sum_i \xi_i^1
S_i^\rho\right]\right\rangle .
\end{align}$$</span></p>
<p>最后假设只有第一个模式被恢复<span
class="math inline"><em>μ</em> = 1</span>，在表达式中将其单独分出来，因此交错项<span
class="math inline"><em>m</em><sub><em>ρ</em></sub><sup><em>μ</em></sup> ∼ <em>O</em>(1)</span>。</p>
<p>接下来考虑没有被恢复的模式(<span
class="math inline"><em>μ</em> ≥ 2</span>)，由于没有被恢复，因此<span
class="math inline">⟨∑<sub><em>i</em></sub><em>ξ</em><sub><em>i</em></sub><sup><em>μ</em></sup><em>S</em><sub><em>i</em></sub><sup><em>ρ</em></sup>⟩<sub><em>ξ</em></sub> = 0</span>并且<span
class="math inline">⟨(∑<sub><em>i</em></sub><em>ξ</em><sub><em>i</em></sub><sup><em>μ</em></sup><em>S</em><sub><em>i</em></sub><sup><em>ρ</em></sup>)<sup>2</sup>⟩<sub><em>ξ</em></sub> = <em>N</em> + ⟨∑<sub><em>i</em> ≠ <em>j</em></sub><em>ξ</em><sub><em>i</em></sub><sup><em>μ</em></sup><em>ξ</em><sub><em>j</em></sub><sup><em>μ</em></sup><em>S</em><sub><em>i</em></sub><sup><em>ρ</em></sup><em>S</em><sub><em>j</em></sub><sup><em>ρ</em></sup>⟩<sub><em>ξ</em></sub> = <em>N</em></span>。<span
class="math inline"><em>m</em><sub><em>ρ</em></sub><sup><em>μ</em></sup>(<em>μ</em> ≥ 2)</span>的大小数量级为（可以将这个过程认为是一维链上的随机游走过程，计算的是绝对值的期望）：
<span class="math display">$$\begin{align}
m_\rho^\mu=\frac{1}{N} \sum_i \xi_i^\mu S_i^\rho \approx
O\left(\frac{1}{\sqrt{N}}\right)
\end{align}$$</span></p>
<p>为了将整个数量级变为<span
class="math inline"><em>O</em>(1)</span>，因此将<span
class="math inline"><em>m</em><sub><em>ρ</em></sub><sup><em>μ</em></sup></span>进行变换<span
class="math inline">$m_\rho^\mu \rightarrow
\frac{m_\rho^\mu}{\sqrt{\beta N}}$</span>。变化之后为：</p>
<p><span class="math display">$$
\begin{align}
\left\langle Z^n\right\rangle= &amp; \left(\frac{1}{\sqrt{2
\pi}}\right)^{n(P-1)}\left\langle\operatorname { T r } \int \prod _ {
\rho' , \mu' &gt; 1 } d m _ { \rho' } ^ { \mu' } \prod _ { \rho } \sqrt
{ \beta N } d m _ { \rho } ^ { 1 } \operatorname { e x p }
\left[-\frac{1}{2} \sum_{\mu \geq 2} \sum_\rho\left(m_\rho^\mu\right)^2+
\sqrt{\frac{\beta}{N}} \sum_{\mu \geq 2} \sum_\rho m_\rho^\mu \sum_i
\xi_i^\mu S_i^\rho-\frac{\beta N}{2}
\sum_\rho\left(m_\rho^1\right)^2+\beta \sum_\rho m_\rho^1 \sum_i \xi_i^1
S_i^\rho\right]\right\rangle \label{znstart}
\end{align}
$$</span></p>
<p>接下来将其中的每一部分进行分解化简。</p>
<p>对于<span
class="math inline"><em>μ</em> ≥ 2</span>的部分，由于模式<span
class="math inline"><em>ξ</em></span>是随机选取的，在模式的数量较多的情况下可以认为存在<span
class="math inline">⟨exp (<em>A</em><em>ξ</em>)⟩<sub>{<em>ξ</em> = ±1}</sub> = exp (−<em>A</em>) + exp (<em>A</em>) = 2cosh (<em>A</em>) ∝ exp (ln cosh (<em>A</em>))</span>，再结合近似<span
class="math inline">$\ln \cosh x=\frac{x^2}{2}+\cdots$</span> 在 <span
class="math inline"><em>x</em> → 0</span>：</p>
<p><span class="math display">$$\begin{aligned}
&amp; \left\langle\exp \left[\sqrt{\frac{\beta}{N}} \sum_{\mu \geq 2}
\sum_\rho m_\rho^\mu \sum_i \xi_i^\mu
S_i^\rho\right]\right\rangle_{\xi_i^\mu: \mu&gt;1} \\
&amp; \propto \exp \left[\sum_{\mu \geq 2, i} \ln \cosh
\left(\sqrt{\frac{\beta}{N}} \sum_\rho m_\rho^\mu S_i^\rho\right)\right]
\\
&amp; \cong \exp \left[\sum_{\mu \geq 2} \sum_i \frac{\beta}{2
N}\left(\sum_\rho m_\rho^\mu S_i^\rho\right)^2\right]
\end{aligned}$$</span></p>
<p>然后将平方项<span
class="math inline">(<em>m</em><sub><em>ρ</em></sub><sup><em>μ</em></sup>)<sup>2</sup></span>改写：
<span class="math display">$$\begin{align}
\sum_\rho\left(m_\rho^\mu\right)^2&amp;=\sum_{\rho, \sigma} m_\rho^\mu
\delta_{\rho \sigma} m_\sigma^\mu \\
\frac{1}{N} \sum_i\left(\sum_\rho m_\rho^\mu S_i^\rho\right)^2 &amp;
=\frac{1}{N} \sum_i \sum_\rho m_\rho^\mu S_i^\rho \sum_\sigma
m_\sigma^\mu S_i^\sigma \\
&amp; =\sum_{\rho, \sigma} m_\rho^\mu \frac{1}{N} \sum_i S_i^\rho
S_i^\sigma m_\sigma^\mu \\
&amp; :=\sum_{\rho, \sigma} m_\rho^\mu q_{\rho \sigma} m_\sigma^\mu
\end{align}$$</span></p>
<p>为了进一步化简该表达式：</p>
<p><span class="math display">$$\begin{align}
\kappa_{\rho \sigma}=\delta_{\rho \sigma}-\frac{\beta}{N} \sum_i
S_i^\rho S_i^\sigma:=\delta_{\rho \sigma}-\beta q_{\rho \sigma}
\end{align}$$</span></p>
<p>写为矩阵形式：</p>
<p><span class="math display">$$\begin{align}
\mathbf{K}&amp;=\mathbf{I}-\beta \mathbf{Q} \\
q_{\rho \sigma}&amp;=\left\{\begin{array}{ll}
\frac{1}{N} \sum_i S_i^\rho S_i^\sigma &amp; \rho \neq \sigma \\
1 &amp; \rho=\sigma
\end{array}\right. \label{q}
\end{align}$$</span></p>
<p><span
class="math inline"><strong>K</strong>, <strong>Q</strong></span>
是对称的 <span class="math inline"><em>n</em> × <em>n</em></span>
矩阵。因此有：</p>
<p><span class="math display">$$\begin{equation}
\begin{aligned}
\left\langle Z^n\right\rangle &amp; \propto \operatorname{Tr} \int
\prod_{\rho, \sigma} d q_{\rho \sigma} \delta\left(q_{\rho
\sigma}-\frac{1}{N} \sum_i S_i^\rho S_i^\sigma\right) \\
&amp; \times \prod_{\mu \geq 2, \rho} d m_\rho^\mu \exp
\left[-\frac{1}{2} \sum_{\mu \geq 2} \sum_{\rho, \sigma} m_\rho^\mu
\kappa_{\rho \sigma} m_\sigma^\mu\right] \\
&amp; \times\left\langle\int \prod_\rho d m_\rho^1 \exp
\left[-\frac{\beta N}{2} \sum_\rho\left(m_\rho^1\right)^2+\beta
\sum_\rho m_\rho^1 \sum_i \xi_i^1 S_i^\rho\right]\right\rangle_{\xi^1}
\end{aligned} \label{zn}
\end{equation}$$</span></p>
<p>式<span
class="math inline">$\eqref{zn}$</span>中第一行的作用，将<span
class="math inline">$\eqref{q}$</span>关系明确的写进计算公式中，忽略不相关因子。利用多变量高斯积分：</p>
<p><span class="math display">$$
\int_{R^n} d \mathbf{m} e^{-\mathbf{M}^{\mathrm{T}} \mathbf{K
M}}=\sqrt{\frac{\pi^n}{\operatorname{det}(\mathbf{K})}},
$$</span></p>
<p>可以得到：</p>
<p><span class="math display">$$\begin{align}
\int \prod_{\mu \geq 2, \rho} d m_\rho^\mu \exp \left[-\frac{1}{2}
\sum_{\mu \geq 2} \sum_{\rho, \sigma} m_\rho^\mu \kappa_{\rho \sigma}
m_\sigma^\mu\right]=\frac{C}{(\operatorname{det}
\mathbf{K})^{\frac{P-1}{2}}}
\end{align}$$</span> 其中<span
class="math inline"><em>C</em></span>是常量。</p>
<p>由于<span
class="math inline">det (<em>e</em><sup><strong>K</strong></sup>) = <em>e</em><sup>Tr <strong>K</strong></sup></span>，可得<span
class="math inline">det <strong>K</strong> = <em>e</em><sup>Tr ln <strong>K</strong></sup></span>，因此有：</p>
<p><span class="math display">$$\begin{align}
(\operatorname{det} \mathbf{K})^{-\frac{P-1}{2}}=e^{-\frac{P-1}{2}
\operatorname{Tr} \ln \mathbf{K}}=e^{-\frac{P-1}{2} \operatorname{Tr}
\ln [\mathbf{I}-\beta \mathbf{Q}]}
\end{align}$$</span></p>
<p>再结合Dirac函数的傅里叶分解：</p>
<p><span class="math display">$$\begin{align}
\delta(x)=\frac{1}{2 \pi} \int_{-\infty}^{+\infty} e^{-\mathrm{i} k x} d
k,
\end{align}$$</span></p>
<p>将<span
class="math inline">$\eqref{zn}$</span>中第一行Dirac函数进行变换：</p>
<p><span class="math display">$$
\begin{aligned}
\delta\left(q_{\rho \sigma}-\frac{1}{N} \sum_i S_i^\rho
S_i^\sigma\right) \propto \int_{-\infty}^{+\infty} d r_{\rho \sigma}
\exp \left[-i r_{\rho \sigma} q_{\rho \sigma}+ i r_{\rho \sigma}
\frac{1}{N} \sum_i S_i^\rho S_i^\sigma\right]
\end{aligned}
$$</span></p>
<p>将<span class="math inline"><em>r</em></span>进行重标度<span
class="math inline">$r_{\rho \sigma} \rightarrow -\frac{\mathrm{i} N
\alpha \beta^2}{2} r_{\rho \sigma}$</span>，使得<span
class="math inline"><em>r</em><sub><em>ρ</em><em>σ</em></sub> ∼ <em>O</em>(1)</span>，这种操作对计算结果没有影响。上式变换忽略了因子项。
<font color='blue'>这样操作使得最后的结果变得简洁。</font>其中<span
class="math inline"><em>α</em> = <em>P</em>/<em>N</em></span>。</p>
<p><span class="math display">$$\begin{align}
\delta\left(q_{\rho \sigma}-\frac{1}{N} \sum_i S_i^\rho
S_i^\sigma\right) \propto \int_{-\infty}^{+\infty} d r_{\rho \sigma}
\exp \left[-\frac{N \alpha \beta^2}{2} r_{\rho \sigma} q_{\rho
\sigma}+\frac{\alpha \beta^2}{2} \sum_{i} r_{\rho \sigma} S_i^\rho
S_i^\sigma\right] \label{fuliye}
\end{align}$$</span></p>
<p>接下来将<span
class="math inline">$\eqref{zn}$</span>中的最后一项与<span
class="math inline">$\eqref{fuliye}$</span>中最后一项组合一起： <span
class="math display">$$\begin{align}
\left\langle\operatorname{Tr} \exp \left[\beta \sum_\rho m_\rho^1 \sum_i
\xi_i^1 S_i^\rho+\frac{\alpha \beta^2}{2} \sum_{i} r_{\rho \sigma}
S_i^\rho S_i^\sigma\right]\right\rangle_{\xi^1} = &amp; \left\langle\exp
\left\{\sum_i \ln \operatorname{Tr} \exp \left(\beta \sum_\rho m_\rho^1
\xi_i^1 S^\rho+\frac{\alpha \beta^2}{2} r_{\rho \sigma} S^\rho
S^\sigma\right)\right\}\right\rangle_{\xi^1} \\
= &amp; \exp \left\{N\left\langle\ln \operatorname{Tr} \exp \left(\beta
\sum_\rho m_\rho^1 \xi^1 S^\rho+\frac{\alpha \beta^2}{2}  r_{\rho
\sigma} S^\rho S^\sigma\right)\right\rangle_{\xi^1}\right\} \\
:= &amp; \exp \left\{N\left\langle\ln \operatorname{Tr} \exp \left(\beta
H_{\xi^1}\right)\right\rangle_{\xi^1}\right\}
\end{align}$$</span></p>
<p>定义，因为上面分析的是一个<span
class="math inline"><em>q</em><sub><em>ρ</em><em>σ</em></sub></span>的<span
class="math inline"><em>δ</em></span>函数展开，原来表达式中在积分符号中含有<span
class="math inline">∏<sub><em>ρ</em><em>σ</em></sub></span>，在指数中将会增加求和符号<span
class="math inline">∑<sub><em>ρ</em><em>σ</em></sub></span>： <span
class="math display">$$\begin{align}
\beta H_{\xi^1}=\frac{1}{2} \alpha \beta^2 \sum_{\rho \sigma}r_{\rho
\sigma} S^\rho S^\sigma+\beta \sum_\rho m_\rho^1 \xi^1 S^\rho,
\end{align}$$</span></p>
<p>最终<span class="math inline">$\eqref{zn}$</span>转化为： <span
class="math display">$$\begin{equation}
\begin{aligned}
&amp; \left\langle Z^n\right\rangle \propto \int \prod_\rho d m_\rho^1
\prod_{\rho, \sigma} d q_{\rho \sigma} d r_{\rho \sigma} \exp
\left[-\frac{N}{2} \alpha \beta^2 \sum_{\rho, \sigma} r_{\rho \sigma}
q_{\rho \sigma}\right] \\
&amp; \times \exp \left[-\frac{P-1}{2} \operatorname{Tr} \ln
[\mathbf{I}-\beta \mathbf{Q}]\right] \exp \left[-\frac{\beta N}{2}
\sum_\rho\left(m_\rho^1\right)^2+N\left\langle\ln \operatorname{Tr}
e^{\beta H_{\xi^1}}\right\rangle_{\xi^1}\right]
\end{aligned} \label{zn2}
\end{equation}$$</span></p>
<p>因为假定<span class="math inline"><em>N</em></span>足够大，使用 <a
href="https://wuli.wiki/changed/LapAsm.html#eq_LapAsm_1">Laplace’s
Method</a>：</p>
<p><span class="math display">$$
\int_a^b e^{N f(z)} d z \approx \sqrt{\frac{2 \pi}{-N f^{\prime
\prime}\left(z_0\right)}} e^{N f\left(z_0\right)}
$$</span></p>
<p>其中<span class="math inline"><em>Z</em><sub>0</sub></span>表示<span
class="math inline"><em>f</em>(<em>z</em>)</span>的极值点，这样就把积分转化为在极值点起作用的标量。采用近似
<span
class="math inline">⟨<em>Z</em><sup><em>n</em></sup>⟩ ∼ <em>e</em><sup><em>N</em><em>F</em>(<em>θ</em><sup>*</sup>)</sup></span>，其中用<span
class="math inline"><em>θ</em></span>表示序参量。</p>
<p><span
class="math display"><em>F</em>(<em>θ</em><sup>*</sup>) = max<sub><em>θ</em></sub><em>F</em>(<em>θ</em>)</span></p>
<p>淬火无序平均值变为： <span class="math display">$$\begin{align}
\langle\ln Z\rangle=\lim _{n \rightarrow 0} \frac{\ln \left\langle
Z^n\right\rangle}{n}=\lim _{n \rightarrow 0} \frac{\ln e^{N
F\left(\theta^*\right)}}{n}=N \lim _{n \rightarrow 0}
\frac{F\left(\theta^*\right)}{n} \label{847}
\end{align}$$</span> 当<span
class="math inline"><em>n</em></span>足够大时，近似有<span
class="math inline"><em>P</em> − 1 ≃ <em>P</em> = <em>α</em><em>N</em></span>：</p>
<p><span class="math display">$$\begin{align}
F\left(r_{\rho \sigma}, q_{\rho \sigma}, m_\rho^1\right)= &amp;
-\frac{\alpha \beta^2}{2} \sum_{\rho, \sigma} r_{\rho \sigma} q_{\rho
\sigma}-\frac{\alpha}{2} \operatorname{Tr} \ln [\mathbf{I}-\beta
\mathbf{Q}] -\frac{\beta}{2}
\sum_\rho\left(m_\rho^1\right)^2+\left\langle\ln \operatorname{Tr}
e^{\beta H_{\xi^1}}\right\rangle_{\xi^1} \label{freeenergy1}
\end{align}$$</span></p>
<h1 id="序参量分析">序参量分析</h1>
<p>通过以上讨论已经得到与相变相关的序参量<span
class="math inline">(<em>r</em><sub><em>ρ</em><em>σ</em></sub>, <em>q</em><sub><em>ρ</em><em>σ</em></sub>, <em>m</em><sub><em>ρ</em></sub><sup>1</sup>)</span>，即其满足的方程<span
class="math inline">$\eqref{freeenergy1}$</span>。在继续进行极值分析前，先回顾一下其中每一个变量的含义。<span
class="math inline"><em>α</em></span>是存储模式的比例；<span
class="math inline"><em>β</em></span>是逆温度；<span
class="math inline"><em>r</em></span>是在进行傅里叶变换的时候引入的变量；<span
class="math inline"><em>q</em></span>是在进行变量替换时候引入的变量；<span
class="math inline"><strong>Q</strong></span>是<span
class="math inline"><em>q</em></span>矩阵形式；<span
class="math inline"><em>m</em></span>是在进行高斯积分引入的变量；<span
class="math inline"><em>H</em><sub><em>ξ</em></sub></span>是将<span
class="math inline"><em>S</em></span>重新组合之后的表达形式。</p>
<p>接下来计算<span
class="math inline"><em>F</em>(<em>r</em><sub><em>ρ</em><em>σ</em></sub>, <em>q</em><sub><em>ρ</em><em>σ</em></sub>, <em>m</em><sub><em>ρ</em></sub><sup>1</sup>)</span>的极值，首先处理<span
class="math inline"><em>q</em><sub><em>ρ</em><em>σ</em></sub></span>，结合<span
class="math inline">$\eqref{zn}$</span>：</p>
<p><span class="math display">$$\begin{align}
\frac{\partial F}{\partial q_{\rho \sigma}}=0 \Rightarrow
\frac{\partial}{\partial q_{\rho \sigma}}\left[-\frac{N \alpha
\beta^2}{2} \sum_{\rho, \sigma} r_{\rho \sigma} q_{\rho
\sigma}+\frac{\beta}{2}(\sqrt{\beta N})^2 \sum_{\mu \geq 2} \sum_{\rho,
\sigma} m_\rho^\mu q_{\rho \sigma} m_\sigma^\mu\right]=0 \text {, }
\end{align}$$</span></p>
<p>得到解为： <span class="math display">$$\begin{align}
r_{\rho \sigma}=\frac{1}{\alpha} \sum_{\mu \geq 2} m_\rho^\mu
m_\sigma^\mu
\end{align}$$</span></p>
<p>从表达式中可以看出，之前进行标度的变换，目的是为了使得最终的共轭关系变得更直观。</p>
<p>然后处理<span
class="math inline"><em>m</em><sub><em>ρ</em></sub><sup><em>μ</em></sup></span>，结合最开始的<span
class="math inline">$\eqref{znstart}$</span>：</p>
<p><span class="math display">$$\begin{align}
\frac{\partial F}{\partial m_\rho^\mu}=0 \Rightarrow
\frac{\partial}{\partial m_\rho^\mu}\left[-\frac{\beta
N}{2}\left(m_\rho^\mu\right)^2+\beta m_\rho^\mu \sum_i \xi_i^\mu
S_i^\rho\right]=0
\end{align}$$</span></p>
<p>可以得到： <span class="math display">$$\begin{align}
m_\rho^\mu=\frac{1}{N} \sum_i \xi_i^\mu S_i^\rho
\end{align}$$</span></p>
<p>可以看出<span
class="math inline"><em>m</em><sub><em>ρ</em></sub><sup><em>μ</em></sup></span>在处理不同的模式之间影响的作用，也称作交叠项。</p>
<p>最后处理<span
class="math inline"><em>r</em><sub><em>ρ</em><em>σ</em></sub></span>，结合<span
class="math inline">$\eqref{zn2}$</span>： <span
class="math display">$$\begin{align}
\frac{\partial F}{\partial r_{\rho \sigma}}=0 \Rightarrow
\frac{\partial}{\partial r_{\rho \sigma}}\left[-\frac{N \alpha
\beta^2}{2} \sum_{\rho, \sigma} r_{\rho \sigma} q_{\rho
\sigma}+\frac{\alpha \beta^2}{2} \sum_{i, \rho, \sigma} r_{\rho \sigma}
S_i^\rho S_i^\sigma\right]=0,
\end{align}$$</span></p>
<p>得到序参量： <span class="math display">$$\begin{align}
q_{\rho \sigma}=\frac{1}{N} \sum_i S_i^\rho S_i^\sigma
\end{align}$$</span></p>
<p><span
class="math inline"><em>q</em><sub><em>ρ</em><em>σ</em></sub></span>通常理解为两个纯态之间的相互重叠。如果一个单一状态主导了相空间，爱德华兹-安德森序参量就用来表征该状态的大小。</p>
<h1 id="append">Append</h1>
<h2 id="行列式与迹的关系证明">行列式与迹的关系证明</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">M = &#123;&#123;a, b&#125;, &#123;c, d&#125;&#125;;</span><br><span class="line">Sum[Det[MatrixPower[M, k]]/(k!), &#123;k, 0, Infinity&#125;] // FullSimplify</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Replica Method</tag>
        <tag>Hopfield Model</tag>
      </tags>
  </entry>
  <entry>
    <title>Hopfield Model</title>
    <url>/2022/11/14/Phys/Hopfield/Hopfield2/</url>
    <content><![CDATA[<p><span class="math display">$$\begin{align}
F\left(r_{\rho \sigma}, q_{\rho \sigma}, m_\rho^1\right)= &amp;
-\frac{\alpha \beta^2}{2} \sum_{\rho, \sigma} r_{\rho \sigma} q_{\rho
\sigma}-\frac{\alpha}{2} \operatorname{Tr} \ln [\mathbf{I}-\beta
\mathbf{Q}] -\frac{\beta}{2}
\sum_\rho\left(m_\rho^1\right)^2+\left\langle\ln \operatorname{Tr}
e^{\beta H_{\xi^1}}\right\rangle_{\xi^1} \label{freeenergy1}
\end{align}$$</span></p>
<p><span class="math display">$$\begin{align}
r_{\rho \sigma}&amp;=\frac{1}{\alpha} \sum_{\mu \geq 2} m_\rho^\mu
m_\sigma^\mu \\
m_\rho^\mu&amp;=\frac{1}{N} \sum_i \xi_i^\mu S_i^\rho \\
q_{\rho \sigma}&amp;=\frac{1}{N} \sum_i S_i^\rho S_i^\sigma
\end{align}$$</span></p>
<p><span class="math display">$$\begin{align}
\langle\ln Z\rangle=\lim _{n \rightarrow 0} \frac{\ln \left\langle
Z^n\right\rangle}{n}=\lim _{n \rightarrow 0} \frac{\ln e^{N
F\left(\theta^*\right)}}{n}=N \lim _{n \rightarrow 0}
\frac{F\left(\theta^*\right)}{n} \label{847}
\end{align}$$</span></p>
<h1 id="replica-symmetric-ansätz">Replica-Symmetric Ansätz</h1>
<p>讨论到以上的情形，为了继续分析，需要对重叠矩阵做一个近似（考虑最简单的形式）：任意两个纯态应该是对称的。这被称为副本对称（RS）假设。</p>
<p><span class="math display">$$\begin{align}
\left\{\begin{array}{l}
r_{\rho \sigma}=r, \forall \rho, \sigma \\
m_\rho^1=m, \forall \rho \\
q_{\rho \sigma}=q, \forall \rho \neq \sigma
\end{array} \right.
\end{align}$$</span></p>
<p>将<span class="math inline">$\eqref{freeenergy1}$</span>改写为：
<span class="math display">$$
\begin{aligned}
F(r, q, m)= &amp; -\frac{\alpha \beta^2}{2} r
q\left(n^2-n\right)-\frac{\alpha \beta^2}{2} n r-\frac{\alpha}{2}
\operatorname{Tr} \ln [\mathbf{I}-\beta \mathbf{Q}] \\
&amp; -\frac{\beta}{2} n m^2+\left\langle\ln \operatorname{Tr} e^{\beta
H_{\xi}}\right\rangle
\end{aligned}
$$</span></p>
<p>结合<span class="math inline">$\eqref{847}$</span>: <span
class="math display">$$\begin{align}
\langle\ln Z\rangle= &amp; \frac{N \alpha \beta^2 r q}{2}-\frac{N \alpha
\beta^2 r}{2}-\frac{\alpha N}{2} \lim _{n \rightarrow 0}
\frac{\operatorname{Tr} \ln [\mathbf{I}-\beta
\mathbf{Q}]}{n}-\frac{\beta N m^2}{2}+N \lim _{n \rightarrow 0}
\frac{\left\langle\ln \operatorname{Tr} e^{\beta
H_{\xi^1}}\right\rangle}{n} \label{zn3}\\
\beta H_{\xi^1}=&amp;\beta m \xi^1 \sum_\rho S^\rho+\frac{1}{2} \alpha
\beta^2 r \sum_{\rho, \sigma} S^\rho S^\sigma
\end{align}$$</span></p>
<p>首先计算<span class="math inline">$\eqref{zn3}$</span>中最后一项：
<span class="math display">$$\begin{align}
\operatorname{Tr} e^{\beta H_{\xi} 1} &amp; =\operatorname{Tr} e^{\beta
m \xi^1 \Sigma_\rho S^\rho+\frac{1}{2} \alpha \beta^2 r\left(\sum_\rho
S^\rho\right)^2} \\
&amp; :=\operatorname{Tr} e^{A\left(\Sigma_\rho S^\rho\right)^2+B
\Sigma_\rho S^\rho} \\
&amp; =\operatorname{Tr} \sqrt{\frac{A}{\pi}} \int d z e^{-A z^2+2 A z
\sum_\rho S^\rho+B \Sigma_\rho S^\rho} \\
&amp; =\sqrt{\frac{A}{\pi}} \int d z e^{-A z^2} \operatorname{Tr}
\prod_\rho e^{(2 A z+B) S^\rho} \\
&amp; =\sqrt{\frac{\alpha \beta^2 r}{2 \pi}} \int d z e^{-\frac{1}{2}
\alpha \beta^2 r z^2}\left[2 \cosh \left(\alpha \beta^2 r z+\beta m
\xi^1\right)\right]^n \\
&amp; =\sqrt{\frac{\alpha \beta^2 r}{2 \pi}} \int d z e^{-\frac{1}{2}
\alpha \beta^2 r z^2+n \ln \left[2 \cosh \left(\alpha \beta^2 r z+\beta
m \xi^1\right)\right]} \\
&amp; =\sqrt{\frac{1}{2 \pi}} \int d z e^{-\frac{1}{2} z^2+n \ln \left[2
\cosh \left(\beta \sqrt{\alpha r} z+\beta m \xi^1\right)\right]} .
\end{align}$$</span></p>
<p>并且可知其极限为： <span class="math display">$$\begin{align}
\lim _{n \rightarrow 0} \operatorname{Tr} e^{\beta H_{\xi}
1}=\sqrt{\frac{1}{2 \pi}} \int d z e^{-\frac{1}{2} z^2}=1
\end{align}$$</span></p>
<p>由此可获得最后一项为： <span class="math display">$$\begin{align}
&amp; \lim _{n \rightarrow 0} \frac{\left\langle\ln \operatorname{Tr}
e^{\beta H_{\xi}}\right\rangle}{n} \\
&amp; =\left\langle\lim _{n \rightarrow 0} \frac{\frac{d}{d n}
\operatorname{Tr} e^{\beta H_{\xi} 1}}{\operatorname{Tr} e^{\beta
H_{\xi} 1}}\right\rangle \\
&amp; =\left\langle\sqrt{\frac{1}{2 \pi}} \lim _{n \rightarrow 0}
\frac{d}{d n} \int d z e^{-\frac{1}{2} z^2+n \ln \left[2 \cosh
\left(\beta \sqrt{a r} z+\beta m \xi^1\right)\right]}\right\rangle \\
&amp; =\left\langle\sqrt{\frac{1}{2 \pi}} \lim _{n \rightarrow 0} \int d
z e^{-\frac{1}{2} z^2} \frac{d}{d n}\left[2 \cosh \left(\beta
\sqrt{\alpha r} z+\beta m \xi^1\right)\right]^n\right\rangle \\
&amp; =\left\langle\sqrt{\frac{1}{2 \pi}} \lim _{n \rightarrow 0} \int d
z e^{-\frac{1}{2} z^2}\left[2 \cosh \left(\beta \sqrt{\alpha r} z+\beta
m \xi^1\right)\right]^n \ln \left[2 \cosh \left(\beta \sqrt{\alpha r}
z+\beta m \xi^1\right)\right]\right\rangle \\
&amp; =\left\langle\sqrt{\frac{1}{2 \pi}} \int d z e^{-\frac{1}{2} z^2}
\lim _{n \rightarrow 0}\left[2 \cosh \left(\beta \sqrt{\alpha r} z+\beta
m \xi^1\right)\right]^n \ln \left[2 \cosh \left(\beta \sqrt{\alpha r}
z+\beta m \xi^1\right)\right]\right\rangle \\
&amp; =\left\langle\sqrt{\frac{1}{2 \pi}} \int d z e^{-\frac{1}{2} z^2}
\ln \left[2 \cosh \left(\beta \sqrt{\alpha r} z+\beta m
\xi^1\right)\right]\right\rangle \\
&amp; =\int D z\left\langle\ln \left[2 \cosh \left(\beta \sqrt{\alpha r}
z+\beta m \xi^1\right)\right]\right\rangle
\end{align}$$</span></p>
<p><font color='blue'>此时用<span
class="math inline"><em>D</em><em>z</em></span>表示对<span
class="math inline"><em>z</em></span>的高斯积分。</font></p>
<p>然后计算<span
class="math inline">$\eqref{zn3}$</span>中第三项。由于<span
class="math inline"><strong>Q</strong></span>是对称矩阵，将其对角化：
<span class="math display">$$\begin{align}
\mathbf{A Q A}^{-1}=\Lambda=\operatorname{diag}\left(\lambda_1,
\lambda_2, \ldots, \lambda_n\right)
\end{align}$$</span></p>
<p>将<span
class="math inline">ln [<strong>I</strong> − <em>β</em><strong>Q</strong>]</span>进行指数展开<span
class="math inline">$\ln (1-x)=-\sum_{n=1}^{\infty}
\frac{x^n}{n}$</span>，得到： <span class="math display">$$\begin{align}
\operatorname{Tr} \ln [\mathbf{I}-\beta \mathbf{Q}] &amp;
=\operatorname{Tr}\left\{\mathbf{A} \cdot \ln [\mathbf{I}-\beta
\mathbf{Q}] \cdot \mathbf{A}^{-1}\right\} \\
&amp; =-\operatorname{Tr}\left\{\sum_{l=1}^{\infty}
\frac{\beta^l\left(\mathbf{A Q A}^{-1}\right)^l}{l}\right\} \\
&amp; =-\operatorname{Tr}\left\{\sum_{l=1}^{\infty}
\frac{\beta^l(\Lambda)^l}{l}\right\} \\
&amp; =-\sum_{l=1}^{\infty} \frac{\beta^l}{l} \sum_{i=1}^n
\lambda_i^l=\sum_{i=1}^n \ln \left[1-\beta \lambda_i\right]
\end{align}$$</span></p>
<p>再结合矩阵恒等式<span
class="math inline">Tr ln <strong>K</strong> = ln det <strong>K</strong></span>，可以计算<span
class="math inline"><strong>Q</strong></span>的本征值： <span
class="math display">$$\begin{align}
&amp; \left|\begin{array}{cccc}
1-\lambda &amp; q &amp; \cdots &amp; q \\
q &amp; 1-\lambda &amp; \cdots &amp; q \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
q &amp; q &amp; \cdots &amp; 1-\lambda
\end{array}\right| \\
&amp; =\left|\begin{array}{cccc}
1-\lambda+(n-1) q &amp; 1-\lambda+(n-1) q &amp; \cdots &amp;
1-\lambda+(n-1) q \\
q &amp; 1-\lambda &amp; \cdots &amp; q \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
q &amp; q &amp; \cdots &amp; 1-\lambda
\end{array}\right| \\
&amp; =[1-\lambda+(n-1) q]\left|\begin{array}{cccc}
1 &amp; 1 &amp; \cdots &amp; 1 \\
q &amp; 1-\lambda &amp; \cdots &amp; q \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
q &amp; q &amp; \cdots &amp; 1-\lambda
\end{array}\right| \\
&amp; =[1-\lambda+(n-1) q]\left|\begin{array}{cccc}
1 &amp; 1 &amp; \cdots &amp; 1 \\
0 &amp; 1-\lambda-q &amp; 0 \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots 1-\lambda-q
\end{array}\right| \\
&amp; =[1-\lambda+(n-1) q](1-q-\lambda)^{n-1}=0
\end{align}$$</span></p>
<p>可以得到<span class="math inline">1</span>个本征值<span
class="math inline">(1 + (<em>n</em> − 1)<em>q</em>)</span>与<span
class="math inline"><em>n</em> − 1</span>个本征值<span
class="math inline">(1 − <em>q</em>)</span>，可以将迹写为： <span
class="math display">$$\begin{align}
\operatorname{Tr} \ln [\mathbf{I}-\beta \mathbf{Q}]=\ln (1-\beta+\beta
q-n \beta q)+(n-1) \ln (1-\beta+\beta q)
\end{align}$$</span> 从而;</p>
<p><span class="math display">$$\begin{align}
\lim _{n \rightarrow 0} \frac{\operatorname{Tr} \ln [\mathbf{I}-\beta
\mathbf{Q}]}{n} &amp; =\lim _{n \rightarrow 0}\left[\frac{\ln
\left(\frac{1-\beta+\beta q-n \beta q}{1-\beta+\beta q}\right)}{n}+\ln
(1-\beta+\beta q)\right] \\
&amp; =-\frac{\beta q}{1-\beta+\beta q}+\ln (1-\beta+\beta q)
\end{align}$$</span></p>
<p>得到自由能为：</p>
<p><span class="math display">$$\begin{align}
-\beta f &amp; =\frac{1}{N}\langle\ln Z\rangle \\
&amp; =\frac{\alpha \beta^2}{2} r(q-1)-\frac{\alpha}{2}\left[\ln
(1-\beta+\beta q)-\frac{\beta q}{1-\beta+\beta q}\right]-\frac{\beta}{2}
m^2 +\int D z\left\langle\ln \left[2 \cosh \left(\beta \sqrt{\alpha r}
z+\beta m \xi^1\right)\right]\right\rangle
\end{align}$$</span></p>
<p>求解其极值： <span class="math display">$$\begin{align}
\left\{\begin{array}{l}
\frac{\partial(-\beta f)}{\partial r}=0 \\
\frac{\partial(-\beta f)}{\partial m}=0 \\
\frac{\partial(-\beta f)}{\partial q}=0
\end{array}\right.
\end{align}$$</span></p>
<p><span class="math display">$$\begin{align}
q= &amp; -\frac{1}{\beta \sqrt{2 \pi \alpha r}} \int d z e^{-\frac{1}{2}
z^2} z\left\langle\tanh \left(\beta \sqrt{\alpha r} z+\beta m
\xi^1\right)\right\rangle+1 \\
= &amp; \frac{1}{\beta \sqrt{2 \pi \alpha r}} \int d z \frac{d
e^{-\frac{1}{2} z^2}}{d z}\left\langle\tanh \left(\beta \sqrt{\alpha r}
z+\beta m \xi^1\right)\right\rangle+1 \\
= &amp; \left.\frac{1}{\beta \sqrt{2 \pi \alpha r}} e^{-\frac{1}{2}
z^2}\left\langle\tanh \left(\beta \sqrt{\alpha r} z+\beta m
\xi^1\right)\right\rangle\right|_{-\infty} ^{+\infty} -\int D
z\left\langle 1-\tanh ^2\left(\beta \sqrt{\alpha r} z+\beta m
\xi^1\right)\right\rangle+1 \\
= &amp; \int D z\left\langle\tanh ^2\left(\beta \sqrt{\alpha r} z+\beta
m \xi^1\right)\right\rangle \\
= &amp; \int D z \tanh ^2 \beta(\sqrt{\alpha r} z+m)
\end{align}$$</span></p>
<p>可以得到联想记忆模型的鞍点方程： <span
class="math display">$$\begin{align}
&amp; q=\int D z \tanh ^2 \beta(\sqrt{\alpha r} z+m) \label{860} \\
&amp; m=\int D z\langle\xi \tanh \beta(\sqrt{\alpha r} z+m
\xi)\rangle=\int D z \tanh \beta(\sqrt{\alpha r} z+m) \label{861} \\
&amp; r=\frac{q}{(1-\beta+\beta q)^2} \label{862}\\
\end{align}$$</span></p>
<p>可以从以上的内容分析相变点。</p>
<h1 id="zero-temperature-limit">Zero-Temperature Limit</h1>
<p>当<span
class="math inline"><em>T</em> → 0(<em>β</em> → ∞)</span>时候，有：</p>
<p><span class="math display">$$\tanh (\beta x) \rightarrow
\operatorname{sign}(x)=\left\{ \begin{array}{ll}
1 &amp; x&gt;0 \\
0 &amp; x=0 \\
-1 &amp; x&lt;0
\end{array} \right.$$</span></p>
<p><span class="math inline">$\eqref{861}$</span>为：</p>
<p><span class="math display">$$\begin{align}
m&amp;=\int D z\operatorname{sign}(\sqrt{\alpha r} z+m)+O(T) \\
&amp; =\operatorname{erf}\left( \frac{m}{\sqrt{2 \alpha r}} \right)+O(T)
\end{align}$$</span></p>
<p><font color='blue'>上面这个变换利用正态分布与误差函数之间的关系完成。</font>
另一方面，当<span class="math inline"><em>β</em> → ∞</span>：</p>
<p><span class="math display">$$\begin{align}
1-q &amp; =\int \frac{d z}{\sqrt{2 \pi}} e^{-\frac{z^2}{2}}\left(1-\tanh
^2 \beta(\sqrt{\alpha r} z+m)\right) \\
&amp; \left.\simeq \frac{1}{\sqrt{2 \pi}}
e^{-\frac{z^2}{2}}\right|_{\tanh ^2 \beta(\sqrt{\alpha r} z+m)=0} \int d
z\left(1-\tanh ^2 \beta(\sqrt{\alpha r} z+m)\right) \\
&amp; =\frac{1}{\sqrt{2 \pi}} e^{-\frac{m^2}{2 \alpha r}} \frac{1}{\beta
\sqrt{\alpha r}} \int d z \frac{\partial}{\partial z} \tanh
\beta(\sqrt{\alpha r} z+m) \\
&amp; =\frac{2}{\sqrt{2 \pi}} \frac{1}{\beta \sqrt{\alpha r}}
e^{-\frac{m^2}{2 \alpha r}}
\end{align}$$</span></p>
<p><span class="math inline">$\eqref{860}$</span>产生<span
class="math inline"><em>q</em> = 1 − <em>C</em><em>T</em></span>，其中</p>
<p><span class="math display">$$\begin{align}
C \stackrel{\text { def }}{=} \sqrt{\frac{2}{\pi r \alpha}}
e^{-\frac{m^2}{2 \alpha r}}
\end{align}$$</span> 将<span
class="math inline">$\eqref{862}$</span>变为<span
class="math inline"><em>r</em> = (1 − <em>C</em>)<sup>−2</sup></span>。</p>
<p>通过定义辅助变量<span class="math inline">$y=m / \sqrt{2 \alpha
r}$</span>将<span class="math inline"><em>m</em></span> 和 <span
class="math inline"><em>r</em></span> 减少为一个方程： <span
class="math display">$$\begin{align}
\operatorname{erf}(y)=y\left(\sqrt{2 \alpha}+\frac{2}{\sqrt{\pi}}
e^{-y^2}\right) .
\end{align}$$</span></p>
<figure>
<img src="./plot.gif" alt="equation" />
<figcaption aria-hidden="true">equation</figcaption>
</figure>
<p>其中的一个恒定解为<span
class="math inline"><em>y</em> = <em>m</em> = 0</span>。对于<span
class="math inline"><em>α</em> ≥ <em>α</em><sub><em>c</em></sub> = 0.138</span>只有为<span
class="math inline">0</span>的唯一解；当<span
class="math inline"><em>a</em> &lt; <em>α</em><sub><em>c</em></sub></span>时,<span
class="math inline"><em>m</em> ≠ 0</span>的解出现；当<span
class="math inline"><em>α</em> = <em>α</em><sub><em>c</em></sub></span>时，<span
class="math inline"><em>m</em> = 0.967</span>。</p>
<figure>
<img src="./error_probability.png" alt="error probability" />
<figcaption aria-hidden="true">error probability</figcaption>
</figure>
<p>通过求解<span
class="math inline"><em>m</em> = erf (<em>y</em>)</span>可以得到<span
class="math inline"><em>m</em></span>的值。图中纵坐标表示误差<span
class="math inline">$P_{\text{error}}=\frac{1-m}{2}$</span>，横坐标表述存储的比例。可以发现，在图中存在一个跃变的点，当<span
class="math inline"><em>α</em>——<em>c</em> = 0.138</span>的时候，误差跳至<span
class="math inline">0.5</span>，这是一个不连续的转变，此时代表跳转至玻璃相；当<span
class="math inline"><em>α</em> &lt; <em>α</em><sub><em>c</em></sub></span>时，误差很小，表示此时网络可以从之前学习的模式中“恢复”；当<span
class="math inline"><em>α</em> &gt; <em>α</em><sub><em>c</em></sub></span>时，误差为<span
class="math inline">0.5</span>，为瞎猜的几率，可以认为此时网络不能恢复之前学习的模式。</p>
<a href="/2022/11/14/Phys/Hopfield/Hopfield2/" title="Hopfield Model">Hopfield 相图</a>
<h1 id="append">Append</h1>
<h2 id="高斯积分和误差函数之间的关系">高斯积分和误差函数之间的关系</h2>
<p>高斯函数（Gaussian
function）与误差函数（erf）之间的关系主要通过正态分布的累积分布函数（CDF）来体现。为了详细说明这一点，我们从标准正态分布及其累积分布函数出发。</p>
<h3 id="标准正态分布">标准正态分布</h3>
<p>标准正态分布的概率密度函数（PDF）定义为：</p>
<p><span class="math display">$$f(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2 /
2}$$</span></p>
<h3 id="标准正态分布的累积分布函数">标准正态分布的累积分布函数</h3>
<p>累积分布函数（CDF）是从负无穷到某个值 <span
class="math inline"><em>x</em></span>
的概率密度函数的积分。对于标准正态分布，CDF 通常记作 <span
class="math inline"><em>Φ</em>(<em>x</em>)</span>：</p>
<p><span class="math display">$$\Phi(x) = \int_{-\infty}^x
\frac{1}{\sqrt{2\pi}} e^{-t^2 / 2} \, dt$$</span></p>
<h3 id="误差函数的定义">误差函数的定义</h3>
<p>误差函数（erf）定义为：</p>
<p><span class="math display">$$\text{erf}(x) = \frac{2}{\sqrt{\pi}}
\int_0^x e^{-t^2} \, dt$$</span></p>
<h3 id="高斯函数与误差函数的关系">高斯函数与误差函数的关系</h3>
<p>我们可以通过一些变换将标准正态分布的 CDF
表示为误差函数。首先，考虑累积分布函数从负无穷大积分到某个值 <span
class="math inline"><em>x</em></span>：</p>
<p><span class="math display">$$\Phi(x) = \int_{-\infty}^x
\frac{1}{\sqrt{2\pi}} e^{-t^2 / 2} \, dt$$</span></p>
<p>通过代换 <span class="math inline">$u =
\frac{t}{\sqrt{2}}$</span>，我们得到：</p>
<p><span class="math display">$$du = \frac{dt}{\sqrt{2}} \quad
\Rightarrow \quad dt = \sqrt{2} \, du$$</span></p>
<p>因此，积分变为：</p>
<p><span class="math display">$$\Phi(x) = \int_{-\infty}^{x}
\frac{1}{\sqrt{2\pi}} e^{-t^2 / 2} \, dt = \int_{-\infty}^{x/\sqrt{2}}
\frac{1}{\sqrt{2\pi}} e^{-2u^2 / 2} \cdot \sqrt{2} \, du$$</span></p>
<p>简化后，我们得到：</p>
<p><span class="math display">$$\Phi(x) = \frac{1}{2} \left(1 +
\text{erf}\left(\frac{x}{\sqrt{2}}\right)\right)$$</span></p>
<p>因此，标准正态分布的累积分布函数与误差函数之间的关系为：</p>
<p><span class="math display">$$\Phi(x) = \frac{1}{2} \left(1 +
\text{erf}\left(\frac{x}{\sqrt{2}}\right)\right)$$</span></p>
<h3 id="逆误差函数">逆误差函数</h3>
<p>误差函数的反函数（逆误差函数）是一个重要工具，用于将累积分布函数反转：</p>
<p><span class="math display">$$x = \sqrt{2} \, \text{erf}^{-1}(2\Phi(x)
- 1)$$</span></p>
<h3 id="示例">示例</h3>
<p>假设我们想计算标准正态分布 <span class="math inline">𝒩(0, 1)</span>
的某个值 <span class="math inline"><em>x</em></span>
的累积分布函数。使用 Python，我们可以如下计算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.special <span class="keyword">as</span> sp</span><br><span class="line"></span><br><span class="line">x = <span class="number">1.0</span></span><br><span class="line">Phi_x = <span class="number">0.5</span> * (<span class="number">1</span> + sp.erf(x / np.sqrt(<span class="number">2</span>)))</span><br><span class="line"><span class="built_in">print</span>(Phi_x)</span><br></pre></td></tr></table></figure>
<p>这个示例计算了 <span class="math inline"><em>x</em> = 1</span>
时的累积分布函数值。</p>
<h3 id="总结">总结</h3>
<p>高斯函数（正态分布）与误差函数之间的关系通过正态分布的累积分布函数（CDF）体现。正态分布的累积分布函数可以表示为误差函数的形式：</p>
<p><span class="math display">$$\Phi(x) = \frac{1}{2} \left(1 +
\text{erf}\left(\frac{x}{\sqrt{2}}\right)\right)$$</span></p>
<p>这使得误差函数成为研究正态分布及其相关问题的一个重要工具。</p>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Replica Method</tag>
        <tag>Hopfield Model</tag>
      </tags>
  </entry>
  <entry>
    <title>Hopfield Model 相图</title>
    <url>/2022/11/15/Phys/Hopfield/Hopfield3/</url>
    <content><![CDATA[<h1 id="phase-diagram">Phase Diagram</h1>
<p>根据<a href="/2022/11/13/Phys/Hopfield/Hopfield1/" title="Hopfield Model 自由能">“Hopfield 自由能”</a>中的讨论，可知：</p>
<p><span class="math display">$$\begin{align}
&amp; q=\int D z \tanh ^2 \beta(\sqrt{\alpha r} z+m) \label{860} \\
&amp; m=\int D z\langle\xi \tanh \beta(\sqrt{\alpha r} z+m
\xi)\rangle=\int D z \tanh \beta(\sqrt{\alpha r} z+m) \label{861} \\
&amp; r=\frac{q}{(1-\beta+\beta q)^2} \label{862}\\
\end{align}$$</span></p>
<p>通过数值求解 <span
class="math inline">$\eqref{860},\eqref{861},\eqref{862}$</span>
可以得到Hopfield的相图：</p>
<figure>
<img src="./diagram.png" alt="diagram" />
<figcaption aria-hidden="true">diagram</figcaption>
</figure>
<p>在高温的情况下，热噪音阻碍了模式恢复的过程，因此<span
class="math inline"><em>m</em> = 0, <em>q</em> = 0, <em>r</em> = 0</span>；当温度下降，参数开始在临界线<span
class="math inline"><em>T</em><sub><em>g</em></sub></span>不稳定，可以通过<span
class="math inline">$\eqref{860}$</span>分析得到；随着温度继续下降，Spin
Glass相变成亚稳定态，相变线<span
class="math inline"><em>T</em><sub><em>M</em></sub></span>，恢复的模式是局域稳定的，这个相变是一阶相变。当记忆率下降，转变为稳定的态，对应相变线为<span
class="math inline"><em>T</em><sub><em>c</em></sub></span>。</p>
<p>在温度<span
class="math inline"><em>T</em> = 0</span>的时候，每个自旋的平均熵值为<span
class="math inline">$S=-\left.\frac{\partial f}{\partial T}\right|_{T
\rightarrow 0}=-\frac{1}{2} \alpha[\ln (1-C)+C
/(1-C)]$</span>，其中<span
class="math inline"><em>C</em> = <em>β</em>(1 − <em>q</em>)</span>，可知自旋平均值是负数，这是非物理的。</p>
<h1 id="hopfield-model-with-arbitrary-hebbian-length">Hopfield Model
with Arbitrary Hebbian Length</h1>
<p>Hebbian learning
是一种学习算法和理论，用于解释神经网络中的突触可塑性。其核心思想可以用一句话概括，即“同步触发的神经元会连线在一起”（“Cells
that fire together, wire together”）。这个概念最早由加拿大心理学家Donald
Hebb在1949年提出，故称为Hebbian学习。</p>
<p>具体来说，Hebbian学习的基本原则如下：</p>
<ol type="1">
<li><p><strong>联结权重的更新</strong>：如果一个神经元A经常且重复地激活神经元B，那么神经元A和神经元B之间的突触连接会变得更强。这意味着两个神经元之间的联结权重会增加。</p></li>
<li><p><strong>时间一致性</strong>：为了使联结权重增加，神经元A的发火必须在神经元B的发火之前或同时发生。这种时间上的一致性被认为是形成记忆和学习的基础。</p></li>
<li><p><strong>局部性</strong>：Hebbian学习是一种局部学习规则，即每个突触的权重更新只依赖于连接的两个神经元的活动，而不依赖于整个网络的状态。</p></li>
</ol>
<p>在数学上，Hebbian学习规则可以表示为：</p>
<p><span class="math display">$$\begin{align}
\Delta w_{ij} = \eta x_i y_j
\end{align}$$</span></p>
<p>其中，<span
class="math inline"><em>Δ</em><em>w</em><sub><em>i</em><em>j</em></sub></span>
表示神经元 <span class="math inline"><em>i</em></span> 和神经元 <span
class="math inline"><em>j</em></span> 之间的突触权重变化，<span
class="math inline"><em>η</em></span> 是学习率，<span
class="math inline"><em>x</em><sub><em>i</em></sub></span> 和 <span
class="math inline"><em>y</em><sub><em>j</em></sub></span> 分别是神经元
<span class="math inline"><em>i</em></span> 和神经元 <span
class="math inline"><em>j</em></span> 的激活状态。</p>
<p>Hebbian学习在神经科学和人工神经网络领域都有重要影响，特别是在理解神经网络如何通过经验和环境进行学习和调整方面。</p>
<p>Hebbian strength
指的是神经元之间突触连接的强度，定义如下的神经元权重：</p>
<p><span class="math display">$$\begin{align}
J_{i j}=\frac{1}{N} \sum_{\mu=1}^P\left[c \xi_i^\mu \xi_j^\mu+\gamma
\sum_{r=1}^d\left(\xi_i^\mu \xi_j^{\mu+r}+\xi_i^{\mu+r}
\xi_j^\mu\right)\right]
\end{align}$$</span></p>
<p>其中<span class="math inline"><em>c</em></span>是标准Hebbian
strength；<span
class="math inline"><em>γ</em></span>是不同记忆模式之间的强度；<span
class="math inline"><em>d</em></span>是模型的Hebbian
长度，之前讨论的是<span
class="math inline"><em>d</em> = 1</span>的情形，只记忆其中一个模式，<span
class="math inline"><em>d</em> = 0</span>是标准的Hopfield Model。<span
class="math inline"><em>ξ</em><sub><em>i</em></sub><sup><em>μ</em></sup></span>是二值变量，例如<span
class="math inline">$p\left(\xi_i^\mu= \pm 1\right)=\frac{1}{2}
\delta\left(\xi_i^\mu+1\right)+\frac{1}{2}
\delta\left(\xi_i^\mu-1\right)$</span>。讨论的是<span
class="math inline"><em>P</em></span>与<span
class="math inline"><em>N</em></span>的极限情形，<span
class="math inline">$\alpha=\frac{P}{N}$</span>是memory load。</p>
<h2 id="computation-of-the-disorder-averaged-free-energy">Computation of
the Disorder-Averaged Free Energy</h2>
<p>将<span class="math inline"><strong>J</strong></span>重新写为：</p>
<p><span class="math display">$$\begin{align}
\mathbf{J}&amp;=\frac{1}{N} \xi^{\mathrm{T}} \mathbf{X} \xi \\
X_{\mu \eta} &amp; =c \delta_{\mu \eta}+\gamma
\sum_{r=1}^d\left(\delta_{\mu,(\eta+r) \bmod P}+\delta_{\mu,(\eta-r)
\bmod P}\right) \\
&amp; =(c-\gamma) \delta_{\mu \eta}+\gamma \sum_{r=-d}^d
\delta_{\mu,(\eta+r) \bmod P}
\end{align}$$</span></p>
<p><span class="math inline"><strong>X</strong></span>的第<span
class="math inline"><em>m</em></span>个本征值为，<a
href="https://ieeexplore.ieee.org/document/8187426">解的思路</a>： <span
class="math display">$$\begin{align}
\lambda_m&amp;=\sum_{k=0}^{P-1} X_{1(k+1)} e^{-2 \pi i m k / P} \\
&amp; =\sum_{k=0}^{P-1} X_{1(k+1)} \cos \left(2 \pi \frac{m k}{P}\right)
\\
&amp; =\sum_{k=0}^{P-1}\left[c \delta_{0 k}+\gamma
\sum_{r=1}^d\left(\delta_{0,(k+r) \bmod P}+\delta_{0,(k-r) \bmod
P)}\right] \cos \left(2 \pi \frac{m k}{P}\right)\right. \\
&amp; =c+\gamma \sum_{r=1}^d\left[\cos \left(-2 \pi \frac{m
r}{P}\right)+\cos \left(2 \pi \frac{m r}{P}\right)\right] \\
&amp; =c+2 \gamma \sum_{r=1}^d \cos \left(2 \pi \frac{m r}{P}\right)
\end{align}$$</span> 其中<span
class="math inline"><em>m</em> = 0, 1, …, <em>P</em> − 1</span>。</p>
<p>Hamiltonian和配分函数为： <span class="math display">$$\begin{align}
\mathcal{H}(\mathbf{s})&amp;=-\frac{1}{2} \sum_{i \neq j} J_{i j} s_i
s_j \\
Z&amp;=\operatorname{Tr} \exp \left[\frac{\beta}{2 N}
\mathbf{s}^{\mathrm{T}} \boldsymbol{\xi}^{\mathrm{T}} \mathbf{X}
\boldsymbol{\xi} \mathbf{s}\right]
\end{align}$$</span></p>
<p>其中 <span class="math inline">Tr</span> 表示对分立 <span
class="math inline"><strong>s</strong></span> 的求和。使用复本技巧：
<span class="math display">$$\begin{align}
\langle\ln Z\rangle=\lim _{n \rightarrow 0} \frac{\ln \left\langle
Z^n\right\rangle}{n},
\end{align}$$</span> 其中<span class="math inline">⟨⋅⟩</span>表示对<span
class="math inline"><strong>ξ</strong></span>的求和。因此有： <span
class="math display">$$\begin{align}
Z^n=\operatorname{Tr} \exp \left[\frac{\beta}{2 N}
\sum_{a=1}^n\left(\mathbf{s}^a\right)^{\mathrm{T}}
\boldsymbol{\xi}^{\mathrm{T}} \mathbf{X} \boldsymbol{\xi}
\mathbf{s}^a\right] .
\end{align}$$</span></p>
<p>考虑<span
class="math inline"><em>S</em></span>个凝聚（foreground）模式<span
class="math inline"><em>P</em> − <em>S</em></span>个非凝聚（background）模式。<font color='red'>这里这两个模式的类别，表示什么含义？每次恢复的模式不应该只有一个么？</font>根据以上定义，可以将<span
class="math inline"><strong>X</strong></span>分为： <span
class="math display">$$\begin{align}
\mathbf{X}=\left[\begin{array}{ll}
\mathbf{X}_{F F} &amp; \mathbf{X}_{F B} \\
\mathbf{X}_{B F} &amp; \mathbf{X}_{B B}
\end{array}\right]
\end{align}$$</span> 其中 <span
class="math inline"><strong>X</strong><sub><em>F</em><em>F</em></sub> ∈ ℝ<sup><em>S</em> × <em>S</em></sup>, <strong>X</strong><sub><em>B</em><em>F</em></sub><sup>T</sup> = <strong>X</strong><sub><em>F</em><em>B</em></sub> ∈ ℝ<sup><em>S</em> × (<em>P</em> − <em>S</em>)</sup></span>
and <span
class="math inline"><strong>X</strong><sub><em>B</em><em>B</em></sub> ∈ ℝ<sup>(<em>P</em> − <em>S</em>) × (<em>P</em> − <em>S</em>)</sup></span>。</p>
<p>将<span
class="math inline"><strong>X</strong><sub><em>B</em><em>B</em></sub></span>
对角化 <span
class="math inline"><strong>X</strong><sub><em>B</em><em>B</em></sub><sup><em>μ</em><em>ν</em></sup> = ∑<sub><em>σ</em></sub><em>λ</em><sub><em>σ</em></sub><em>η</em><sub><em>μ</em></sub><sup><em>σ</em></sup><em>η</em><sub><em>ν</em></sub><sup><em>σ</em></sup></span>，其中
<span class="math inline"><em>λ</em><sub><em>σ</em></sub></span> 与
<span
class="math inline"><em>η</em><sub><em>μ</em></sub><sup><em>σ</em></sup></span>
是本征值与本征态。</p>
<p>使用Hubbard-Stratonovich transformation：<span
class="math inline">$\exp \left[\frac{1}{2} b^2\right]=$</span> <span
class="math inline">∫<em>D</em><em>x</em>exp [±<em>b</em><em>x</em>]</span>,
其中 <span class="math inline">$D x=\frac{1}{\sqrt{2 \pi}} \exp
\left(-\frac{x^2}{2}\right) d x$</span>，因此有： <span
class="math display">$$
\begin{align}
Z^n=&amp;\operatorname{Tr} \exp \left[ \frac{\beta}{2 N} \sum_{a, i, j,
\mu \in B, v \in B} s_i^a \xi_i^\mu X_{\mu \nu} \xi_j^v
s_j^a+\frac{\beta}{N} \sum_{a, i, j, \mu \in B, v \in F} s_i^a \xi_i^\mu
X_{\mu \nu} \xi_j^v s_j^a  +\frac{\beta}{2 N} \sum_{a, i, j, \mu \in F,
v \in F} s_i^a \xi_i^\mu X_{\mu \nu} \xi_j^v s_j^a\right] \\
=&amp;\operatorname{Tr} \exp  {\left[\frac{\beta}{2 N} \sum_{a, \sigma}
\lambda_\sigma\left(\sum_{i, \mu \in B} s_i^a \xi_i^\mu
\eta_\mu^\sigma\right)^2+\frac{\beta}{N} \sum_{a, i, j, \mu \in B, v \in
F} s_i^a \xi_i^\mu X_{\mu \nu} \xi_j^v s_j^a +\frac{\beta}{2 N} \sum_{a,
i, j, \mu \in F, v \in F} s_i^a \xi_i^\mu X_{\mu \nu} \xi_j^\nu
s_j^a\right]} \\
= &amp; \operatorname{Tr} \prod_{a, \sigma} \int D x_\sigma^a \exp
\left[\sum_{i, \mu \in B} \frac{\xi_i^\mu}{\sqrt{N}}\left(\sum_{a,
\sigma} s_i^a \eta_\mu^\sigma \sqrt{\beta \lambda_\sigma}
x_\sigma^a+\frac{\beta}{\sqrt{N}} \sum_{a, j, v \in F} s_i^a X_{\mu \nu}
\xi_j^v s_j^a\right)+\frac{\beta}{2 N} \sum_{a, i, j, \mu \in F, v \in
F} s_i^a \xi_i^\mu X_{\mu \nu} \xi_j^v s_j^a\right]
\end{align}
$$</span></p>
<p>定义： <span class="math display">$$\begin{align}
\Phi_B=&amp;\exp \left[\sum_{i, \mu \in B}
\frac{\xi_i^\mu}{\sqrt{N}}\left(\sum_{a, \sigma} s_i^a \eta_\mu^\sigma
\sqrt{\beta \lambda_\sigma} x_\sigma^a+\frac{\beta}{\sqrt{N}} \sum_{a,
j, v \in F} s_i^a X_{\mu \nu} \xi_j^v s_j^a\right)\right] \\
\Phi_F=&amp;\exp \left[\frac{\beta}{2 N} \sum_{a, i, j, \mu \in F, v \in
F} s_i^a \xi_i^\mu X_{\mu \nu} \xi_j^v s_j^a\right]
\end{align}$$</span></p>
<p>计算无序平均<span
class="math inline">{<em>ξ</em><sub><em>i</em></sub><sup><em>μ</em></sup>}</span>，可以有：
<span class="math display">$$\begin{align}
\left\langle Z^n\right\rangle=\left\langle\operatorname{Tr} \prod_{a,
\sigma} \int D x_\sigma^a \Phi_B \Phi_F\right\rangle
\end{align}$$</span></p>
<p>首先分析<span
class="math inline">⟨<em>Φ</em><sub><em>B</em></sub>⟩</span>，结合两个序参量<span
class="math inline">$q_{a b}=\frac{1}{N} \sum_i^N s_i^a s_i^b
\quad\text{for}\quad a \neq b$</span> 和 <span
class="math inline">$m_\mu^a=\frac{1}{N} \sum_i \xi_i^\mu s_i^a$</span>
有： <span class="math display">$$\begin{aligned}
\left\langle\Phi_B\right\rangle=&amp;\exp \left\{\frac{1}{2 N} \sum_{i,
\mu \in B}\left[\sum_a s_i^a\left(\sum_\sigma \eta_\mu^\sigma
\sqrt{\beta \lambda_\sigma} x_\sigma^a+\frac{\beta}{\sqrt{N}} \sum_{j, v
\in F} X_{\mu \nu} \xi_j^v s_j^a\right)\right]^2\right\} \\
\left\langle\Phi_B\right\rangle= &amp; \int \prod_{a \neq b} \frac{d
q_{a b} d \hat{q}_{a b}}{2 \pi / N} \prod_{a, \mu \in F} \frac{d m_\mu^a
d \hat{m}_\mu^a}{2 \pi / N} \\
&amp; \times \exp \left[-\frac{1}{2} N \sum_{a \neq b} \hat{q}_{a b}
q_{a b}+\frac{1}{2} \sum_{a \neq b} \hat{q}_{a b} \sum_i s_i^a s_i^b-N
\sum_{a, \mu \in F} m_\mu^a \hat{m}_\mu^a+\sum_{a, \mu \in F}
\hat{m}_\mu^a \sum_i \xi_i^\mu s_i^a\right] \\
&amp; \times \exp \left[\frac{1}{2} \sum_{\mu \in B}
\sum_a\left(\sum_\sigma \eta_\mu^\sigma \sqrt{\beta \lambda_\sigma}
x_\sigma^a+\beta \sqrt{N} \sum_{\nu \in F} X_{\mu \nu}
m_v^a\right)^2\right] \\
&amp; \times \exp \left[\frac{1}{2} \sum_{\mu \in B} \sum_{a \neq b}
q_{a b}\left(\sum_\sigma \eta_\mu^\sigma \sqrt{\beta \lambda_\sigma}
x_\sigma^a+\beta \sqrt{N} \sum_{\nu \in F} X_{\mu \nu}
m_v^a\right)\right. \\
&amp; \left.\times\left(\sum_\sigma \eta_\mu^\sigma \sqrt{\beta
\lambda_\sigma} x_\sigma^b+\beta \sqrt{N} \sum_{\nu \in F} X_{\mu \nu}
m_v^b\right)\right]
\end{aligned}$$</span></p>
<p>再结合<span
class="math inline"><em>δ</em></span>函数傅里叶变换与复本对称的应用：</p>
<p><span class="math display">$$
\begin{aligned}
\left\langle\Phi_B\right\rangle=&amp;\int \frac{d q d \hat{q}}{(2 \pi /
N)^{n(n-1)}} \frac{d m d \hat{m}}{(2 \pi / N)^{n S}}-N n \sum_{\mu \in
F} m_\mu \hat{m}_\mu \exp \left[-\frac{1}{2} N n(n-1) \hat{q} q
\quad+\frac{1}{2} \hat{q} \sum_{a \neq b} \sum_i s_i^a s_i^b+\sum_{a,
\mu \in F} \hat{m}_\mu \sum_i \xi_i^\mu s_i^a\right] \exp \left[\frac {
1 } { 2 } \sum _ { \mu \in B } \sum _ { a } \left(\sum_\sigma
\eta_\mu^\sigma \sqrt{\beta \lambda_\sigma}
x_\sigma^a\right.\left.\quad+\beta \sqrt{N} \sum_{\nu \in F} X_{\mu \nu}
m_v\right)^2\right] \\
&amp;\times \exp \left[\frac{q}{2} \sum_{\mu \in B} \sum_{a \neq
b}\left(\sum_\sigma \eta_\mu^\sigma \sqrt{\beta \lambda_\sigma}
x_\sigma^a+\beta \sqrt{N} \sum_{v \in F} X_{\mu \nu} m_\nu\right)
\times\left(\sum_\sigma \eta_\mu^\sigma \sqrt{\beta \lambda_\sigma}
x_\sigma^b+\beta \sqrt{N} \sum_{v \in F} X_{\mu \nu} m_v\right)\right]
\\
=&amp;\int \frac{d q d \hat{q}}{(2 \pi / N)^{n(n-1)}} \frac{d m d
\hat{m}}{(2 \pi / N)^{n S}} \exp \left[-\frac{1}{2} N n(n-1) \hat{q}
q+\frac{1}{2} \hat{q} \sum_{a \neq b} \sum_i s_i^a s_i^b-N n \sum_{\mu
\in F} m_\mu \hat{m}_\mu +\sum_{a, \mu \in F} \hat{m}_\mu \sum_i
\xi_i^\mu s_i^a\right] \\
&amp;\times \exp \left[\frac{1-q}{2} \sum_{\mu \in B}
\sum_a\left(\sum_\sigma \eta_\mu^\sigma \sqrt{\beta \lambda_\sigma}
x_\sigma^a+\beta \sqrt{N} \sum_{\nu \in F} X_{\mu \nu}
m_v\right)^2\right] \\
&amp; \times \exp \left[\frac{q}{2} \sum_{\mu \in B}\left(\sum_{a,
\sigma} \eta_\mu^\sigma \sqrt{\beta \lambda_\sigma} x_\sigma^a+\beta n
\sqrt{N} \sum_{\nu \in F} X_{\mu \nu} m_\nu\right)^2\right] \\
= &amp; \int \frac{d q d \hat{q}}{(2 \pi / N)^{n(n-1)}} \frac{d m d
\hat{m}}{(2 \pi / N)^{n S}} \prod_{\mu, a} D y_\mu^a \prod_\mu D z_\mu
\\
&amp; \times \exp \left[-\frac{1}{2} N n(n-1) \hat{q} q+\frac{1}{2}
\hat{q} \sum_{a \neq b} \sum_i s_i^a s_i^b-N n \sum_{\mu \in F} m_\mu
\hat{m}_\mu+\sum_{a, \mu \in F} \hat{m}_\mu \sum_i \xi_i^\mu
s_i^a\right] \\
&amp; \times \exp \left[\sqrt{1-q} \sum_{\mu \in B}
\sum_a\left(\sum_\sigma \eta_\mu^\sigma \sqrt{\beta \lambda_\sigma}
x_\sigma^a+\beta \sqrt{N} \sum_{\nu \in F} X_{\mu \nu} m_\nu\right)
y_\mu^a\right] \\
&amp; \times \exp \left[\sqrt{q} \sum_{\mu \in B}\left(\sum_{a, \sigma}
\eta_\mu^\sigma \sqrt{\beta \lambda_\sigma} x_\sigma^a+\beta n \sqrt{N}
\sum_{\nu \in F} X_{\mu \nu} m_\nu\right) z_\mu\right] \\
= &amp; \int \frac{d q d \hat{q}}{(2 \pi / N)^{n(n-1)}} \frac{d m d
\hat{m}}{(2 \pi / N)^{n S}} \prod_{\mu, a} D y_\mu^a \prod_\mu D z_\mu
\\
&amp; \times \exp \left[-\frac{1}{2} N n(n-1) \hat{q} q+\frac{1}{2}
\hat{q} \sum_{a \neq b} \sum_i s_i^a s_i^b-N n \sum_{\mu \in F} m_\mu
\hat{m}_\mu+\sum_{a, \mu \in F} \hat{m}_\mu \sum_i \xi_i^\mu
s_i^a\right] \\
&amp; \times \exp \left[\sum_{a, \sigma} x_\sigma^a \sqrt{\beta
\lambda_\sigma} \sum_{\mu \in B} \eta_\mu^\sigma\left(\sqrt{1-q}
y_\mu^a+\sqrt{q} z_\mu\right)\right] \\
&amp; \times \exp \left[\beta \sqrt{N} \sum_{a, \mu \in B} \sum_{v \in
F} X_{\mu \nu} m_\nu\left(\sqrt{1-q} y_\mu^a+\sqrt{q}
z_\mu\right)\right]
\end{aligned}
$$</span></p>
<p><font color='red'>参考原书</font></p>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Replica Method</tag>
        <tag>Hopfield Model</tag>
      </tags>
  </entry>
  <entry>
    <title>Reclaiming the Lost Conformality in a non-Hermitian Quantum 5-state Potts Model</title>
    <url>/2024/03/20/Phys/five_state_Pottes_Model/five_state_Pottes_Model/</url>
    <content><![CDATA[<p><font color='red'>这篇文章并未读完，更多的内容在于知识上的补充</font></p>
<h1 id="abstract">Abstract</h1>
<p>共形对称发生在临界点，，但是在重整化群的相变点相撞的时候，这种对称性会消失。认为这种实平面的固定点将会变到复平面中。这篇工作，利用非厄米量子5态Potts模型，成功提取到了复平面的高度。</p>
<span id="more"></span>
<h1 id="the-potts-model">The Potts Model</h1>
<p><span class="math display">$$\begin{align}
H = -\sum_{&lt;ij&gt;}J(\theta_{ij})
\end{align}$$</span></p>
<p>planar Potts model: <span class="math display">$$\begin{align}
\theta_{ij}=&amp;\theta_{n_i}-\theta_{n_j} \\
J(\theta) =&amp; -\epsilon_1\cos \theta_{ij}
\end{align}$$</span></p>
<p>standard Potts model (simply the Potts model): <span
class="math display">$$\begin{align}
J(\theta_{ij}) =&amp; \epsilon_2\delta(n_i, n_j)
\end{align}$$</span></p>
<p>以下讨论针对标准的Potts模型。分别用转移矩阵、乘积展开、染色多项式三种方法，得到配分函数。</p>
<ol type="1">
<li><p>转移矩阵法 <span
class="math display"><em>Z</em><sub><em>N</em></sub> = Tr <em>V</em><sup><em>N</em></sup></span></p>
<p>其中 $$ $$</p>
<p>于是配分函数也就是 <span
class="math display"><em>Z</em><sub><em>N</em></sub> = (<em>e</em><sup><em>β</em><em>J</em></sup> + <em>q</em> − 1)<sup><em>N</em></sup> + (<em>q</em> − 1)(<em>e</em><sup><em>β</em><em>J</em></sup> − 1)<sup><em>N</em></sup></span></p></li>
<li><p>乘积展开 <span class="math display">$$
\begin{aligned}
Z_N &amp; =\sum_{\{\sigma\}} \exp \left[\beta J \sum_{\langle i
j\rangle} \delta\left(\sigma_i, \sigma_j\right)\right] \\
&amp; =\sum_{\{\sigma\}} \prod_{\langle i j\rangle}\left[1+v
\delta\left(\sigma_i, \sigma_j\right)\right]
\end{aligned}
$$</span></p>
<p>其中 <span
class="math inline"><em>v</em> = <em>e</em><sup><em>β</em><em>J</em></sup> − 1</span>
。</p></li>
</ol>
<p><a href="https://zhuanlan.zhihu.com/p/101045691">关于Pottes
Model相变精确解，与纽结理论的联系</a></p>
<h1 id="append">Append</h1>
<h2 id="共形对称">共形对称</h2>
<p>共形对称（Conformal
symmetry）是一种特殊类型的全局对称性，它指的是一个理论或系统在共形变换下保持不变的性质。</p>
<p>共形变换是一类特殊的坐标变换，它们不仅包括了普通的平移和旋转，还包括了尺度变换（即缩放）和特殊的非线性变换。共形对称在物理学中尤其重要，因为它通常与系统的尺度不变性相关联，这意味着物理规律在不同的长度尺度上都是相同的。</p>
<h2 id="高低温展开">高低温展开</h2>
<p><a
href="https://zhuanlan.zhihu.com/p/409549219">Ising模型与Duality——相变温度</a></p>
<p><a
href="https://zhuanlan.zhihu.com/p/409603668">Ising模型与Duality——特殊的相变</a></p>
<p><a
href="https://ocw.mit.edu/courses/8-334-statistical-mechanics-ii-statistical-physics-of-fields-spring-2014/pages/lecture-notes/">MIT
notes</a></p>
<p>以2-D Ising 模型为例，配分函数写为： <span
class="math display"><em>Z</em> = ∑<sub>{<em>s</em><sub><em>i</em></sub>}</sub>exp (−<em>β</em><em>H</em>) = ∑<sub>{<em>s</em><sub><em>i</em></sub>}</sub>exp (<em>K</em>∑<sub> &lt; <em>i</em>, <em>j</em>&gt;</sub><em>s</em><sub><em>i</em></sub><em>s</em><sub><em>j</em></sub>)</span>
其中<span
class="math inline"><em>K</em> = <em>β</em><em>J</em></span>。</p>
<p>高温展开，即<span class="math inline"><em>K</em> → 0</span>为： <span
class="math display">$$\begin{equation}
\begin{aligned}
Z= &amp; \sum_{\left\{s_i\right\}} \prod_{&lt;i, j&gt;} \exp \left(K s_i
s_j\right) \\
= &amp; \sum_{\left\{s_i\right\}} \prod_{&lt;i, j&gt;}\left[\sinh
\left(K s_i s_j\right)+\cosh \left(K s_i s_j\right)\right] \\
= &amp; (\cosh K)^{2 N} \sum_{\left\{s_i\right\}} \prod_{&lt;i,
j&gt;}\left[1+s_i s_j \tanh K\right] \\
= &amp; (\cosh K)^{2 N}\left[\sum_{\left\{s_i\right\}} 1+\tanh K
\sum_{\left\{s_i\right\}&lt;i, j&gt;} s_i s_j
+(\tanh K)^2 \sum_{\left\{s_i\right\}} \sum_{&lt;i, j&gt;} \sum_i s_j
s_n s_m+\cdots \right ]\\
=&amp; (\cosh K)^{2 N} 2^N\left[1+N(\tanh K)^4+2 N(\tanh K)^6 +(4
N+N(N-5))(\tanh K)^8 \cdots\right]
\end{aligned}
\end{equation}$$</span></p>
<figure>
<img src="./ap2-1.png" alt="高温展开" />
<figcaption aria-hidden="true">高温展开</figcaption>
</figure>
<p>低温展开，即<span
class="math inline"><em>K</em> → +∞</span>，零温极限： <span
class="math display"><em>Z</em> = 2<em>e</em><sup>2<em>K</em><em>N</em></sup>[1 + <em>N</em>exp (−2<em>K</em>)<sup>4</sup> + 2<em>N</em>exp (−2<em>K</em>)<sup>6</sup> + (4<em>N</em> + (<em>N</em> − 5)<em>N</em>)exp (−2<em>K</em>)<sup>8</sup>⋯]</span>
<img src="./ap2-2.png" alt="低温展开" /></p>
<h3 id="对偶">对偶</h3>
<p>至此我们已经发现对于正方格点上的二维Ising模型,
高温理论的形式与低温理论一致。我们可以在热力学极限下计算自由能 <span
class="math display">$$
F=-\frac{\ln Z}{N}=-\ln 2-2 \ln \cosh K-g(\tanh K)=-2 K-g(\exp (-2 K))
$$</span></p>
<p>后面两项分别是高温展开的自由能与低温展开的自由能。其中 <span
class="math inline"><em>g</em>(<em>x</em>) = ln (1 + <em>N</em><em>x</em><sup>4</sup> + 2<em>N</em><em>x</em><sup>6</sup>⋯)/<em>N</em></span>.
我们知道 2 维Ising模型只有两相, 也就是自由能有且只有一个奇异点。也就是
<span class="math inline">g(x)</span>
有且只有一个奇异点。对于上面两个表达式, 奇异点对应的 <span
class="math inline"><em>K</em><sub><em>c</em></sub></span>
一定相同.那就意味着 <span class="math display">$$
\begin{aligned}
\tanh K_c &amp; =\exp \left(-2 K_c\right) \\
\exp \left(4 K_c\right)-2 \exp \left(2 K_c\right)-1 &amp; =0 \\
K_c &amp; =\frac{\ln (1+\sqrt{2})}{2} \approx 0.4407
\end{aligned}
$$</span></p>
<h2 id="dual-lattice">dual lattice</h2>
<figure>
<img src="./ap3-1.png" alt="dual lattice" />
<figcaption aria-hidden="true">dual lattice</figcaption>
</figure>
<p>上图实心格点表示原晶格，然后在原格点围城的最小面积上，构造一个新的格点，用空心原点表示，代表对偶晶格。</p>
<p>在几何上将原晶格和对偶晶格联系起来，如下图所示，其中格点内的正负代表原晶格，如果反平行边线用实线条连接，平行则不连接。这样便可以把格点转化为图进行研究。</p>
<figure>
<img src="./ap3-2.png" alt="dual lattice" />
<figcaption aria-hidden="true">dual lattice</figcaption>
</figure>
<h2 id="self-duality">Self-duality</h2>
<p>高温展开与低温展开在无限项的情况下是严格的，因此其对应的相应该有相同的关系。利用自对偶的性质求解相变温度。</p>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Potts Model</tag>
      </tags>
  </entry>
  <entry>
    <title>Replica</title>
    <url>/2022/11/13/Phys/replica/replica/</url>
    <content><![CDATA[<p>Reference: * <a
href="https://campus.swarma.org/course/4913/study">Hopfield模型的统计物理视频</a>
* <a href="https://www.science.org/doi/10.1126/science.1073287">Analytic
and Algorithmic Solution of Random Satisfiability Problems</a> * <a
href="https://www.pnas.org/doi/full/10.1073/pnas.0703685104">Gibbs
states and the set of solutions of random constraint satisfaction
problems</a></p>
<span id="more"></span>
<h1 id="average">Average</h1>
<p>淬火平均（quenched average）： <span
class="math display">$$\begin{align}
\langle\ln Z\rangle
\end{align}$$</span></p>
<p>退火平均（annealed average）: <span
class="math display">$$\begin{align}
\ln\langle Z\rangle
\end{align}$$</span></p>
<p>首先从数学上来看这两者是不一样的，可以把<span
class="math inline">ln </span>当成一个凸函数来看待，本质分析的就是先平均再经过凸函数处理，还是先经过凸函数处理然后再平均。通过Jensen不等式有：
<span class="math display">⟨ln <em>Z</em>⟩ ≤ ln ⟨<em>Z</em>⟩</span></p>
<p>这两者有什么区别呢？</p>
<p>对于一个物理量 <span class="math inline"><em>Q</em></span>
，淬火平均表示为： <span class="math display">$$\begin{align}
\langle Q \rangle_{\text{quenched}} = \int P(\{J\}) \left[
\frac{1}{Z(\{J\})} \sum_{\{s\}} Q(\{s\}, \{J\}) e^{-\beta H(\{s\},
\{J\})} \right] d\{J\}
\end{align}$$</span></p>
<ul>
<li><span
class="math inline">{<em>J</em>}</span>：无序变量的集合（如随机耦合）。</li>
<li><span
class="math inline"><em>P</em>({<em>J</em>})</span>：无序变量的概率分布。</li>
<li><span
class="math inline">{<em>s</em>}</span>：系统的状态（如自旋配置）。</li>
<li><span
class="math inline"><em>H</em>({<em>s</em>}, {<em>J</em>})</span>：系统的哈密顿量（能量函数）。</li>
<li><span
class="math inline"><em>Z</em>({<em>J</em>})</span>：对固定无序变量的配分函数。</li>
</ul>
<p>对于一个物理量 <span class="math inline"><em>Q</em></span>
，退火平均表示为： <span class="math display">$$\begin{align}
\langle Q \rangle_{\text{annealed}} = \frac{1}{Z_{\text{annealed}}}
\sum_{\{s\}, \{J\}} Q(\{s\}, \{J\}) e^{-\beta H(\{s\}, \{J\})}
\end{align}$$</span></p>
<ul>
<li><span
class="math inline"><em>Z</em><sub>annealed</sub></span>：退火平均下的配分函数：
<span class="math display">$$\begin{align}
Z_{\text{annealed}} = \sum_{\{s\}, \{J\}} e^{-\beta H(\{s\}, \{J\})}
\end{align}$$</span></li>
</ul>
<p>两者的主要区别在<span
class="math inline">{<em>J</em>}</span>的处理上，对于淬火平均只是选取了其中的几个无序点进行计算，即固定<span
class="math inline"><em>J</em></span>为特定的值。而退火平均将会考虑所有<span
class="math inline"><em>J</em></span>的分布组合。以$Z<span
class="math inline"><em>和</em></span>Z<span
class="math inline"><em>进</em><em>行</em><em>区</em><em>别</em>，<em>其</em><em>中</em></span>Z<span
class="math inline"><em>是</em><em>对</em><em>所</em><em>有</em><em>构</em><em>型</em></span>S<span
class="math inline"><em>进</em><em>行</em><em>的</em><em>求</em><em>和</em>；</span><span
class="math inline"><em>是</em><em>对</em><em>耦</em><em>合</em><em>系</em><em>数</em></span>J$的平均，对于两种方式这个符号有着不同的含义，淬火平均只会选取其中的一些点进行计算，而退火平均将会计算所有的构型。</p>
<p>对于铁磁Ising模型，由于<span
class="math inline"><em>J</em></span>本固定的，因此不会存在上面的讨论的<span
class="math inline">⟨⋅⟩</span>符号的区别。但是对于Hopfield模型，其相互作用是随机连接的，这里存在<span
class="math inline"><em>J</em></span>变化的问题，因此需要讨论退火与淬火的区别。</p>
<p>这里有一个猜测，Jensen不等式什么时候取等号，就是对<span
class="math inline">{<em>J</em>}</span>进行平均的时候，由于每一个具体的<span
class="math inline"><em>J</em></span>存在一个权重，因此可能将<span
class="math inline">ln </span>计算变成一个线性的计算。<font color='red'>想办法证明这个猜测。</font></p>
<p>通常对于这两个概念来说，是从演化时间尺度上比较，淬火表示无序的时间尺度远大于动力学的时间尺度，退火表示无序与动力学的时间尺度差不多大。</p>
<h1 id="replica-method">Replica Method</h1>
<p>利用数学中的极限有：</p>
<p><span class="math display">$$\begin{align}
\ln Z=\lim_{n\to0}\frac{Z^n-1}{n}
\end{align}$$</span></p>
<p>然后计算两边的期望值： <span class="math display">$$\begin{align}
\langle\ln Z\rangle=\lim _{n \rightarrow 0} \frac{\left\langle
Z^n\right\rangle-1}{n}=\lim _{n \rightarrow 0} \frac{\ln \left\langle
Z^n\right\rangle}{n},
\end{align}$$</span></p>
<p>其中 <span class="math inline">⟨⋅⟩</span> 是 <span
class="math inline"><em>ξ</em></span> 的无序平均。</p>
<p>以下证明，以上公式第二个等号成立。首先利用<span
class="math inline">$\ln z=z-\frac{z^2}{2}+\cdots$</span>：</p>
<p><span class="math display">$$\begin{align}
\langle\ln Z\rangle=\lim _{n \rightarrow 0} \frac{n\langle\ln
Z\rangle}{n}=\lim _{n \rightarrow 0} \frac{\ln (1+n\langle\ln
Z\rangle)}{n}
\end{align}$$</span> 因为 <span
class="math inline"><em>Z</em><sup><em>n</em></sup> ≃ 1 + <em>n</em>ln <em>Z</em> + ⋯</span>,
得到 <span
class="math inline">⟨<em>Z</em><sup><em>n</em></sup>⟩ ≃ 1 + <em>n</em>⟨ln <em>Z</em>⟩⋯</span>，故当<span
class="math inline"><em>n</em></span>是一个小量的时候可以将展开公式从右到左用：
<span class="math display">$$\begin{align}
\lim _{n \rightarrow 0} \frac{\ln (1+n\langle\ln Z\rangle)}{n}=\lim _{n
\rightarrow 0} \frac{\ln \left\langle Z^n\right\rangle}{n}
\end{align}$$</span></p>
<p>其中 <span class="math inline"><em>n</em></span>
足够小，可以利用展开<span
class="math inline"><em>Z</em><sup><em>n</em></sup> = <em>e</em><sup><em>n</em>ln <em>Z</em></sup> = 1+</span>
<span class="math inline"><em>n</em>ln <em>Z</em> + ⋯</span>.
每个自旋的平均值可以写为：</p>
<p><span class="math display">$$\begin{align}
f=\lim _{n \rightarrow 0} \lim _{N \rightarrow \infty} \frac{-\ln
\left\langle Z^n\right\rangle}{\beta n N} .
\end{align}$$</span></p>
<h1 id="replica-symmetry-and-replica-symmetry-breaking">Replica Symmetry
and Replica Symmetry Breaking</h1>
<p>在<a href="/2022/11/15/Phys/Hopfield/Hopfield3/" title="Hopfield Model 相图">Hopfield Model 相图</a>中已经展示了复本技巧的使用，得到相图：</p>
<figure>
<img src="./diagram.png" alt="diagram" />
<figcaption aria-hidden="true">diagram</figcaption>
</figure>
<p>在虚线下方将会得到非物理的解，这个现象的发生是因为之前进行的复本对称假设过于简陋，需要使用更高阶的近似——复本对称破缺。</p>
<h2 id="generalized-free-energy-and-complexity-of-states">Generalized
Free Energy and Complexity of States</h2>
<figure>
<img src="./landscape.png" alt="free energy landscape" />
<figcaption aria-hidden="true">free energy landscape</figcaption>
</figure>
<p>随着温度的下降，一整个自由能的解，将会分裂为几个部分。每一最小值点对应TAP方程或者信念传播方程的解。为了解开Gibbs
measure中的耦合，需要引入额外的一项刻画自由能的波动，定义为<span
class="math inline"><em>y</em></span>。有如下的关系：</p>
<p><span class="math display">$$\begin{align}
e^{-y \Phi}=\sum_\alpha e^{-y F_\alpha}=\int \mathrm{d} f e^{N(-y
f+\Sigma(f))} \label{RSB}
\end{align}$$</span></p>
<p>其中<span
class="math inline">∑(<em>f</em>)</span>是指数多个态的集合体；<span
class="math inline"><em>Φ</em></span>表示复本（广义）自由能；通过自由能密度<span
class="math inline"><em>f</em><sub><em>α</em></sub></span>表示波动。这个式子也被叫做
1-RSB 。</p>
<p>自由能表达式<span class="math inline">$F=-\frac{1}{\beta}\ln
Z$</span>，可以得到<span
class="math inline"><em>Z</em> = exp [−<em>β</em><em>F</em>]</span>;同时配分函数为<span
class="math inline"><em>Z</em> = ∑<sub><strong>S</strong></sub>exp [−<em>β</em><em>H</em>(<strong>S</strong>)]</span>。将两项结合有<span
class="math inline">exp [−<em>β</em><em>F</em>] = <em>Z</em> = ∑<sub><strong>S</strong></sub>exp [−<em>β</em><em>H</em>(<strong>S</strong>)]</span>。将<span
class="math inline"><em>Φ</em></span>类比<span
class="math inline"><em>F</em></span>；<span
class="math inline"><em>α</em></span>类比<span
class="math inline"><strong>S</strong></span>，由此可以得到<span
class="math inline">$\eqref{RSB}$</span>中第一个等号。</p>
<p>将自由能写成平均值表达方式<span
class="math inline"><em>F</em><sub><em>α</em></sub> = <em>N</em><em>f</em><sub><em>α</em></sub></span>，其中<span
class="math inline"><em>f</em><sub><em>α</em></sub></span>为<span
class="math inline"><em>α</em></span>构型下的自由能密度，则<span
class="math inline"><em>e</em><sup>−<em>y</em><em>F</em><sub><em>α</em></sub></sup> = <em>e</em><sup>−<em>N</em><em>y</em><em>f</em><sub><em>α</em></sub></sup></span>。对所有构型求和<span
class="math inline">∑<sub><em>α</em></sub></span>不能直接写为积分，因为可能出现简并度问题，需要引入额外的简并度参数<span
class="math inline"><em>C</em>(<em>f</em>)</span>：<span
class="math inline">∑<sub><em>α</em></sub><em>e</em><sup>−<em>y</em><em>F</em><sub><em>α</em></sub></sup> = ∫d<em>f</em><em>C</em>(<em>f</em>)<em>e</em><sup><em>N</em>(−<em>y</em><em>f</em>)</sup></span>，将<span
class="math inline"><em>C</em>(<em>f</em>)</span>写在指数上可以得到<span
class="math inline">∫d<em>f</em><em>e</em><sup><em>N</em>(−<em>y</em><em>f</em> + <em>Σ</em>(<em>f</em>))</sup></span>。不难发现<span
class="math inline"><em>Σ</em>(<em>f</em>)</span>是<span
class="math inline"><em>f</em></span>数量的熵。由此可以得到<span
class="math inline">$\eqref{RSB}$</span>中第二个等号。</p>
<p>对于第三项，在热力学极限下，结合 Laplace 近似得到： <span
class="math display">$$\begin{align}
-y \phi =\max _f\{\Sigma(f)-y f\} \label{laplace}
\end{align}$$</span></p>
<p>求极值点，可知 <span class="math inline">$y=\frac{\partial
\Sigma(f)}{\partial
f}$</span>，这里已经符合勒让德变化的前提条件了。<span
class="math inline"><em>ϕ</em></span>表示复本自由能密度（自旋的平均值）。通过勒让德变换，得到如下的关系：
<span class="math display">$$
\begin{align}
f &amp; =\frac{\partial(y \phi)}{\partial y} \label{94}\\
\Sigma &amp; =y(f-\phi)=y^2 \frac{\partial \phi}{\partial y} \label{95}
\end{align}
$$</span></p>
<p><span class="math inline"><em>y</em></span>也被称作Parisi参数。</p>
<h2 id="claculate-generalized-free-energy">Claculate Generalized Free
Energy</h2>
<p>利用一阶复本对称破缺的空腔方法可以计算<span
class="math inline"><em>ϕ</em></span>。空腔方法首先是构建因子图，利用功能节点与变量节点表示整个计算目标，然后通过分析功能节点与变量节点增删对整体变化的影响，将整个计算目标表示为功能节点与变量节点的变化点关系。</p>
<p>首先在因子图上增加一个变量节点，在增加节点前后变化如下：</p>
<p><span class="math display">$$
\begin{align}
e^{-y \phi_i^{\text {new }}} &amp; =\sum_\alpha e^{-y F^\alpha-y \Delta
F_i^\alpha}=e^{-y \phi^{\text {old }}} \sum_\alpha \omega(\alpha) e^{-y
\Delta F_i^\alpha} \\
&amp; =e^{-y \phi^{\text {old }}}\left\langle e^{-y \Delta
F_i}\right\rangle
\end{align}
$$</span> 其中<span class="math inline">$\omega(\alpha)=\frac{e^{-y
F^\alpha}}{\sum_a e^{-y F^\alpha}}$</span>，并且<span
class="math inline">⟨⋅⟩</span>表示所有<span
class="math inline"><em>α</em></span>构型的平均。
对于增加功能节点有关系式： <span class="math display">$$\begin{align}
e^{-y \phi_a^{\text {new }}}=e^{-y \phi^{\mathrm{old}}}\left\langle
e^{-y \Delta F_a}\right\rangle
\end{align}$$</span> 由此复本自由能转变为空腔操作： <span
class="math display">$$
\begin{align}
&amp; -y \Delta \phi_i=\ln \left\langle e^{-y \Delta F_i}\right\rangle
\\
&amp; -y \Delta \phi_a=\ln \left\langle e^{-y \Delta F_a}\right\rangle
\end{align}
$$</span></p>
<p>最后利用Bethe近似，复本自由能得到表示为： <span
class="math display">$$\begin{align}
\phi=\sum_i \Delta \phi_i-\sum_a(|\partial a|-1) \Delta \phi_a,
\end{align}$$</span> 因此<span
class="math inline">$\eqref{94}$</span>和<span
class="math inline">$\eqref{95}$</span>表示为： <span
class="math display">$$
\begin{aligned}
f &amp; =\frac{\left\langle\Delta F_i e^{-y \Delta
F_i}\right\rangle}{\left\langle e^{-y \Delta
F_i}\right\rangle}-\sum_a(|\partial a|-1) \frac{\left\langle\Delta F_a
e^{-y \Delta F_a}\right\rangle}{\left\langle e^{-y \Delta
F_a}\right\rangle} \\
\Sigma &amp; =y(f-\phi)
\end{aligned}
$$</span></p>
<h2 id="cavity">Cavity</h2>
<h3 id="energy-cavity">Energy Cavity</h3>
<p>已知 Parisi 参量<span
class="math inline"><em>y</em></span>与逆温度<span
class="math inline"><em>β</em></span>，引入新的参数<span
class="math inline"><em>m</em></span>构建极限情况下这两者的关系：</p>
<p><span class="math display">$$\begin{align}
\lim_{\beta\to\infty,m\to 0}\beta m=y
\end{align}$$</span></p>
<p>广义自由能与能量之间的关系为： <span
class="math display">$$\begin{align}
\exp[-y\Phi]&amp;=\sum_{\alpha,S}\exp[-m\beta H_\alpha(S)] \\
&amp;= \sum_{\alpha}Z^m_\alpha
\end{align}$$</span></p>
<p>利用Laplace近似，<span
class="math inline">$\eqref{laplace}写为$</span>：</p>
<p><span class="math display">$$\begin{align}
-\beta m \phi(\beta, m)=\max _{s, \epsilon}\{\Sigma(s,
\epsilon)+m(s-\beta \epsilon)\} \label{913}
\end{align}$$</span></p>
<p>其中将自由能写为熵和能量的依赖关系<span
class="math inline">−<em>β</em><em>f</em> = <em>s</em> − <em>β</em><em>ϵ</em></span>。</p>
<p>如果看能量的关系，将<span
class="math inline">$\eqref{913}$</span>中熵固定，有关系： <span
class="math display">$$\begin{align}
\phi_\epsilon(y)=\max _\epsilon\{\Sigma(\epsilon)-y \epsilon\} .
\end{align}$$</span></p>
<p><span
class="math inline"><em>Σ</em>(<em>ϵ</em>)</span>表示在能量密度为<span
class="math inline"><em>ϵ</em></span>的情况下，集团的数目。以K-SAT问题为例，其中<span
class="math inline"><em>ϵ</em></span>表示能量（如果全部满足则<span
class="math inline"><em>ϵ</em> = 0</span>，存在不满足的情况时则<span
class="math inline"><em>ϵ</em> &gt; 0</span>）；<span
class="math inline"><em>Σ</em>(<em>ϵ</em>)</span>表示在该能量的情况下有多少个解的数目，当<span
class="math inline">lim <em>ϵ</em> → 0</span>的时候就表示全部满足的解的数目。</p>
<h3 id="entropic-cavity">Entropic Cavity</h3>
<p>将<span class="math inline">$\eqref{913}$</span>中能量固定，得到：
<span class="math display">$$\begin{align}
\phi(m) = \max_{s}\{\Sigma(s)+ms\}
\end{align}$$</span></p>
<p>并且有勒让德变化关系： <span class="math display">$$\begin{align}
s=&amp;\frac{\partial \phi(m)}{\partial m}\\
\Sigma(s)&amp;=\phi(m)-ms
\end{align}$$</span></p>
<figure>
<img src="./ms.png" alt="ms" />
<figcaption aria-hidden="true">ms</figcaption>
</figure>
<p>上图中的小图斜率为<span class="math inline">$-m=\frac{\partial
\Sigma(s)}{\partial s}$</span>，只有<span
class="math inline">0 ≤ <em>m</em></span>的部分有意义，随着熵<span
class="math inline"><em>s</em></span>增大广义熵<span
class="math inline"><em>Σ</em></span>开始减少，对于优化问题对应含义为：当组合优化问题中（例如k-SAT问题）一个特定问题（给定参数）的解的数量增加时候（<span
class="math inline"><em>s</em></span>增加），则组合优化问题（没有指定具体参数）存在这样解的问题数量将会减少（<span
class="math inline"><em>Σ</em></span>变小）；小图中<span
class="math inline"><em>m</em> = 0</span>的点，对应上面讨论的<span
class="math inline"><em>ϵ</em> = 0</span>的情况，即问题只需满足有解条件。大图是<span
class="math inline"><em>m</em></span>与k-SAT问题中<span
class="math inline"><em>α</em></span>的关系，当问题比较容易的时候(<span
class="math inline"> ≤ <em>α</em><sub><em>c</em></sub></span>)<span
class="math inline"><em>m</em> = 1</span>，随着问题的难度增加（<span
class="math inline"><em>α</em></span>变大），<span
class="math inline"><em>m</em></span>开始变小，逐渐出现小图中的的关系，当达到一定程度（<span
class="math inline"> = <em>α</em><sub><em>c</em></sub></span>）的时候成为是否有解的阈值。</p>
<figure>
<img src="./sigma_alpha.png" alt="sigma-alpha" />
<figcaption aria-hidden="true">sigma-alpha</figcaption>
</figure>
<p>上图中两条曲线是两条相变线。<span
class="math inline"><em>m</em> = 1</span>的红线是动力学相变线，遍历性开始破缺，从此线开始问题对应的解的数量开始变少，例如蒙卡模拟临界慢化；<span
class="math inline"><em>m</em> = 0</span>是熵为<span
class="math inline">0</span>的线，分别基态与激发态，判别是否有解。</p>
<h3 id="rsb">1RSB</h3>
<p>之前已经将Cavity
用于计算一个构型的自由能，接下来将其引入复本技巧中，计算不同构型自由能之间的耦合。</p>
<p><span class="math display">$$\begin{align}
P\left(m_{i \rightarrow a}\right)=\frac{1}{\mathcal{Z}_{i \rightarrow
a}} \prod_{b \in \partial i \backslash a} \int \mathrm{d} \hat{m}_{b
\rightarrow i} \delta\left(m_{i \rightarrow
a}-\mathcal{F}\left(\left\{\hat{m}_{b \rightarrow
a}\right\}\right)\right) Z_{i \rightarrow a}^m
\end{align}$$</span></p>
<p>其中必须考虑一个构型自由能变化，导致对其它自由能构型的扰动，这一项用<span
class="math inline"><em>Z</em><sub><em>i</em> → <em>a</em></sub><sup><em>m</em></sup></span>表示。<font color='red'>可以尝试使用变分导出。</font></p>
<p>当<span class="math inline"><em>m</em> = 1</span>的时候，<span
class="math inline">$\eqref{913}$</span>有: <span
class="math display">$$\begin{align}
\phi(\beta,m=1)=\epsilon-\frac{\Sigma(s,\epsilon)+s}{\beta}=\epsilon-Ts_\text{tot}
\end{align}$$</span> 其中<span
class="math inline"><em>s</em><sub>tot</sub> = <em>Σ</em> + <em>s</em></span>是总解的数目，<span
class="math inline"><em>Σ</em></span>为构型数目，<span
class="math inline"><em>s</em></span>为一个构型中解的数目，两者相乘则得到<span
class="math inline"><em>s</em><sub>tot</sub></span>。这个公式说明了
Cavity的方法在1阶复本破缺的情况下适用的原因。</p>
<p>接下来讨论冻结（Frozen）的情况，也即是每一个构型只有一个解的情况，此时<span
class="math inline"><em>s</em> = 0</span>： <span
class="math display">$$\begin{align}
e^{-N \beta m \phi(\beta, m)}=\sum_\alpha e^{-N \beta m
f_\alpha}=\sum_\alpha e^{-N \beta m \epsilon_\alpha}=e^{-N \beta m
f_{\mathrm{RS}}(\beta m)}
\end{align}$$</span></p>
<p>存在冻结温度<span
class="math inline"><em>s</em><sub>RS</sub>(<em>β</em><sub><em>s</em></sub>) = 0</span>，此时有<span
class="math inline">$m=\frac{\beta_s}{\beta}=1$</span>。<span
class="math inline"><em>m</em></span>的含义为<span
class="math inline">$1-m=\sum_\alpha\overline{\omega_\alpha^2}$</span>，其中<span
class="math inline"><em>ω</em></span>为构型权重，更多的参考<a
href="https://www.semanticscholar.org/paper/Storage-capacity-of-memory-networks-with-binary-Krauth-M%C3%A9zard/。5c7af8e28ed12cbfb07cbbcfb001919f6a081d5a">Storage
capacity of memory networks with binary couplings</a>。
<font color='red'>阅读这篇文献。</font></p>
<h2 id="more-steps-of-replica-symmetry-breaking">More Steps of Replica
Symmetry Breaking</h2>
<p>由于使用了空腔方法，在具备长程关联的情况下将会变得不精确。解正确的必要条件是spin
glass susceptibility <span class="math inline"><em>χ</em><sub>SG
</sub></span>没有发散:</p>
<p><span class="math display">$$\begin{align}
\chi_{\mathrm{SG}}=\frac{1}{N} \sum_{i, j}
\overline{\left(\left\langle\sigma_i
\sigma_j\right\rangle-\left\langle\sigma_i\right\rangle\left\langle\sigma_j\right\rangle\right)^2}
\end{align}$$</span> 其中<span
class="math inline">⟨⋅⟩</span>是热力学平均，上划线是无序平均。当这个条件不满足的时候，需要引入更高阶的复本对称。</p>
<h1 id="optimization">Optimization</h1>
<p>参考 Spin Glass Theory and Beyound 书籍。</p>
<h1 id="append">Append</h1>
<h2 id="勒让德变换">勒让德变换</h2>
<p>勒让德变换（Legendre
transform）是一种数学工具，用于在物理学和数学中将一个函数转换为其共轭变量的函数。它在经典力学、热力学、统计力学以及最优化问题中有重要应用。勒让德变换能够将一个变量的依赖关系转化为另一变量的依赖关系，从而简化问题的求解。</p>
<h3 id="定义">定义</h3>
<p>设 <span class="math inline"><em>f</em>(<em>x</em>)</span>
是一个实函数，其导数 <span
class="math inline"><em>f</em><sup>′</sup>(<em>x</em>)</span>
单调且严格递增，这样 <span
class="math inline"><em>f</em><sup>′</sup>(<em>x</em>)</span> 的反函数
<span
class="math inline"><em>x</em> = (<em>f</em><sup>′</sup>)<sup>−1</sup>(<em>p</em>)</span>
存在。勒让德变换将函数 <span
class="math inline"><em>f</em>(<em>x</em>)</span> 转换为一个新函数 <span
class="math inline"><em>g</em>(<em>p</em>)</span>，其中 <span
class="math inline"><em>p</em></span> 是 <span
class="math inline"><em>f</em>(<em>x</em>)</span> 的导数 <span
class="math inline"><em>p</em> = <em>f</em><sup>′</sup>(<em>x</em>)</span>。勒让德变换定义为：</p>
<p><span
class="math display"><em>g</em>(<em>p</em>) = <em>x</em><em>p</em> − <em>f</em>(<em>x</em>)</span></p>
<p>其中， <span class="math inline"><em>x</em></span> 满足 <span
class="math inline"><em>p</em> = <em>f</em><sup>′</sup>(<em>x</em>)</span>。这个过程可以逆转过来得到
<span class="math inline"><em>f</em>(<em>x</em>)</span>，即</p>
<p><span
class="math display"><em>f</em>(<em>x</em>) = <em>x</em><em>p</em> − <em>g</em>(<em>p</em>)</span></p>
<p>其中，<span class="math inline"><em>p</em></span> 满足 <span
class="math inline"><em>x</em> = <em>g</em><sup>′</sup>(<em>p</em>)</span>。</p>
<h3 id="性质">性质</h3>
<ol type="1">
<li><strong>对偶关系</strong>：勒让德变换是自反的，即对 <span
class="math inline"><em>f</em>(<em>x</em>)</span> 进行勒让德变换得到
<span class="math inline"><em>g</em>(<em>p</em>)</span>，再对 <span
class="math inline"><em>g</em>(<em>p</em>)</span> 进行勒让德变换会返回到
<span class="math inline"><em>f</em>(<em>x</em>)</span>。</li>
<li><strong>凸函数</strong>：勒让德变换通常用于凸函数。对于一个凸函数
<span class="math inline"><em>f</em>(<em>x</em>)</span>，其勒让德变换
<span class="math inline"><em>g</em>(<em>p</em>)</span> 也是凸的。</li>
</ol>
<h3 id="应用">应用</h3>
<ol type="1">
<li><p><strong>热力学</strong>：在热力学中，勒让德变换用于将内能 <span
class="math inline"><em>U</em>(<em>S</em>, <em>V</em>)</span>
转换为其他热力学势，例如亥姆霍兹自由能 <span
class="math inline"><em>F</em>(<em>T</em>, <em>V</em>)</span>、吉布斯自由能
<span class="math inline"><em>G</em>(<em>T</em>, <em>P</em>)</span> 和焓
<span
class="math inline"><em>H</em>(<em>S</em>, <em>P</em>)</span>。这些不同的热力学势对应于不同的自然变量，可以更方便地描述系统的平衡状态。</p>
<ul>
<li>亥姆霍兹自由能：<span
class="math inline"><em>F</em>(<em>T</em>, <em>V</em>) = <em>U</em> − <em>T</em><em>S</em></span></li>
<li>吉布斯自由能：<span
class="math inline"><em>G</em>(<em>T</em>, <em>P</em>) = <em>U</em> − <em>T</em><em>S</em> + <em>P</em><em>V</em></span></li>
<li>焓：<span
class="math inline"><em>H</em>(<em>S</em>, <em>P</em>) = <em>U</em> + <em>P</em><em>V</em></span></li>
</ul></li>
<li><p><strong>经典力学</strong>：在哈密顿力学中，勒让德变换将拉格朗日函数
<span class="math inline"><em>L</em>(<em>q</em>, <em>q̇</em>)</span>
转换为哈密顿函数 <span
class="math inline"><em>H</em>(<em>q</em>, <em>p</em>)</span>： <span
class="math display"><em>H</em>(<em>q</em>, <em>p</em>) = <em>q̇</em><em>p</em> − <em>L</em>(<em>q</em>, <em>q̇</em>)</span>
其中，<span class="math inline">$p = \frac{\partial L}{\partial
\dot{q}}$</span>。</p></li>
<li><p><strong>优化理论</strong>：勒让德变换在优化问题中也有应用，特别是在凸优化问题中，它将原函数的最小化问题转换为共轭函数的最小化问题。</p></li>
</ol>
<h3 id="例子">例子</h3>
<p>考虑一个简单的函数 <span class="math inline">$f(x) = \frac{1}{2} k
x^2$</span>，其中 <span class="math inline"><em>k</em></span>
是常数。我们可以计算其勒让德变换 <span
class="math inline"><em>g</em>(<em>p</em>)</span>：</p>
<ol type="1">
<li>计算导数：<span
class="math inline"><em>p</em> = <em>f</em><sup>′</sup>(<em>x</em>) = <em>k</em><em>x</em></span></li>
<li>解出 <span class="math inline"><em>x</em></span>：<span
class="math inline">$x = \frac{p}{k}$</span></li>
<li>代入勒让德变换公式：<span class="math inline">$g(p) = x p - f(x) =
\frac{p}{k} p - \frac{1}{2} k \left( \frac{p}{k} \right)^2 =
\frac{p^2}{2k}$</span></li>
</ol>
<p>所以， <span class="math inline">$f(x) = \frac{1}{2} k x^2$</span>
的勒让德变换是 <span class="math inline">$g(p) =
\frac{p^2}{2k}$</span>。</p>
<p>总结来说，勒让德变换是一个强大的数学工具，用于在不同变量之间进行转换，从而简化物理和数学问题的求解过程。</p>
<h2 id="k-sat">K-SAT</h2>
<p>K-SAT问题是计算复杂性理论和计算机科学中一个著名的逻辑难题，属于布尔可满足性问题（Boolean
satisfiability problem,
SAT）的一个特例。K-SAT问题被广泛研究，因为它在理论计算机科学中具有重要意义，并且是NP完全问题的典型代表。</p>
<h3 id="k-sat问题的定义">K-SAT问题的定义</h3>
<ol type="1">
<li><p><strong>布尔变量</strong>：K-SAT问题涉及一组布尔变量 <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>n</em></sub></span>，每个变量可以取值为真（True,
1）或假（False, 0）。</p></li>
<li><p><strong>子句</strong>：一个子句是布尔变量的若干文字（变量或其否定）的或（OR）运算。例如，((x_1
x_2 x_3)) 是一个包含三个文字的子句。</p></li>
<li><p><strong>K个文字</strong>：在K-SAT问题中，每个子句包含恰好K个不同的文字（变量或其否定）。</p></li>
<li><p><strong>公式</strong>：K-SAT问题给定一个布尔公式，是这些子句的与（AND）运算。例如，一个3-SAT问题的公式可能是：
<span
class="math display">(<em>x</em><sub>1</sub> ∨ ¬<em>x</em><sub>2</sub> ∨ <em>x</em><sub>3</sub>) ∧ (¬<em>x</em><sub>1</sub> ∨ <em>x</em><sub>2</sub> ∨ <em>x</em><sub>4</sub>) ∧ (<em>x</em><sub>2</sub> ∨ ¬<em>x</em><sub>3</sub> ∨ ¬<em>x</em><sub>4</sub>)</span></p></li>
</ol>
<h3 id="问题描述">问题描述</h3>
<p>K-SAT问题的目标是确定是否存在一个布尔变量的赋值，使得整个公式为真。换句话说，找到一种布尔变量的赋值，使得所有子句都为真。</p>
<h3 id="重要性">重要性</h3>
<ol type="1">
<li><p><strong>NP完全性</strong>：对于 <span
class="math inline"><em>K</em> ≥ 3</span>，K-SAT问题是NP完全的，这意味着它是NP问题中最难的一类。如果可以找到一个多项式时间算法来解决任意的3-SAT问题，那么所有NP问题都可以在多项式时间内解决。</p></li>
<li><p><strong>应用</strong>：K-SAT问题在计算机科学和工程中有广泛的应用，包括电路设计、软件验证、人工智能中的约束满足问题、规划和调度等。</p></li>
<li><p><strong>理论研究</strong>：K-SAT问题是计算复杂性理论的重要研究对象，许多复杂性理论的基本结果都是通过研究K-SAT问题得到的。</p></li>
</ol>
<h3 id="特例">特例</h3>
<ol type="1">
<li><p><strong>2-SAT问题</strong>：2-SAT问题是K-SAT问题的一个特例，其中每个子句包含恰好两个文字。2-SAT问题可以在多项式时间内解决，不像K-SAT问题（对于
<span
class="math inline"><em>K</em> ≥ 3</span>）那样是NP完全的。</p></li>
<li><p><strong>1-SAT问题</strong>：1-SAT问题是最简单的特例，其中每个子句只包含一个文字。显然，1-SAT问题可以在线性时间内解决。</p></li>
</ol>
<h3 id="示例">示例</h3>
<p>考虑以下3-SAT问题： <span
class="math display">(<em>x</em><sub>1</sub> ∨ ¬<em>x</em><sub>2</sub> ∨ <em>x</em><sub>3</sub>) ∧ (¬<em>x</em><sub>1</sub> ∨ <em>x</em><sub>2</sub> ∨ <em>x</em><sub>4</sub>) ∧ (<em>x</em><sub>2</sub> ∨ ¬<em>x</em><sub>3</sub> ∨ ¬<em>x</em><sub>4</sub>)</span></p>
<p>我们需要找到一种布尔变量的赋值，使得上述公式为真。例如，赋值 <span
class="math inline"><em>x</em><sub>1</sub> = <em>T</em><em>r</em><em>u</em><em>e</em>, <em>x</em><sub>2</sub> = <em>F</em><em>a</em><em>l</em><em>s</em><em>e</em>, <em>x</em><sub>3</sub> = <em>T</em><em>r</em><em>u</em><em>e</em>, <em>x</em><sub>4</sub> = <em>F</em><em>a</em><em>l</em><em>s</em><em>e</em></span>
可以使得每个子句都为真，从而满足整个公式。</p>
<h3 id="结论">结论</h3>
<p>K-SAT问题在计算机科学中具有重要意义，尤其是作为NP完全问题的代表。它不仅在理论研究中扮演重要角色，也在许多实际应用中得到广泛应用。通过研究和解决K-SAT问题，计算机科学家可以更好地理解复杂问题的本质以及开发有效的算法。</p>
<p><font color='red'>stop</font></p>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Replica Method</tag>
        <tag>Hopfield Model</tag>
      </tags>
  </entry>
  <entry>
    <title>Replica in Random Matrix</title>
    <url>/2024/07/03/Phys/replica/replica_math/</url>
    <content><![CDATA[<span id="more"></span>
<h3 id="stieltjes变换概述">Stieltjes变换概述</h3>
<h4 id="定义">定义</h4>
<p>Stieltjes变换是一个用于分析和研究实数轴上测度（或函数）性质的工具。对于一个实数轴上的测度
<span class="math inline"><em>μ</em></span>，它的Stieltjes变换 $ G(z) $
定义为：</p>
<p><span class="math display">$$ G(z) = \int_{-\infty}^{\infty}
\frac{d\mu(x)}{x - z} $$</span></p>
<p>其中，$ z $ 是复平面上的一个点，且不在实数轴上的支撑集（即测度 <span
class="math inline"><em>μ</em></span> 为零的区域）内。</p>
<p>对于一个函数 $ f(x) $
来说，如果我们把它视为测度的密度函数，则Stieltjes变换可以写成：</p>
<p><span class="math display">$$ G(z) = \int_{-\infty}^{\infty}
\frac{f(x) \, dx}{x - z} $$</span></p>
<h4 id="逆变换">逆变换</h4>
<p>Stieltjes变换的逆变换用于从变换后的函数恢复原始测度或密度函数。对于谱密度
$ (x) $，我们有：</p>
<p><span class="math display">$$ \rho(x) = \lim_{\epsilon \to 0^+}
\frac{1}{\pi} \Im[G(x + i\epsilon)] $$</span></p>
<p>其中，$ $ 表示 $ G(z) $ 在 $ z = x + i$ 处的虚部。</p>
<h3 id="应用示例wigner矩阵的谱密度">应用示例：Wigner矩阵的谱密度</h3>
<h4 id="背景">背景</h4>
<p>Wigner矩阵是一个对称的随机矩阵，其元素是独立同分布的随机变量。设 $ W
$ 是一个 $ N N $ 的Wigner矩阵，其元素 $ W_{ij} $ 满足以下条件： - $
W_{ij} = W_{ji} $ - 对于 $ i j $， $ W_{ij} $ 是均值为零、方差为 $ $
的独立随机变量 - 对角元素 $ W_{ii} $ 是均值为零、方差为 $ $
的独立随机变量</p>
<p>Wigner半圆定律描述了在 $ N $
的极限下，Wigner矩阵的特征值分布趋向于一个半圆分布。</p>
<h4 id="自洽方程推导">自洽方程推导</h4>
<p>随机矩阵 $ W $ 的Stieltjes变换 $ G(z) $ 定义为： <span
class="math display">$$ G_N(z) = \frac{1}{N} \sum_{i=1}^N
\frac{1}{\lambda_i - z} $$</span></p>
<p>在大尺寸极限下， $ G_N(z) $ 的期望值趋向于一个确定的值 $ G(z) <span
class="math inline">，<em>满</em><em>足</em><em>自</em><em>洽</em><em>方</em><em>程</em>：</span>$
G(z) = $$</p>
<p>这个方程通过以下步骤推导： 1. 将Stieltjes变换定义为矩阵的特征值求和。
2. 利用矩阵的迹和逆矩阵的关系，展开逆矩阵。 3.
在大尺寸极限下，假设矩阵元素足够小，进行平均场近似。 4.
得到自洽方程，并通过二次方程求解。</p>
<h4 id="解析自洽方程">解析自洽方程</h4>
<p>解自洽方程 $ G(z) = <span class="math inline">：</span>$ G(z)^2 +
zG(z) + 1 = 0 $$</p>
<p>解得： <span class="math display">$$ G(z) = \frac{-z \pm \sqrt{z^2 -
4}}{2} $$</span></p>
<p>选择满足 $ $ 的分支： <span class="math display">$$ G(z) = \frac{-z +
\sqrt{z^2 - 4}}{2} $$</span></p>
<h4 id="恢复谱密度">恢复谱密度</h4>
<p>根据逆Stieltjes变换公式，谱密度 $ (x) $ 为： <span
class="math display">$$ \rho(x) = \frac{1}{\pi} \text{Im}[G(x +
i\epsilon)] $$</span></p>
<p>代入 $ G(z) $ 的表达式，计算得： <span class="math display">$$
\rho(x) = \frac{1}{2\pi} \sqrt{4 - x^2} $$</span></p>
<p>这正是Wigner半圆分布的谱密度。</p>
<h3 id="总结">总结</h3>
<p>Stieltjes变换在随机矩阵理论中是一个强大的工具，特别适用于谱密度的计算。通过Stieltjes变换，可以将实数轴上的谱密度问题转化为复平面上的解析问题，利用其逆变换可以从复平面上的函数恢复原始的谱密度，从而简化了复杂的计算过程。</p>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Replica Method</tag>
        <tag>Random Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title>Student of Games:Aunified learning algorithm forboth perfect andimperfect information games</title>
    <url>/2024/08/24/RL/SoG/SoG/</url>
    <content><![CDATA[<p>在之前的游戏引擎中存在仅适用于单一游戏的问题，虽然AlphaZero解决对于完全完美信息的通用性，但是对于扑克等依旧不存在通用算法。这篇文章提出一种新的通用算法Student
of
Games（SoG），该算法类似于AlphaZero通过自博弈的方式完成训练，同时拓展了适用边界——适用于非完全信息博弈，例如扑克。</p>
<p><font color='red'>这个算法结合了很多内容，并没有读懂</font></p>
<p>Link: * <a
href="https://www.science.org/doi/10.1126/sciadv.adg3256#supplementary-materials">Student
of Games:Aunified learning algorithm forboth perfect andimperfect
information games</a></p>
<span id="more"></span>
<h1 id="introduction">Introduction</h1>
<p>对于完全完美信息博弈，在理论上存在最优策略，难点在于如何找到该策略。对于不完全信息博弈，是否存在最优策略呢？因为不知道别人的信息，自己作出的选择可能因为对方的信息得到截然不同的结果，因此在这种情况下如何判断不同策略之间哪一个是最优策略？解答这个问题，需要换一个角度从纳什均衡出发，如果存在一种策略使得处于纳什均衡，一旦脱离该策略，那么必然会使得其处于更糟糕的点，那么这个策略就是最优策略。<font color='red'>但是可能存在多个均衡策略，哪一个策略是最优的呢？</font></p>
<p>对于不完全信息博弈，已有的算法是<a
href="https://proceedings.neurips.cc/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf">counterfactual
regretminimiza tion (CFR)</a>，以及后续衍生的<a
href="https://www.science.org/doi/10.1126/science.aam6960">DeepStack</a>算法。</p>
<p>这篇文章结合了AlphaZero与DeepStack算法，与AlphaZero的区别在于自博弈的记录可以用于训练非完全信息博弈；与DeepStack的区别在于使用更少的domain
knowledge，单个网络对于所有策略。与该算法接近的算法是<a
href="https://arxiv.org/abs/2007.13544">Recurr entBelief-based Learning
(ReBeL)</a></p>
<h1 id="sog-algorithm">SoG algorithm</h1>
<figure>
<img src="./CVPN.png" alt="CVPN" />
<figcaption aria-hidden="true">CVPN</figcaption>
</figure>
<p>将参数给入网络得到私有策略。</p>
<figure>
<img src="./GT-CFR.png" alt="GT-CFR" />
<figcaption aria-hidden="true">GT-CFR</figcaption>
</figure>
<p>包含后悔值的搜索树。</p>
<figure>
<img src="./train.png" alt="Train" />
<figcaption aria-hidden="true">Train</figcaption>
</figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Student of Games</tag>
      </tags>
  </entry>
  <entry>
    <title>Policy Gradients In Reinforcement</title>
    <url>/2024/03/08/RL/PPO/PPO/</url>
    <content><![CDATA[<p><font color='red'>文中有一些问题仍未处理，缺失具体代码的解读，对于TRPO算法的认知仍然存在不清楚的地方，高阶梯度怎么算的</font></p>
<h1 id="abstract">Abstract</h1>
<p>这是一篇关于策略梯度算法的总结。首先给出梯度策略，介绍其基本含义，但是初始方案存在一个问题，可以知道梯度变化的方向，不知道梯度的步长。然后，提出自然梯度算法，通过加入约束的方案计算出梯度的步长。接下来，Trust
Region Policy
Optimization（TRPO）算法在此基础上进一步优化，进一步提出约束，使得满足该约束条件的样本可以稳定提升策略性能。最后，虽然TRPO十分优秀，但是大量的计算使其效率不高，因此进行简化提出Proximal
Policy Optimization（PPO）算法。</p>
<p><strong>基于值函数的强化学习</strong>：通过递归，求解bellman
方程维护Q值（离散列表或者神经网络），每次选择动作会选择该状态下对应Q值最大的动作。使得期望奖励值最大。</p>
<p><strong>基于策略的强化学习</strong>：不再通过价值函数确定动作，而是直接学习策略本身，通过一组参数<span
class="math inline"><em>θ</em></span>对策略进行参数化，并通过神经网络优化<span
class="math inline"><em>θ</em></span>。</p>
<span id="more"></span>
<p>Reference: * 十分推荐的博客，其延伸阅读有很多关于策略梯度的资料<a
href="https://towardsdatascience.com/policy-gradients-in-reinforcement-learning-explained-ecec7df94245">Policy
Gradients In Reinforcement Learning Explained</a> *
基于策略强化学习的开篇鼻祖<a
href="https://link.springer.com/article/10.1007/BF00992696">Simple
Statistical Gradient-Following Algorithms for Connectionist
Reinforcement Learning</a> * 自然梯度算法<a
href="https://ieeexplore.ieee.org/abstract/document/6790500">Natural
Gradient Works Efficiently in Learning</a> * 对自然梯度算法很好的总结<a
href="https://arxiv.org/pdf/2209.01820.pdf">Natural Policy Gradients In
Reinforcement Learning Explained</a> * CMU深度强化学习<a
href="https://www.andrew.cmu.edu/course/10-403/#readings">课程主页</a><a
href="https://cmudeeprl.github.io/403_website/lectures/">GitHub地址</a>
* <a href="https://arxiv.org/abs/1502.05477">Trust Region Policy
Optimization</a> * <a href="https://arxiv.org/abs/1707.06347">Proximal
Policy Optimization Algorithms</a> * <a
href="https://sham.seas.harvard.edu/publications/approximately-optimal-approximate-reinforcement-learning">Approximately
Optimal Approximate Reinforcement Learning</a> * Berkeley深度强化学习<a
href="https://rail.eecs.berkeley.edu/deeprlcourse-fa17/">课程主页</a> *
<a
href="https://www.telesens.co/2018/06/09/efficiently-computing-the-fisher-vector-product-in-trpo/">Efficiently
Computing the Fisher Vector Product in TRPO</a> * 代码库<a
href="https://spinningup.openai.com/en/latest/index.html">Spinning Up in
Deep RL</a></p>
<h1
id="policy-approximation-methods-moving-to-stochastic-policies">Policy
approximation methods : Moving to stochastic policies</h1>
<p>在策略近似的方法中，忽略传统的价值函数，直接调整策略本身。通过<span
class="math inline"><em>θ</em></span>（可能是神经网络参数）参数化策略<span
class="math inline"><em>π</em><sub><em>θ</em></sub></span>。</p>
<p>需要解决的问题： 1. 如何评估策略的质量 2. 如何更新<span
class="math inline"><em>θ</em></span></p>
<p>策略梯度算法有很多种，这篇文章聚焦于likelihood ratio policy
gradients。这种算法的核心思想是将策略转化为一种概率分布<span
class="math inline"><em>π</em><sub><em>θ</em></sub>(<em>a</em>|<em>s</em>) = <em>P</em>(<em>a</em>|<em>s</em>; <em>θ</em>)</span>，从而返回的不是一个单一的结果，而是动作的分布概率，然后进行采样。</p>
<h2 id="establishing-the-objective-function">Establishing the objective
function</h2>
<p>在进行一系列决策之后，得到状态-动作轨迹<span
class="math inline"><em>τ</em> = (<em>s</em><sub>1</sub>, <em>a</em><sub>1</sub>⋯<em>s</em><sub><em>T</em></sub>, <em>a</em><sub><em>T</em></sub>)</span>，每一条轨迹有相应的概率<span
class="math inline"><em>P</em>(<em>τ</em>)</span>和积累回报<span
class="math inline"><em>R</em>(<em>τ</em>) = ∑<em>γ</em><sup><em>t</em></sup><em>R</em><sub><em>t</em></sub></span>（<span
class="math inline"><em>γ</em></span>是折扣率，<span
class="math inline"><em>R</em><sub><em>t</em></sub></span>是<span
class="math inline"><em>t</em></span>时刻回报），同时定义目标函数：</p>
<p><span class="math display">$$\begin{align}
J(\theta)&amp;=\mathbb{E}_{\tau \sim\pi_{\theta}}R(\tau)=\sum_\tau
P(\tau;\theta)R(\tau) \\
\max_{\theta}J(\theta)&amp;=\max_{\theta}E_{\tau
\sim\pi_{\theta}}R(\tau)=\max_{\theta}\sum_\tau P(\tau;\theta)R(\tau)
\end{align}$$</span></p>
<h2 id="defining-trajectory-probabilities">Defining trajectory
probabilities</h2>
<p>接下来的主要任务是如何计算<span
class="math inline"><em>P</em>(<em>τ</em>; <em>θ</em>)</span>。</p>
<p>需要处理两类概率： * <strong>策略概率分布</strong>：<span
class="math inline"><em>π</em><sub><em>θ</em></sub>(<em>a</em>|<em>s</em>) = <em>P</em>(<em>a</em>|<em>s</em>; <em>θ</em>)</span>，描述在给定状态与参数下，动作的概率。
* <strong>概率转移分布</strong>：<span
class="math inline"><em>P</em>(<em>s</em><sub><em>t</em> + 1</sub>|<em>s</em><sub><em>t</em></sub>, <em>a</em><sub><em>t</em></sub>)</span>。在相同的状态下，作出相同的动作，环境也会以概率返回不同的状态。该参数描述在相同环境中，同一动作，下一状态分布的几率。</p>
<p>轨迹<span class="math inline"><em>τ</em></span>在策略<span
class="math inline"><em>π</em><sub><em>θ</em></sub>(<em>a</em>|<em>s</em>)</span>下发生的概率定义为：
<span class="math display">$$\begin{align}
P(\tau;\theta)=\left[\prod_{t=0}^T P(s_{t+1}|s_t,a_t)\cdot
\pi_{\theta}(a_t|s_t) \right]
\end{align}$$</span></p>
<h2 id="deriving-the-policy-gradient">Deriving the policy gradient</h2>
<p>为了得到<span
class="math inline">max<sub><em>θ</em></sub><em>J</em>(<em>θ</em>)</span>，可以利用求极值的方法（一阶导数为零），方法采用牛顿梯度迭代法。</p>
<p>为了优化<span
class="math inline"><em>θ</em></span>，计算目标参数<span
class="math inline"><em>J</em>(<em>θ</em>)</span>对<span
class="math inline"><em>θ</em></span>的导数。</p>
<p><span class="math display">$$\begin{align}
\nabla_{\theta}J(\theta) &amp;= \nabla_{\theta} \mathbb{E}_{\tau
\sim\pi_{\theta}}R(\tau)\\
&amp;= \sum_\tau \nabla_{\theta} P(\tau;\theta)R(\tau) \\
&amp;= \sum_\tau P(\tau;\theta)\frac{\nabla_{\theta}
P(\tau;\theta)}{P(\tau;\theta)}R(\tau) \\
&amp;= \sum_\tau P(\tau;\theta) \nabla_{\theta} \ln P(\tau;\theta)
R(\tau) \\
&amp;= \mathbb{E}_{\tau \sim\pi_{\theta}} R(\tau)  \nabla_{\theta} \ln
P(\tau;\theta) \\
&amp;= \mathbb{E}_{\tau \sim\pi_{\theta}} R(\tau)
\nabla_{\theta}\ln\left[\prod_{t=0}^T P(s_{t+1}|s_t,a_t)\cdot
\pi_{\theta}(a_t|s_t) \right] \\
&amp;= \mathbb{E}_{\tau \sim\pi_{\theta}} R(\tau)
\left[\nabla_{\theta}\sum_{t=0}^T \ln P(s_{t+1}|s_t,a_t)+
\nabla_{\theta}\sum_{t=0}^T\ln\pi_{\theta}(a_t|s_t) \right] \\
&amp;= \mathbb{E}_{\tau \sim\pi_{\theta}} R(\tau)
\nabla_{\theta}\sum_{t=0}^T\ln\pi_{\theta}(a_t|s_t) \\
\end{align}$$</span></p>
<p>直接计算存在困难，需要近似处理： <span
class="math display">$$\begin{align}
\nabla_{\theta}J(\theta) &amp;= \mathbb{E}_{\tau \sim\pi_{\theta}}
R(\tau)  \nabla_{\theta} \ln P(\tau;\theta) \\
&amp;\approx \frac{1}{m}\sum_{i=0}^m R(\tau^i)  \nabla_{\theta} \ln
P(\tau^i;\theta) \\
&amp;= \frac{1}{m}\sum_{i=0}^m R(\tau^i) \sum_{t^i=0}^{T^i}
\nabla_{\theta} \ln \pi_{\theta}(a_{t^i}|s_{t^i})\\
&amp;\approx \frac{1}{n} \sum_{i=1}^n R(t^i) \nabla_{\theta} \ln
\pi_{\theta}(a_{t^i}|s_{t^i})
\end{align}$$</span></p>
<p>现在梯度完全可以计算，只需要给出策略<span
class="math inline"><em>π</em><sub><em>θ</em></sub></span>的定义，就可以计算出<span
class="math inline">∇<sub><em>θ</em></sub><em>J</em>(<em>θ</em>)</span>，从而用策略梯度更新规则：
<span class="math display">$$\begin{align}
\theta \leftarrow \theta + \alpha \nabla_{\theta}J(\theta)
\end{align}$$</span></p>
<h2 id="examples-softmax-and-gaussian-policies">Examples: Softmax and
Gaussian policies</h2>
<p>为了说明以上策略的可行性，下面给出离散空间与连续空间的两个例子。其中<span
class="math inline"><em>ϕ</em>(<em>s</em>, <em>a</em>)</span>一个包含基本信息的向量，包含当前状态的信息与动作信息，<span
class="math inline"><em>θ</em></span>为权重因子。假设一个最简单的网络：<span
class="math inline"><em>ϕ</em>(<em>s</em>, <em>a</em>)<sup><em>T</em></sup> ⋅ <em>θ</em></span>，乘积结果就是对当前状态与动作的评估。</p>
<p>基于以上假设，下面两种常见策略： * Softmax策略
对于离散动作空间，多使用Softmax策略。定义如下： <span
class="math display">$$\begin{align}
  \pi_{\theta}(a|s) &amp;= \frac{e^{\phi(s,a)^T \cdot
\theta}}{\sum_{a'\in A}e^{\phi(s,a)^T \cdot \theta}}
  \end{align}$$</span></p>
<p>对应策略的梯度为： <span class="math display">$$\begin{align}
  \nabla_\theta \ln \pi_{\theta}(a|s) = \phi(s,a)- \sum_{a'\in
A}\pi_\theta(a|s)\phi(s,a')
  \end{align}$$</span></p>
<ul>
<li>高斯策略 对于连续动作空间，经常使用高斯策略。定义如下： <span
class="math display">$$\begin{align}
\pi_{\theta}(a|s) &amp;=
\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(a-\mu_\theta)^2}{2\sigma^2}}
\end{align}$$</span> 其中<span
class="math inline"><em>μ</em><sub><em>θ</em></sub></span>是正态分布的均值，<span
class="math inline"><em>σ</em><sub><em>θ</em></sub></span>是标准差（这里假设为一个不依赖于<span
class="math inline"><em>θ</em></span>的超参），<font color='red'>实践中均值和方差（一般生成的是对数方差）均是由神经网络生成，具体论述参见VAE相关内容</font>。对应的策略梯度为：
<span class="math display">$$\begin{align}
\nabla_\theta \ln\pi_{\theta}(a|s) =
\frac{(a-\mu_\theta)\phi(s)}{\sigma^2}
\end{align}$$</span></li>
</ul>
<h2 id="loss-functions-and-algorithmic-implementation-reinforce">Loss
functions and Algorithmic implementation (REINFORCE)</h2>
<p>在实际的计算中不需要计算梯度，只需要设置损失函数，计算机自动求导就行（<span
class="math inline"><em>r</em></span>是reward）： <span
class="math display">$$\begin{align}
\cal L(a,s,r) = -\ln(\pi_\theta(a|s))r
\end{align}$$</span></p>
<figure>
<img src="./REINFORCE.png" alt="REINFORCE" />
<figcaption aria-hidden="true">REINFORCE</figcaption>
</figure>
<h1 id="natural-gradients">Natural Gradients</h1>
<p>尽管自然梯度已被TRPO和PPO等算法超越，但掌握它的基本原理对于理解这些当代RL算法至关重要。</p>
<h2 id="the-problems-with-first-order-policy-gradients">The problems
with first-order policy gradients</h2>
<p>传统策略梯度算法只是提供了参数的更新方向，没有直接说明更新的步长。下面是众所周知的策略梯度更新方程：
<span class="math display">$$\begin{align}
\theta \leftarrow \theta + \alpha \nabla_{\theta}J(\theta)
\end{align}$$</span> 传统方法基于目标函数梯度<span
class="math inline">∇<sub><em>θ</em></sub><em>J</em>(<em>θ</em>)</span>与步长因子<span
class="math inline"><em>α</em></span>。会导致以下两个常见的问题：</p>
<ul>
<li>Overshooting:更新直接错过目标。虽然在有监督学习中不成问题，可以通过逐步修改更新率接近目标值。但是在强化学习中，可能会因为新的值导致3梯度消失。
<img src="./figure1.png" alt="overshoot" /></li>
<li>Undershooting:步长因子<span
class="math inline"><em>α</em></span>过小，无法收敛到目标位置。</li>
</ul>
<p>但是，并不能简单的限制更新步长<span
class="math inline">||<em>Δ</em><em>θ</em>||</span>（Euclidian
distance），例如： <span class="math display">$$\begin{align}
\Delta \theta^* = \arg\max_{||\Delta\theta||&lt;\epsilon}J(\theta+\Delta
\theta)
\end{align}$$</span> 因为在不同的参数中，<span
class="math inline"><em>θ</em></span>对于步长的敏感性不同。</p>
<figure>
<img src="./figure2.png" alt="fig2" />
<figcaption aria-hidden="true">fig2</figcaption>
</figure>
<h2 id="capping-the-difference-between-policies">Capping the difference
between policies</h2>
<p>因为参数对于步长的敏感性不同，因此采用KL散度衡量参数变化前后对分布的影响，将参数<span
class="math inline"><em>θ</em></span>的变化限制在一定的范围内。 <span
class="math display">$$\begin{align}
D_{KL}(\pi_\theta||\pi_{\theta+\Delta \theta}) = \sum_{x\in \mathcal
x}\pi_\theta(x)\ln
\left(\frac{\pi_\theta(x)}{\pi_{\theta+\Delta\theta}(x)}\right)
\end{align}$$</span></p>
<p>调整后的更新策略限制为： <span class="math display">$$\begin{align}
\Delta \theta^* = \arg\max_{D_{KL}(\pi_\theta||\pi_{\theta+\Delta
\theta})&lt;\epsilon}J(\theta+\Delta \theta)
\end{align}$$</span></p>
<p>这样处理之后，在参数空间进行更新，同时也能保证策略本身变化不会十分剧烈。但是遇到一个问题，在计算KL散度的时候需要对所有动作空间进行运算，这会对计算带来问题，下面是简化方法。</p>
<p>使用Lagrangian方法，将约束项变成惩罚项： <span
class="math display">$$\begin{align}
\Delta \theta^* = \arg\max_{\Delta\theta}J(\theta+\Delta
\theta)-\lambda(D_{KL}(\pi_\theta||\pi_{\theta+\Delta \theta})-\epsilon)
\end{align}$$</span></p>
<p>进行Taylor展开，为了记号统一，以下将<span
class="math inline"><em>θ</em> = <em>θ</em><sub><em>o</em><em>l</em><em>d</em></sub> + <em>Δ</em><em>θ</em></span>：
<span class="math display">$$\begin{align}
\Delta \theta^* &amp;\approx \arg\max_{\Delta\theta}\left[
J(\theta_{old})+ \nabla_\theta J(\theta)|_{\theta=\theta_{old}}\cdot
\Delta\theta-\frac{1}{2}\lambda(\Delta\theta^T \nabla_{\theta}^2
D_{KL}(\pi_{\theta_{old}}||\pi_{\theta}|_{\theta=\theta_{old}
})\Delta\theta+\lambda \epsilon\right]\\
\end{align}$$</span></p>
<p>这里只计算<span
class="math inline"><em>D</em><sub><em>K</em><em>L</em></sub></span>的二阶项，因为其零阶与一阶项均为零，证明如下：
<span class="math display">$$\begin{align}
D_{KL}(p_{\theta_{old}}||p_{\theta}）&amp;\approx
D_{KL}(p_{\theta_{old}}||p_{\theta_{old}})+\Delta\theta^T\nabla_{\theta}D_{KL}(p_{\theta_{old}}|p_{\theta})+\frac{1}{2}\Delta\theta^T
\nabla_{\theta}^2
D_{KL}(\pi_{\theta_{old}}||\pi_{\theta}|_{\theta=\theta_{old}
})\Delta\theta \\
\nabla_{\theta}D_{KL}(p_{\theta_{old}}|p_{\theta})|_{\theta=\theta_{old}}
&amp; = -\nabla_{\theta}\mathbb{E}_{x\sim p_{\theta_{old}}}\ln
p_{\theta}(x)|_{\theta=\theta_{old}}+\nabla_{\theta}\mathbb{E}_{x\sim
p_{\theta_{old}}}\ln p_{\theta_{old}}(x)|_{\theta=\theta_{old}} \\
&amp;= -\mathbb{E}_{x\sim p_{\theta_{old}}}\nabla_{\theta}\ln
p_{\theta}(x)|_{\theta=\theta_{old}} \\
&amp;= -\mathbb{E}_{x\sim
p_{\theta_{old}}}\frac{1}{p_{\theta_{old}}}\nabla_{\theta}
p_{\theta}(x)|_{\theta=\theta_{old}} \\
&amp;= -\int_x p_{\theta_{old}}\frac{1}{p_{\theta_{old}}}\nabla_{\theta}
p_{\theta}(x)|_{\theta=\theta_{old}} \\
&amp;= -\int_x \nabla_{\theta} p_{\theta}(x)|_{\theta=\theta_{old}} \\
&amp;= -\nabla_{\theta} \int_x p_{\theta}(x)|_{\theta=\theta_{old}} \\
&amp;= 0 \\
\nabla_{\theta}^2
D_{KL}(\pi_{\theta_{old}}||\pi_{\theta}|_{\theta=\theta_{old}
})|_{\theta=\theta_{old}} &amp;= -\mathbb{E}_{x\sim
p_{\theta_{old}}}\nabla_{\theta}^2\ln
p_{\theta}(x)|_{\theta=\theta_{old}}  \\
&amp;= -\mathbb{E}_{x\sim
p_{\theta_{old}}}\nabla_{\theta}\left(\frac{\nabla_{\theta}
p_{\theta}(x)}{p_{\theta}(x)}\right)|_{\theta=\theta_{old}}  \\
&amp;= -\mathbb{E}_{x\sim p_{\theta_{old}}}\nabla_{\theta}\ln
p_{\theta}\nabla_{\theta}\ln p_{\theta}^T|_{\theta=\theta_{old}}  \\
\end{align}$$</span></p>
<p><font color='red'>二阶导数可以表述为Hessian matrix，等价于Fisher
information matrix（这两个矩阵有什么样的含义？）。</font></p>
<p><span class="math display">$$\begin{align}
F(\theta) &amp;= \mathbb{E}_{\theta}\nabla_{\theta}\ln
p_{\theta}\nabla_{\theta}\ln p_{\theta}^T\\
F(\theta_{old}) &amp;=\nabla^2_\theta
D_{KL}(p_{\theta_{old}}||p_{\theta})|_{\theta=\theta_{old}} \\
D_{KL}(p_{\theta_{old}}||p_{\theta}）&amp;\approx
\frac{1}{2}\Delta\theta^T F(\theta_{old})\Delta\theta \\
&amp;= \frac{1}{2}(\theta-\theta_{old})^T
F(\theta_{old})(\theta-\theta_{old}) \\
\end{align}$$</span></p>
<p>根据以上证明可知： <span class="math display">$$\begin{align}
\Delta \theta^* &amp;\approx \arg\max_{\Delta\theta}\left[
J(\theta_{old})+ \nabla_\theta J(\theta)|_{\theta=\theta_{old}}\cdot
\Delta\theta-\frac{1}{2}\lambda(\Delta\theta^T \nabla_{\theta}^2
D_{KL}(\pi_{\theta_{old}}||\pi_{\theta}|_{\theta=\theta_{old}
})\Delta\theta+\lambda \epsilon \right ]\\
&amp;= \arg\max_{\Delta\theta} \left[ \nabla_\theta
J(\theta)|_{\theta=\theta_{old}}\cdot
\Delta\theta-\frac{1}{2}\lambda\Delta\theta^T
F(\theta_{old})\Delta\theta\right]\\
\end{align}$$</span></p>
<p>计算梯度为零的点： <span class="math display">$$\begin{align}
0 &amp;= \frac{\partial}{\partial \Delta \theta} \left[ \nabla_\theta
J(\theta)|_{\theta=\theta_{old}}\cdot
\Delta\theta-\frac{1}{2}\lambda\Delta\theta^T
F(\theta_{old})\Delta\theta\right] \\
&amp;= \nabla_\theta J(\theta)|_{\theta=\theta_{old}}-\frac{1}{2}\lambda
F(\theta_{old})\Delta\theta \\
\Delta\theta &amp;= \frac{2}{\lambda}F^{-1}(\theta_{old})\nabla_\theta
J(\theta)|_{\theta=\theta_{old}}
\end{align}$$</span></p>
<p>其中<span
class="math inline">$\frac{1}{\lambda}$</span>是一个常数，可以收缩进学习率<span
class="math inline"><em>α</em></span>。并且根据对更新步长的限制关系，可以得到学习率表达式：
<span class="math display">$$\begin{align}
D_{KL}(p_{\theta_{old}}||p_{\theta}）&amp;\approx
\frac{1}{2}(\Delta\theta)^T F(\theta_{old})(\Delta \theta) \\
&amp;= \frac{1}{2}(\alpha g_N)^T F(\theta_{old})(\alpha g_N)
&lt;\epsilon \\
\alpha &amp;=\sqrt{\frac{2\epsilon}{(g_N^TFg_N)}} \\
\end{align}$$</span> 其中<span
class="math inline"><em>g</em><sub><em>N</em></sub> = <em>F</em><sup>−1</sup>(<em>θ</em>)∇<sub><em>θ</em></sub><em>J</em>(<em>θ</em>)</span>。自然梯度与更新权重可写为：
<span class="math display">$$\begin{align}
\tilde\nabla J(\theta) &amp;= F^{-1}(\theta)\nabla_\theta J(\theta) \\
\Delta \theta &amp;= \alpha \tilde\nabla J(\theta) \\
\theta &amp;= \theta_{old}+\alpha \tilde\nabla J(\theta)
\end{align}$$</span></p>
<p>该方案的核心思想在于通过引入KL散度，对不同参数的步长进行限制，缓解了Overshoot与Undershoot问题。</p>
<h2 id="algorithm">Algorithm</h2>
<figure>
<img src="./natural_gradients.png" alt="Algorithm" />
<figcaption aria-hidden="true">Algorithm</figcaption>
</figure>
<p>自然梯度方法在两个方面不同于传统的策略梯度算法： *
考虑到策略对局部变化的敏感性，策略梯度由逆Fisher矩阵校正，而传统的梯度方法假定更新为欧几里得距离。
* 更新步长 <span class="math inline"><em>α</em></span>
具有适应梯度和局部敏感性的动态表达式，确保无论参数化如何，策略变化幅度为
<span
class="math inline"><em>ϵ</em></span>。在传统方法中，通常设置为一些标准值，如<span
class="math inline">0.1</span>或<span
class="math inline">0.01</span>。</p>
<p>但是这个算法也存在缺陷： *
Taylor提供了一个局域二阶近似，<font color='red'>这会导致Hessian可能非正定（为什么？）。</font>
* Fisher information matrix
计算量过大，尤其是神经网络这种大量参数的情况。</p>
<h1 id="trust-region-policy-optimization">Trust Region Policy
Optimization</h1>
<p>Trust Region Policy
Optimization（TRPO）算法保证了策略梯度算法每次更新始终会提升策略。</p>
<p>定义<span
class="math inline"><em>η</em></span>为期望折扣奖励（此处符号发生改变）：
<span class="math display">$$\begin{align}
\eta(\pi)=\mathbb{E}_{s_0, a_0, \ldots}\left[\sum_{t=0}^{\infty}
\gamma^t r\left(s_t\right)\right]
\end{align}$$</span></p>
<p>定义状态-动作价值函数<span
class="math inline"><em>Q</em><sub><em>π</em></sub>(<em>s</em><sub><em>t</em></sub>, <em>a</em><sub><em>t</em></sub>)</span>和价值函数<span
class="math inline"><em>V</em><sub><em>π</em></sub></span>，以及优势函数<span
class="math inline"><em>A</em><sub><em>π</em></sub></span>:</p>
<p><span class="math display">$$\begin{align}
Q_\pi\left(s_t, a_t\right)&amp;=\mathbb{E}_{s_{t+1}, a_{t+1},
\ldots}\left[\sum_{l=0}^{\infty} \gamma^l r\left(s_{t+l}\right)\right]
\\ V_\pi\left(s_t\right)&amp;=\mathbb{E}_{a_t, s_{t+1},
\ldots}\left[\sum_{l=0}^{\infty} \gamma^l r\left(s_{t+l}\right)\right]
\\
A_\pi(s, a)&amp;=Q_\pi(s, a)-V_\pi(s) \\
\quad a_t \sim \pi\left(a_t \mid s_t\right), &amp;s_{t+1} \sim
P\left(s_{t+1} \mid s_t, a_t\right) \text { for } t \geq 0
\end{align}$$</span></p>
<p>其中优势函数，是在给定的策略和状态下，计算特定动作<span
class="math inline"><em>a</em></span>的期望累积奖励与总体期望值（该状态的期望奖励）的差值。</p>
<p>下面的式子表达了策略<span
class="math inline"><em>π</em></span>与优势策略<span
class="math inline"><em>π̃</em></span>之间的差异（详细证明参见原始论文附录A）：
<span class="math display">$$\begin{equation}
\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{s_0, a_0, \cdots \sim
\tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^t A_\pi\left(s_t,
a_t\right)\right]
\end{equation}$$</span> 其中<span
class="math inline"><em>a</em><sub><em>t</em></sub></span>的采样概率为<span
class="math inline"><em>π̃</em>(⋅|<em>s</em><sub><em>t</em></sub>)</span>，<span
class="math inline"><em>s</em><sub><em>t</em></sub></span>的采样概率依赖于<span
class="math inline"><em>ρ</em><sub><em>π</em></sub></span><strong>这里本质上是重要性采样</strong>:
<span
class="math display"><em>ρ</em><sub><em>π</em></sub>(<em>s</em>) = <em>P</em>(<em>s</em><sub>0</sub> = <em>s</em>) + <em>γ</em><em>P</em>(<em>s</em><sub>1</sub> = <em>s</em>) + <em>γ</em><sup>2</sup><em>P</em>(<em>s</em><sub>2</sub> = <em>s</em>) + …</span>
因此将式改写为： <span class="math display">$$
\begin{align}
\eta(\tilde{\pi}) &amp; =\eta(\pi)+\sum_{t=0}^{\infty} \sum_s
P\left(s_t=s \mid \tilde{\pi}\right) \sum_a \tilde{\pi}(a \mid s)
\gamma^t A_\pi(s, a) \\
&amp; =\eta(\pi)+\sum_s \sum_{t=0}^{\infty} \gamma^t P\left(s_t=s \mid
\tilde{\pi}\right) \sum_a \tilde{\pi}(a \mid s) A_\pi(s, a) \\
&amp; =\eta(\pi)+\sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a \mid
s) A_\pi(s, a) \\
&amp;= \eta(\pi)+\sum_s \rho_{\tilde{\pi}}(s) \sum_a
\pi(a|s)\frac{\tilde{\pi}(a \mid s)}{\pi(a|s)} A_\pi(s, a) \\
&amp;=
\eta(\pi)+\mathbb{E}_{s\sim\rho_{\theta_{old}},a\sim\pi_{\theta_{old}}}\left[\frac{\tilde{\pi}(a
\mid s)}{\pi(a|s)} A_\pi(s, a)\right]
\end{align}
$$</span> 如果能够保证<span
class="math inline">∑<sub><em>a</em></sub><em>π̃</em>(<em>a</em> ∣ <em>s</em>)<em>A</em><sub><em>π</em></sub>(<em>s</em>, <em>a</em>) ≥ 0</span>，策略将会始终得以提升或者等价，然而并不能保证为非负，因为一些动作可能导致<span
class="math inline"><em>A</em></span>为负。而且由于<span
class="math inline"><em>ρ</em><sub><em>π̃</em></sub></span>的存在，使得很难直接去优化，因此采用近似，用<span
class="math inline"><em>ρ</em><sub><em>π</em></sub></span>替换<span
class="math inline"><em>ρ</em><sub><em>π̃</em></sub></span>： <span
class="math display">$$\begin{align}
L_\pi(\tilde{\pi})=\eta(\pi)+\sum \rho_\pi(s) \sum \tilde{\pi}(a \mid s)
A_\pi(s, a)
\end{align}$$</span>
可以证明该近似在一阶导数下是精确的，存在以下的关系<font color='red'>proof
it</font>： <span class="math display">$$
\begin{align}
L_{\pi_{\theta_0}}\left(\pi_{\theta_0}\right) &amp;
=\eta\left(\pi_{\theta_0}\right) \\
\left.\nabla_\theta
L_{\pi_{\theta_0}}\left(\pi_\theta\right)\right|_{\theta=\theta_0} &amp;
=\left.\nabla_\theta
\eta\left(\pi_\theta\right)\right|_{\theta=\theta_0}
\end{align}
$$</span>
从实际含义上可以理解，如果策略不变，那么前后策略应当是相同的。第二部分保证，只要能提升<span
class="math inline"><em>L</em><sub><em>π</em><sub><em>θ</em><sub>0</sub></sub></sub></span>，也会提升<span
class="math inline"><em>η</em></span>。</p>
<p>文献<a
href="https://sham.seas.harvard.edu/publications/approximately-optimal-approximate-reinforcement-learning">Approximately
Optimal Approximate Reinforcement
Learning</a><font color='red'>有时间看看</font>，给出了一种混合更新策略，并且证明了更新后的策略的下界。</p>
<p><span class="math display">$$
\begin{aligned}
\pi_{\text {new }}(a \mid s)&amp;=(1-\alpha) \pi_{\text {old }}(a \mid
s)+\alpha \pi^{\prime}(a \mid s) \\
\eta\left(\pi_{\text {new }}\right) &amp; \geq L_{\pi_{\text {old
}}}\left(\pi_{\text {new }}\right)-\frac{2 \epsilon
\gamma}{(1-\gamma)^2} \alpha^2 \\
\epsilon&amp;=\max _s\left|\mathbb{E}_{a \sim \pi^{\prime}(a \mid
s)}\left[A_\pi(s, a)\right]\right|
\end{aligned}
$$</span></p>
<h2
id="monotonic-improvement-guarantee-for-general-stochastic-policies">Monotonic
Improvement Guarantee for General Stochastic Policies</h2>
<p>文章 Approximately Optimal Approximate Reinforcement Learning
提出的混合策略过强不够普适用，同时不便于实践，不具备一般性。TRPO算法在此基础上进行弱化，但是同时要保证下界不变。</p>
<p>引入总变差（Total Variation Distance）：<span
class="math inline">$D_{T V}(p \| q)=\frac{1}{2}
\sum_i\left|p_i-q_i\right|$</span>，将<span
class="math inline"><em>α</em></span>定义如下：</p>
<p>$$ $$</p>
<p>上面计算下界的证明参见原文附录。根据TV与KL散度的关系<font color='red'>证明它</font>：
<span class="math display">$$\begin{align}
D_{T V}(p \| q)^2 \leq D_{\mathrm{KL}}(p \| q)
\end{align}$$</span> 令<span
class="math inline"><em>D</em><sub><em>T</em><em>V</em></sub><sup>max</sup>(<em>π</em>, <em>π̃</em>)<sup>2</sup> = max<sub><em>s</em></sub><em>D</em><sub>KL</sub>(<em>π</em>(⋅|<em>s</em>)∥<em>π̃</em>(⋅|<em>s</em>))</span>，可以得到如下的下界：
<span class="math display">$$
\begin{align}
\eta(\tilde{\pi}) &amp;\geq L_\pi(\tilde{\pi})-C D_{\mathrm{KL}}^{\max
}(\pi, \tilde{\pi}) \\
C&amp;=\frac{4 \epsilon \gamma}{(1-\gamma)^2}
\end{align}
$$</span></p>
<p>令<span
class="math inline"><em>M</em><sub><em>i</em></sub>(<em>π</em>) = <em>L</em><sub><em>π</em><sub><em>i</em></sub></sub>(<em>π</em>) − <em>C</em><em>D</em><sub><em>K</em><em>L</em></sub><sup>max</sup>(<em>π</em><sub><em>i</em></sub>, <em>π</em>)</span>，利用这个下界证明单调性。</p>
<p>首先: <span
class="math display"><em>η</em>(<em>π</em><sub><em>i</em> + 1</sub>) ≥ <em>M</em><sub><em>i</em></sub>(<em>π</em><sub><em>i</em> + 1</sub>)</span>
并且: <span
class="math display"><em>η</em>(<em>π</em><sub><em>i</em></sub>) = <em>M</em><sub><em>i</em></sub>(<em>π</em><sub><em>i</em></sub>)</span>
则： <span
class="math display"><em>η</em>(<em>π</em><sub><em>i</em> + 1</sub>) − <em>η</em>(<em>π</em><sub><em>i</em></sub>) ≥ <em>M</em><sub><em>i</em></sub>(<em>π</em><sub><em>i</em> + 1</sub>) − <em>M</em>(<em>π</em><sub><em>i</em></sub>)</span>
如果新策略<span
class="math inline"><em>π</em><sub><em>i</em> + 1</sub></span>能使得<span
class="math inline"><em>M</em><sub><em>i</em></sub></span>最大，就有<span
class="math inline"><em>M</em><sub><em>i</em></sub>(<em>π</em><sub><em>i</em> + 1</sub>) − <em>M</em>(<em>π</em><sub><em>i</em></sub>) ≥ 0</span>，从而就保证了策略必然稳步提升。</p>
<p>这样通过优化下界便可以使得策略得到稳定的提升。</p>
<p>算法流程如下： <img src="./TRPO.png" alt="TRPO" /></p>
<h2 id="optimization-of-parameterized-policies">Optimization of
Parameterized Policies</h2>
<p>下面将要基于以上的理论基础，在有限的空间以及任意参数下，给出具体的算法。首先更改符号注记，用<span
class="math inline"><em>θ</em></span>表示重要的参数，而非策略<span
class="math inline"><em>π</em><sub><em>θ</em></sub></span>。<span
class="math inline"><em>η</em>(<em>θ</em>) := <em>η</em>(<em>π</em><sub><em>θ</em></sub>), <em>L</em><sub><em>θ</em></sub>(<em>θ̃</em>) := <em>L</em><sub><em>π</em><sub><em>θ</em></sub></sub>(<em>π</em><sub><em>θ̃</em></sub>), <em>D</em><sub><em>K</em><em>L</em></sub>(<em>θ</em>∥<em>θ̃</em>) := <em>D</em><sub><em>K</em><em>L</em></sub>(<em>π</em><sub><em>θ</em></sub>∥<em>π</em><sub><em>θ̃</em></sub>)</span></p>
<p>可以将： <span class="math display">$$\begin{equation}
\underset{\theta}{\operatorname{maximize}}\left[L_{\theta_{\text {old
}}}(\theta)-C D_{\mathrm{KL}}^{\max }\left(\theta_{\text {old }},
\theta\right)\right]
\end{equation}$$</span> 改写为： <span
class="math display">$$\begin{aligned}
&amp; \underset{\theta}{\operatorname{maximize}} L_{\theta_{\text {old
}}}(\theta) \\
&amp; \text { subject to } D_{\mathrm{KL}}^{\max }\left(\theta_{\text
{old }}, \theta\right) \leq \delta
\end{aligned}$$</span> 由于<span
class="math inline"><em>D</em><sub>KL</sub><sup>max</sup></span>的计算过于麻烦，采用带权重的近似替代：
<span
class="math display"><em>D̄</em><sub>KL</sub><sup><em>ρ</em></sup>(<em>θ</em><sub>1</sub>, <em>θ</em><sub>2</sub>) := 𝔼<sub><em>s</em> ∼ <em>ρ</em></sub>[<em>D</em><sub>KL</sub>(<em>π</em><sub><em>θ</em><sub>1</sub></sub>(⋅ ∣ <em>s</em>)∥<em>π</em><sub><em>θ</em><sub>2</sub></sub>(⋅ ∣ <em>s</em>))]</span></p>
<p>基于此最终解决的优化问题形式是： <span
class="math display">$$\begin{equation}
\begin{aligned}
&amp; \underset{\theta}{\operatorname{maximize}} L_{\theta_{\text {old
}}}(\theta) \\
&amp; \text { subject to } \bar{D}_{\mathrm{KL}}^{\rho_{\theta_{\text
{old }}}}\left(\theta_{\text {old }}, \theta\right) \leq \delta .
\end{aligned}
\end{equation}$$</span></p>
<h2 id="connections-with-natural-gradients">Connections with Natural
Gradients</h2>
<p>从TRPO算法最终解决问题的形式可以看出，这是一种针对特定形式优化问题的解决方案。通过计算惩罚因子，从而保证在每一次更新迭代之后就能保证策略得到稳定的提升。为了实现这个目标，引入了两个重要的机制：
* Advantage Estimates *
检查机制：随机采样并不能确定结果是否得到提升，但是可以检查采样结果，选取确实提升效果的样本，对于其他样本则直接放弃。</p>
<p>对于自然梯度算法，也是其一个特例。 <span
class="math display">$$\begin{equation}
\begin{aligned}
&amp;
\underset{\theta}{\operatorname{maximize}}\left[\left.\nabla_\theta
L_{\theta_{\text {old }}}(\theta)\right|_{\theta=\theta_{\text {old }}}
\cdot\left(\theta-\theta_{\text {old }}\right)\right] \\
&amp; \text { subject to } \frac{1}{2}\left(\theta_{\text {old
}}-\theta\right)^T A\left(\theta_{\text {old
}}\right)\left(\theta_{\text {old }}-\theta\right) \leq \delta \\
&amp; \text { where } A\left(\theta_{\text {old }}\right)_{i j}= \left.
\frac{\partial}{\partial \theta_i} \frac{\partial}{\partial \theta_j}
\mathbb{E}_{s \sim \rho_\pi}\left[D_{\mathrm{KL}}\left(\pi\left(\cdot
\mid s, \theta_{\text {old }}\right) \| \pi(\cdot \mid s,
\theta)\right)\right]\right|_{\theta=\theta_{\text {old }}}
\end{aligned}
\end{equation}$$</span> 其中参数更新为<span
class="math inline">$\theta_{\text {new }}=\theta_{\text {old
}}+\left.\frac{1}{\lambda} A\left(\theta_{\text {old }}\right)^{-1}
\nabla_\theta L(\theta)\right|_{\theta=\theta_{\text {old
}}}$</span>，利用TRPO算法，可以限制惩罚项<span
class="math inline">$\frac{1}{\lambda}$</span>，虽然这只是一个很小的算法参数，但是在大问题上显著提升了算法表现能力。</p>
<p><font color='red'>缺失共轭梯度法，与算法简化的实现</font></p>
<figure>
<img src="./TRPO2.png" alt="TRPO structure" />
<figcaption aria-hidden="true">TRPO structure</figcaption>
</figure>
<h1 id="proximal-policy-optimization-algorithms">Proximal Policy
Optimization Algorithms</h1>
<p>TRPO算法保证了稳定提升，但是由于计算二阶梯度，运算量十分巨大。为了解决运算量大的问题，提出了一种近似方法，通过对惩罚因子进行限制，大幅减少运算量。</p>
<h2 id="clipped-surrogate-objective">Clipped Surrogate Objective</h2>
<p>在TRPO中优化的目标为： <span class="math display">$$\begin{equation}
L^{C P I}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(a_t \mid
s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}
\hat{A}_t\right]=\hat{\mathbb{E}}_t\left[r_t(\theta) \hat{A}_t\right]
\end{equation}$$</span> 其中<span
class="math inline">$r_t(\theta)=\frac{\pi_\theta\left(a_t \mid
s_t\right)}{\pi_{\theta_{\text {old }}}}$</span>，CPI指的是conservative
policy iteration。</p>
<p>调整TRPO的优化目标为： <span class="math display">$$\begin{equation}
L^{C L I P}(\theta)=\hat{\mathbb{E}}_t\left[\min \left(r_t(\theta)
\hat{A}_t, \operatorname{clip}\left(r_t(\theta), 1-\epsilon,
1+\epsilon\right) \hat{A}_t\right)\right]
\end{equation}$$</span> 其中<span
class="math inline"><em>ϵ</em></span>为超参。该式的第一项表示<span
class="math inline"><em>L</em><sup><em>C</em><em>P</em><em>I</em></sup></span>，第二项利用剪切权重调整了优势函数，将<span
class="math inline"><em>r</em><sub><em>t</em></sub></span>限制在一个范围内。最后返回剪切与非剪切之后的较小值，这样将会返回一个下界。</p>
<figure>
<img src="./CLIP.png" alt="clip" />
<figcaption aria-hidden="true">clip</figcaption>
</figure>
<p>算法框架如下：</p>
<figure>
<img src="./PPO_Clip.png" alt="PPO Clip" />
<figcaption aria-hidden="true">PPO Clip</figcaption>
</figure>
<h2 id="adaptive-kl-penalty-coefficient">Adaptive KL Penalty
Coefficient</h2>
<p>另一种优化方案是针对惩罚项： <span
class="math display">$$\begin{equation}
L^{K L P E N}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(a_t
\mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}
\hat{A}_t-\beta \operatorname{KL}\left[\pi_{\theta_{\text {old
}}}\left(\cdot \mid s_t\right), \pi_\theta\left(\cdot \mid
s_t\right)\right]\right]
\end{equation}$$</span> 然后计算KL散度，从而确定惩罚因子的大小。 <span
class="math display">$$
\begin{aligned}
&amp;d=\hat{\mathbb{E}}_t\left[\operatorname{KL}\left[\pi_{\theta_{\text
{old }}}\left(\cdot \mid s_t\right), \pi_\theta\left(\cdot \mid
s_t\right)\right]\right] \\
&amp; \quad-\text { If } d&lt;d_{\text {targ }} / 1.5, \beta \leftarrow
\beta / 2 \\
&amp; \quad-\text { If } d&gt;d_{\text {targ }} \times 1.5, \beta
\leftarrow \beta \times 2
\end{aligned}
$$</span> 其中<span
class="math inline">1.5, 2</span>是启发式得到，初始值<span
class="math inline"><em>β</em></span>也是一个超参，但是对结果影响不大。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Natural Gradients</tag>
        <tag>PPO</tag>
        <tag>TRPO</tag>
        <tag>Policy Gradients</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine learning renormalization group for statistical physics</title>
    <url>/2024/08/21/ai4sci/MLRG/MLRG/</url>
    <content><![CDATA[<p>通过构建三个网络学习实空间重整化过程中的参数变化。</p>
<p>Reference: * <a
href="https://doi.org/10.1088/2632-2153/ad0101">Machine learning
renormalization group for statistical physics</a></p>
<span id="more"></span>
<h1 id="network">Network</h1>
<figure>
<img src="./architecture.png" alt="architecture of algorithm" />
<figcaption aria-hidden="true">architecture of algorithm</figcaption>
</figure>
<p>一共有三个网络，其中两个是 RBM，另一个是深度网络。这两个 RBM
分别表示重整化前与重整化后的模型，可以给出波尔兹曼分布。在重整化变换前后分布应当是不变的，因此通过KL散度衡量这两个网络之间的距离，然而衡量KL散度存在困难，使用CD散度进行替代。</p>
<p>那么这两个网络之间的重整化参数是如何确定的呢？这里引入了三个网络
moderator，用以将第一个网络参数转化为第第二个网络的参数，是学习重整化流的主要网络。</p>
<h1 id="algorithm">Algorithm</h1>
<p>算法流程为： 1. 通过参数J进行采样（HMC），得到一些用于训练的 sample。
2. J 作为 fine-grained RBM 的参数。 3. 这些sample输入 fine-grained
RBM，产生根据其概率分布的sample1。 4. 通过 Moderator 将 J 变化为
J’，输入进 corase-grained RBM。 5. 将 sample1 输入 corase-grained
RBM，产生根据其概率分布的sample2。 6.
根据sample2和sample1之间的概率分布区别作为loss。 7. 利用loss训练
Moderator。 8. 将 J’ 作为下一次循环的 J。</p>
<h1 id="rg">RG</h1>
<figure>
<img src="./lattice.png" alt="lattice" />
<figcaption aria-hidden="true">lattice</figcaption>
</figure>
<p>这里的重整化变换见上图中的阴影部分，a、b分别表示变换前后，绿色格点是sample的位置，蓝色和红色表示相互作用J。</p>
<figure>
<img src="./tensor.png" alt="tensor" />
<figcaption aria-hidden="true">tensor</figcaption>
</figure>
<p>这里存在一个问题，两个不同的网络结构，相互作用是如何进行转化的？上图将这个相互用进行分解，将自旋、对称性、相互作用系数进行分解。</p>
<h1 id="result">Result</h1>
<figure>
<img src="./result.png" alt="result" />
<figcaption aria-hidden="true">result</figcaption>
</figure>
<p>绘制出 RG flow，并且能够准确找到鞍点。</p>
]]></content>
      <categories>
        <category>AI for Science</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Combinatorial Optimization Methods</tag>
      </tags>
  </entry>
  <entry>
    <title>Analytic and Algorithmic Solution of Random Satisfiability Problems</title>
    <url>/2024/07/09/Phys/science_3831989/RSP/</url>
    <content><![CDATA[<p>Parisi关于组合优化问题的分析。</p>
<p>reference: * <a href="https://www.jstor.org/stable/3831989">Analytic
and Algorithmic Solution of Random Satisfiability Problems</a></p>
<span id="more"></span>
<p>关于组合优化问题关心两类求解算法、理论分析，第二类理论分析具体为一类组合优化问题在不同实例的情况下，有哪些共性特征。</p>
<p>考虑热力学极限<span
class="math inline"><em>α</em> = <em>M</em>/<em>N</em></span>，其中<span
class="math inline"><em>N</em></span>是变量数，<span
class="math inline"><em>M</em></span>是子句数目，其中<span
class="math inline"><em>α</em></span>是一个恒定值。将其转化为统计物理问题，<span
class="math inline"><em>N</em></span>个布尔变量用二值<span
class="math inline"><em>s</em></span>表示，每个子句<span
class="math inline"><em>a</em></span>包含<span
class="math inline"><em>k</em></span>个变量（K-SAT问题）即<span
class="math inline"><em>k</em></span>个自旋相互作用，相互作用强度<span
class="math inline"><em>J</em> ∈ {−1, 1}</span>通过子句中的<span
class="math inline">¬</span>决定，将所有子句进行加和：</p>
<p><span class="math display">$$\begin{align}
H=\sum_a \prod_k \frac{1+J_a s_k}{2^k}
\end{align}$$</span></p>
<p>最终<span
class="math inline"><em>H</em></span>的值表示违反的关系的个数，当全部满足的时候<span
class="math inline"><em>H</em> = 0</span>。定义零温下的自由能关系：</p>
<p><span
class="math display">exp (−<em>N</em><em>y</em><em>Φ</em>(<em>y</em>)) = ∫𝕕<em>w</em>exp (<em>N</em>[<em>Σ</em>(<em>e</em>) − <em>y</em><em>e</em>])</span></p>
<p>其中<span class="math inline"><em>e</em></span>表示自由能，<span
class="math inline"><em>Σ</em></span>为对应自由能量的熵。为了计算<span
class="math inline"><em>Φ</em></span>，使用空腔的方法。</p>
<p><span class="math inline">$\begin{aligned}
\min _{s_2, \ldots, s_K}\left(H-\frac{1}{2} \sum_{j=2}^K h_j s_j\right)
=-\frac{1}{2}\left[a_J\left(h_2, \ldots, h_K\right)+s_1 u_J\left(h_2,
\ldots, h_K\right)\right]
\end{aligned}$</span></p>
<p><img src="./fig1.png" alt="Cavity" /> 上图为表示空腔的因子图。</p>
<figure>
<img src="./fig2.png" alt="phase diagram" />
<figcaption aria-hidden="true">phase diagram</figcaption>
</figure>
<p>相图可以用如上的结果表示，红线代表平均每个变量不满足的几率；绿线代表遍历性破缺线条；蓝线表示解的熵线。</p>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Spin Glass</tag>
        <tag>Combinatorial Optimization Methods</tag>
        <tag>Boltzmann Machine</tag>
        <tag>Replica Method</tag>
        <tag>Replica Symmetry Breaking</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning with Graphs</title>
    <url>/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/</url>
    <content><![CDATA[<p>图论（Graphs）是数学的一个分支，它研究的是图这种数学结构。图论中的图是由点（也称为顶点）和连接这些点的线（也称为边）组成的。点通常用来代表事物，而边则表示事物之间的关系。图可以是有向的，也可以是无向的，这取决于边是否有方向。</p>
<p>图论的研究内容包括图的各种性质，如度（一个点连接的边的数量）、连通性（图中的点是否都通过边相互可达）、路径（点与点之间的连线序列）等。图论不仅在数学领域内有广泛的应用，还在计算机科学、工程学、经济学等多个领域发挥着重要作用。</p>
<p>因多次发现与图论相关的内容，故在这里记录学习图论的笔记与想法。</p>
<p>参考文献： * <a
href="https://datawhalechina.github.io/grape-book/#/">图深度学习（葡萄书）</a>
* <a
href="https://jingboyang.github.io/stanford-cs224w-graph-ml/stanford_cs224w_graph_ml.pdf">The
Healthy Birds Trio, Jure Leskovec, Notes for Stanford CS224W Machine
Learning with Graphs, 2020</a> *
《图强化学习：原理与实践入门》谢文杰、周炜星，清华大学出版社
<font color='gray'>不推荐阅读，东拼西凑强行逻辑合理，很多内容点不到就止，同时不给深入阅读的材料。</font>
* <a href="http://cs-www.cs.yale.edu/homes/spielman/sagt/">Spectral and
Algebraic Graph Theory Incomplete Draft</a></p>
<p>感谢<a
href="https://github.com/datawhalechina/grape-book">Datawhale开源社区</a>提供学习平台与相关资源。</p>
<span id="more"></span>
<h1 id="why-graphs">Why Graphs</h1>
<p>最近频繁的看到图网络的相关内容，产生了好奇：图网络究竟有哪些优越性？</p>
<h2 id="数据结构">数据结构</h2>
<p>在拓扑结构上图明显具备更高的普适性质。语言可以理解为一维数据，图像可以理解为二维数据，这两种类型显然是图的一种特殊结构。对于复杂网络，如果使用NLP、CV等模型，则必须进行“近似”，而图网络就能囊括数据的信息。</p>
<p>这种“近似”因为要确定顺序，造成不同数据之间不等价，会破坏数据中的对称性。例如“茶也可以吃”，写成首尾相接的形式，无论从哪里开始读到结束，均保持语言上的合理性，并且意思一致。在这句话中重要的不同字之间的相对位置，而不是绝对位置。图在处理这类问题上具有优越性。</p>
<p>这种“近似”影响，在图像上更为明显。一张图像往往不会因其旋转、镜像变换而造成含义上的变化，而CNN等等网络不能理解这种对称。一般在进行训练的时候，都会使用数据增强的方法，尽可能让网络理解这种对称。</p>
<p>因此处理这种相对位置关键，绝对位置不关键的数据，图网络具备其先天的优势。</p>
<p>同时具备图这种拓扑结构，与许多问题等价。可以将其它问题转化为图上进行求解。例如对于物理中求解模型的配分函数：将配分函数转化为扭结展开，而扭结就是一种图；除此之外，图的形式与张量网络也极为相似，两者之间可能存在相关性。</p>
<p>许多优化问题，本身就是通过图结构描述，旅行商问题、最大割问题；其它优化问题虽然不是用图显式表达，但是也可以<a
href="https://renyixiong-ai.github.io/2024/04/15/Math/Ising_formulations_of_many_NP_problems/Ising_formulations_of_many_NP_problems/">转化为图结构</a>。</p>
<p>复杂网络和图的邻接矩阵可以任意的交换行和列，同样可以表示一个图或者网络，也就是图论中图同构的概念。邻接矩阵的行和列的交换知识重新对节点进行标号，并不会改变网络的结构和特征。图数据特征可以分为结构特征、属性特征、拓扑特征等，属性特征包含节点属性、连边属性和全局图属性。</p>
<h2 id="what-types-of-problems-have-graph-structured-data">What types of
problems have graph structured data?</h2>
<p>通常有三种类型的图任务： * graph-level In a graph-level task, our
goal is to predict the property of an entire graph. * node-level
Node-level tasks are concerned with predicting the identity or role of
each node within a graph. * edge-level Edge-level task get the
relationship between the nodes.</p>
<h1 id="graphs-with-complex-network">Graphs with Complex Network</h1>
<ul>
<li><a
href="https://distill.pub/2021/gnn-intro/">优秀的可视化网站</a></li>
<li>python中关于图的包<a
href="https://networkx.github.io/">Networkx</a></li>
</ul>
<h2 id="图的定义">图的定义</h2>
<h2 id="图的性质">图的性质</h2>
<h3 id="邻接节点neighbors">邻接节点（neighbors）</h3>
<h3 id="图的度degree">图的度（degree）</h3>
<h3 id="行走walk和路径path">行走（walk）和路径（path）</h3>
<h3
id="距离distance和直径diameter">距离（distance）和直径（diameter）</h3>
<h3
id="子图subgraph连通分量connected-component连通图conected-graph">子图（subgraph）、连通分量（connected
component）、连通图（conected graph）</h3>
<h3 id="聚类系数clustering-coefficient">聚类系数（Clustering
Coefficient）</h3>
<h3 id="接近中心度closeness-centrality">接近中心度（closeness
centrality）</h3>
<h2 id="图的连接表示">图的连接表示</h2>
<h3 id="邻接矩阵">邻接矩阵</h3>
<p>邻接矩阵（adjacency matrix）刻画点和点之间的关系。</p>
<p>给定一个图 <span
class="math inline"><em>G</em> = {<em>V</em>, <em>E</em>}</span> ,
其对应的邻接矩阵被记为 <span
class="math inline"><strong>A</strong> ∈ {0, 1}<sup><em>N</em> × <em>N</em></sup></span>
。<span
class="math inline"><strong>A</strong><sub><strong>i</strong>, j</sub> = 1</span>
表示存在从节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 到 <span
class="math inline"><em>v</em><sub><em>j</em></sub></span> 的边, <span
class="math inline"><strong>A</strong><sub><em>i</em>, <em>j</em></sub> = 0</span>
表示不存在从节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 到 <span
class="math inline"><em>v</em><sub><em>j</em></sub></span> 的边。</p>
<ul>
<li>在无向图中, 从节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 到 <span
class="math inline"><em>v</em><sub><em>j</em></sub></span> 的边存在,
意味着从节点 <span
class="math inline"><em>v</em><sub><em>j</em></sub></span> 到 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span>
的边也存在。因而无向图的邻接矩阵是对称的。</li>
<li>在无权图中, 各条边的权重被认为是等价的, 即认为各条边的权重为 1
。</li>
<li>对于有权图, 其对应的邻接矩阵通常被记为 <span
class="math inline"><strong>W</strong> ∈ R<sup><em>N</em> × <em>N</em></sup></span>,
其中 <span
class="math inline"><strong>W</strong><sub>i, j</sub> = <em>w</em><sub><em>i</em><em>j</em></sub></span>
表示从节 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 到 <span
class="math inline"><em>v</em><sub><em>j</em></sub></span>
的边的权重。若边不存在时, 边的权重为 0 。</li>
</ul>
<figure>
<img src="./2-1.png" alt="邻接矩阵ssss" />
<figcaption aria-hidden="true">邻接矩阵ssss</figcaption>
</figure>
<h3 id="关联矩阵">关联矩阵</h3>
<p>关联矩阵（incidence matrix）描述定点和边的关系。</p>
<p>给定一个图 <span
class="math inline"><em>G</em> = {<em>V</em>, <em>E</em>}</span>,
其对应的关联矩阵被记为<span
class="math inline"><strong>M</strong> ∈ {0, 1}<sup><em>N</em> × <em>M</em></sup></span>
。</p>
<ul>
<li><span
class="math inline"><strong>M</strong><sub><strong>i</strong>, j</sub> = 1</span>
表示节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 和边 <span
class="math inline"><em>e</em><sub><em>j</em></sub></span> 相连接, <span
class="math inline"><strong>M</strong><sub><strong>i</strong>, j</sub> = 0</span>
表示节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 和边 <span
class="math inline"><em>e</em><sub><em>j</em></sub></span>
不相连接。</li>
<li>与邻接矩阵不同, 关联矩阵描述的是定点和边之间的关系。</li>
</ul>
<figure>
<img src="./2-2.png" alt="关联矩阵" />
<figcaption aria-hidden="true">关联矩阵</figcaption>
</figure>
<h3 id="拉普拉斯矩阵">拉普拉斯矩阵</h3>
<p>拉普拉斯矩阵（Laplacian Matrix）：（也叫做 admittance matrix,
Kirchhoff matrix）给定一个图 <span
class="math inline"><em>G</em> = {<em>V</em>, <em>E</em>}</span>,
其邻接矩阵为 <span class="math inline"><em>A</em></span>, 其拉普拉斯矩阵
<span class="math inline"><em>L</em></span> 定义为 <span
class="math display"><strong>L</strong> = <strong>D</strong> − <strong>A</strong></span></p>
<p>其中 <span
class="math inline"><strong>D</strong> = diag (<strong>d</strong>(<strong>v</strong><sub>1</sub>), …, <strong>d</strong>(<strong>v</strong><sub><strong>N</strong></sub>))</span>
是度矩阵（表示每一个节点的度）。更具体地,
我们记拉普拉斯矩阵中每一个元素为 <span
class="math inline"><em>L</em><sub><em>i</em><em>j</em></sub></span>,
那么每一个元素可以被定义为 <span class="math display">$$
L_{i j}=\left\{\begin{array}{cl}
d_i, &amp; \text { if } i=j \\
-1, &amp; \text { if } i \neq j \text { and } v_i \text { adjacent with
} v_j \\
0, &amp; \text { otherwise }
\end{array}\right.
$$</span></p>
<figure>
<img src="./2-3.png" alt="拉普拉斯矩阵" />
<figcaption aria-hidden="true">拉普拉斯矩阵</figcaption>
</figure>
<p>拉普拉斯矩阵也有标准化的表示方法，即矩阵的主对角线元素为<span
class="math inline">1</span>： <span
class="math display">$$\begin{align}
L_{\text {norm }}=&amp; D^{-1 / 2} L D^{-1 / 2} \\
=&amp;\boldsymbol{I}_n-D^{-1 / 2} W D^{-1 / 2} \label{normal_laplace}\\
=&amp;\left\{\begin{array}{c}
1, i=j \\
-\frac{1}{\sqrt{d_i d_j}}, e_{i j} \in E \\
0, e_{i j} \notin E
\end{array}\right.
\end{align}$$</span></p>
<h2 id="图的类型">图的类型</h2>
<h3 id="图的拓扑结构">图的拓扑结构</h3>
<h3
id="同质图homogeneous-graph和异质图heterogeneous-graph">同质图（Homogeneous
Graph）和异质图（Heterogeneous Graph）</h3>
<h3 id="二分图-bipartite-graph">二分图 （bipartite graph）</h3>
<h2 id="节点指标">节点指标</h2>
<p>节点是构成图的关键因素，表示的是个体。 ### 节点的度
有向网络中节点<span class="math inline"><em>i</em></span>的度： * 出度
<span class="math inline">$k_i^{\text{out}}=\sum_{j=1}^N a_{ij}$</span>
* 入度 <span class="math inline">$k_i^{\text{in}}=\sum_{j=1}^N
a_{ji}$</span> * 节点的度 <span
class="math inline"><em>k</em><sub><em>i</em></sub> = <em>k</em><sub><em>i</em></sub><sup>in</sup> + <em>k</em><sub><em>i</em></sub><sup>out</sup></span></p>
<p>无向网络： * <span class="math inline">$k_i = \sum_{j=1}^N a_{ij} =
\sum_{j=1}^N a_{ji}$</span></p>
<h3 id="节点的强度">节点的强度</h3>
<p>加权网络中更准确刻画节点的中心性（重性质），必须要考虑边的权重。在有向网络中：
* 出度 <span class="math inline">$k_i^{\text{out}}=\sum_{j=1}^N
w_{ij}$</span> * 入度 <span
class="math inline">$k_i^{\text{in}}=\sum_{j=1}^N w_{ji}$</span> *
节点的度 <span
class="math inline"><em>k</em><sub><em>i</em></sub> = <em>k</em><sub><em>i</em></sub><sup>in</sup> + <em>k</em><sub><em>i</em></sub><sup>out</sup></span></p>
<h3 id="聚簇系数">聚簇系数</h3>
<p>聚簇系数（Clustering
Coefficient）刻画网络节点的邻居节点之间的连接紧密程度。节点<span
class="math inline"><em>i</em></span>的局部聚簇系数<span
class="math inline"><em>C</em><sub><em>c</em></sub><em>i</em></span>是它的相离节点之间的关系系数与它们所有可能存在的关系数量的比值：
<span class="math display">$$\begin{align}
C_c(i) = \frac{\sum_{i,j\in\mathcal{N}_i}a_{ij}}{k_i (k_i-1)}
\end{align}$$</span> 其中<span
class="math inline">𝒩<sub><em>i</em></sub></span>为节点<span
class="math inline"><em>i</em></span>的邻居节点集合，<span
class="math inline"><em>a</em><sub><em>i</em><em>j</em></sub></span>为邻接矩阵元素，<span
class="math inline"><em>k</em><sub><em>i</em></sub></span>表示节点<span
class="math inline"><em>i</em></span>相邻的边数。</p>
<p>整个网络的平均聚簇系数： <span class="math display">$$\begin{align}
C_c(i) = \frac{\sum_{i=1}^N C_c(i)}{N}
\end{align}$$</span></p>
<p>可以知道，星状网络的节点聚簇系数为<span
class="math inline">0</span>，全连接图的节点聚簇系数为<span
class="math inline">1</span>。</p>
<h2 id="网络连边指标">网络连边指标</h2>
<p>通过将原始图转化对偶图，原始图中的边成为对偶图中的节点，因此研究节点中心性和重要性的指标与方法同样适用于研究边。</p>
<h2 id="网络模体结构">网络模体结构</h2>
<p>网络模体（Network
Motif）是指在网络中重复出现的子网络的结构模式，被认为是在复杂网络中扮演重要功能角色的基本构建单元。</p>
<h1 id="graph-representation-learning">Graph Representation
Learning</h1>
<p>经典机器学习任务不能简单的迁移到图数据上，因此需要基于图类型数据开发新的算法（监督、无监督、强化学习）。图表示学习是其中的重要组件。</p>
<h2 id="图表示一般框架">图表示一般框架</h2>
<p>将图机器学习过程描述为学习一个函数映射的过程： <span
class="math display">$$\begin{align}
\boldsymbol{y} =&amp; f_\boldsymbol{w} (\boldsymbol{A}, \boldsymbol{X})
\\
\boldsymbol{y} =&amp; f_\boldsymbol{w} (\boldsymbol{A}, \boldsymbol{X},
\boldsymbol{X}_{\text{edge}})
\end{align}$$</span> 其中<span
class="math inline"><strong>y</strong></span>是标签，<span
class="math inline"><strong>w</strong></span>是模型参数，<span
class="math inline"><strong>A</strong><strong>X</strong></span>与<span
class="math inline"><strong>A</strong><strong>X</strong><strong>X</strong><sub>edge</sub></span>为图的相关信息。</p>
<p>可以构建基于均方差的损失函数： <span
class="math display">$$\begin{align}
\mathcal{L}(w)=&amp;\frac{1}{N}\sum_{k=1}^N \left(f_{w}(A_k, X_k)-y_k
\right) ^2 \\
\hat{\boldsymbol{w}}=&amp;\arg\max_{\boldsymbol{w}}\mathcal{L}\left(\boldsymbol{w}|(A_k,X_k,y_k)_{k=1,2,3\dots
N} \right)
\end{align}$$</span></p>
<p>自编码器架构，在图网络中广泛使用，通过encoder-decoder过程，将图信息从高维离散转化到低维稠密信息空间，在稠密信息空间计算相关特征值。其中encoder过是称为：图嵌入（Graph
Embedding）或者网络嵌入（Network Embedding）。</p>
<h3 id="编码器">编码器</h3>
<p>图嵌入模型的映射关系可用函数表示, 一般称之为编码器 <span
class="math inline"><em>f</em><sub>Encoder </sub></span>,
或者特征提取器: <span class="math display">$$\begin{align}
f_{\text {Encoder }}: V \rightarrow \mathbb{R}^D
\end{align}$$</span> 其中， <span class="math inline"><em>D</em></span>
表示低维嵌入空间的维度。编码器将网络节点映射为低维稠密向量。</p>
<p>下面给出一个浅层嵌入（shallow
Embedding）的例子，将所有节点向量表示成矩阵形式 <span
class="math inline"><em>Z</em></span>, 矩阵如下所示: <span
class="math display">$$
\begin{aligned}
&amp; \\
&amp; 1 \\
&amp; 2 \\
&amp; \vdots \\
&amp; D-1 \\
&amp; D
\end{aligned}\begin{array}{cccccc}
v_1 &amp; v_2 &amp; v_3 &amp; \cdots &amp; v_{N-1} &amp; v_N \\
z_{11} &amp; z_{12} &amp; z_{13} &amp; \cdots &amp; z_{1, N-1} &amp;
z_{1 N} \\
z_{21} &amp; z_{22} &amp; z_{23} &amp; \cdots &amp; z_{2, N-1} &amp;
z_{2 N} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots
\\
z_{D-1,1} &amp; z_{D-1,2} &amp; z_{D-1,3} &amp; \cdots &amp; z_{D-1,
N-1} &amp; z_{D-1, N} \\
z_{D 1} &amp; z_{D 2} &amp; z_{D 3} &amp; \cdots &amp; z_{D, N-1} &amp;
z_{D N}
\end{array}
$$</span></p>
<p>在节点属性向量矩阵中，第 <span class="math inline"><em>i</em></span>
行表示节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span>
的嵌入空间属性向量值,
属性向量表示节点的结构属性特征和语义属性特征的融合信息，可作为下游图机器学习任务的决策变量。因此，我们可定义编码器函数为
<span class="math display">$$\begin{align}
f_{\text {Encoder
}}\left(v_i\right)=\boldsymbol{Z}\left[\boldsymbol{e}_i\right]
\end{align}$$</span></p>
<p>其中 <span
class="math inline">[<em>e</em><sub><em>i</em></sub>]</span>
表示单位矩阵的第 <span class="math inline"><em>i</em></span>
列，即列向量长度为 <span class="math inline"><em>N</em></span>, 第 <span
class="math inline"><em>i</em></span> 个元素为 1 , 其他元素为 0 。矩阵
<span class="math inline"><em>Z</em></span> 和列向量 <span
class="math inline">[<strong>e</strong><sub><em>i</em></sub>]</span>
相乘，得到矩阵 <span class="math inline"><em>Z</em></span>的第 <span
class="math inline"><em>i</em></span> 列。 编码器映射函数 <span
class="math inline"><em>f</em><sub>Encoder </sub></span>
输出结果为节点对应的低维嵌入空间的坐标信息，或者称之为隐空间信息（潜变量空间）：
<span class="math display">$$\begin{align}
f_{\text {Encoder }}\left(v_i\right)=z_i=\left[z_{i 1}, z_{i 2}, \cdots,
z_{i D}\right]^{\top}
\end{align}$$</span></p>
<h3 id="解码器">解码器</h3>
<p>在图表示学习中，一般将解码器表示成函数形式： <span
class="math display">$$\begin{align}
f_{\text {Decoder }}: \mathbb{R}^D \times \mathbb{R}^D \rightarrow
\mathbb{R}^{+}
\end{align}$$</span></p>
<p>其中, <span
class="math inline">ℝ<sup>+</sup></span>表示解码器的输出。解码过程为信息还原的过程。一般而言,
现流行的一些图嵌入算法都选择重建原始网络结构信息。这样的好处在于为监督学习，有含标签的数据集，产生了高效的编码器，方便下游任务的使用。</p>
<p>为衡量与原始网络之间的差异大小，会选取一些特征值指标，如网络节点邻域信息或网络节点间相似性信息。最简单的情况则为基于编码信息预测节点之间的连边情况，或是节点之间的相似性测度
<span
class="math inline"><em>S</em>(<em>v</em><sub><em>i</em></sub>, <em>v</em><sub><em>j</em></sub>)</span>,
解码器可以表示为: <span class="math display">$$\begin{align}
f_{\text {Decoder }}\left(f_{\text {Encoder }}\left(v_i\right), f_{\text
{Encoder }}\left(v_j\right)\right)=f_{\text {Decoder
}}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) \approx S\left(v_i,
v_j\right)
\end{align}$$</span></p>
<p>公式表明, 越好的图表示向量或图嵌入属性 <span
class="math inline"><em>Z</em></span>,
能使得解码器更好地还原原始节点之间关联信息，如相似性或邻域结构。相似性测度
<span
class="math inline"><em>S</em>(<em>v</em><sub><em>i</em></sub>, <em>v</em><sub><em>j</em></sub>)</span>
可以作为先验知识，如邻接矩阵等。</p>
<h3 id="模型优化">模型优化</h3>
<p>编码器和解码器组合构成模型的整体框架和信息处理流程图,
而节点嵌入向量是需要最终学习的属性向量,
即为模型的可学习参数，也是可优化参数，因此设定损失函数为 <span
class="math display">$$\begin{align}
\mathcal{L}=\sum_{(i, j) \in E} f_{\text {Loss }}\left(f_{\text {Decoder
}}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right), S\left(v_i,
v_j\right)\right)
\end{align}$$</span></p>
<p>上式中，损失函数 <span class="math inline"><em>f</em><sub>Loss
</sub></span> 计算解码器输出值 <span
class="math inline"><em>f</em><sub>Decoder
</sub>(<em>z</em><sub><em>i</em></sub>, <em>z</em><sub><em>j</em></sub>)</span>
和相似性测度值 <span
class="math inline"><em>S</em>(<em>v</em><sub><em>i</em></sub>, <em>v</em><sub><em>j</em></sub>)</span>
之间的差异。差异测度函数有多种选择，如绝对值、均方差等。因此，最优化的图嵌入空间向量矩阵为:
<span class="math display">$$\begin{align}
\hat{\boldsymbol{Z}}=\arg \min _{\boldsymbol{Z}}
\mathcal{L}(\boldsymbol{Z} \mid G(V, E))
\end{align}$$</span></p>
<p>上式中，网络 <span
class="math inline"><em>G</em>(<em>V</em>, <em>E</em>)</span>
包含图嵌入算法需要的信息，如邻接矩阵和节点属性等。</p>
<h2 id="基于矩阵分解的图嵌入">基于矩阵分解的图嵌入</h2>
<p>图嵌入或网络嵌入是将复杂网络映射到一个维稠密空间。</p>
<figure>
<img src="./3-1.png" alt="图嵌入" />
<figcaption aria-hidden="true">图嵌入</figcaption>
</figure>
<h3 id="图分解方法">图分解方法</h3>
<p>图分解方法（GF），其解码器为： <span
class="math display"><em>f</em><sub>Decoder
</sub> = <em>z</em><sub><em>i</em></sub><sup><em>T</em></sup><em>z</em><sub><em>j</em></sub>,</span></p>
<p>表示节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 和 <span
class="math inline"><em>v</em><sub><em>j</em></sub></span>
在嵌入空间的属性向量 <span
class="math inline"><em>z</em><sub><em>i</em></sub></span> 和 <span
class="math inline"><em>z</em><sub><em>j</em></sub></span>
的内积，度量嵌入向量 <span
class="math inline"><em>z</em><sub><em>i</em></sub></span> 和 <span
class="math inline"><em>z</em><sub><em>j</em></sub></span>
之间的相似性。选择二范数作为 GF 方法的损失函数: <span
class="math display">ℒ = ∑<sub>(<em>i</em>, <em>j</em>) ∈ <em>E</em></sub>∥<strong>z</strong><sub><em>i</em></sub><sup><em>T</em></sup><strong>z</strong><sub><em>j</em></sub> − <strong>A</strong>[<em>i</em>, <em>j</em>]∥<sub>2</sub><sup>2</sup>,</span></p>
<p>损失函数度量解码器输出值和相似性测度 <span
class="math inline"><strong>A</strong>[<em>i</em>, <em>j</em>]</span>
之间的差异。</p>
<h3 id="grarep方法">GraRep方法</h3>
<p>GraRep方法在 GF 方法的基础上考虑了次邻接、第三邻接等信息。例如<span
class="math inline"><strong>A</strong><sup>2</sup>[<em>i</em>, <em>j</em>], <strong>A</strong><sup>3</sup>[<em>i</em>, <em>j</em>]⋯<strong>A</strong><sup><em>k</em></sup>[<em>i</em>, <em>j</em>]</span>表示图的二阶邻接、三阶邻接、<span
class="math inline"><em>k</em></span>阶邻接，表示距离为<span
class="math inline">2、3、<em>k</em></span>的节点连接情况。</p>
<h3 id="hope方法">HOPE方法</h3>
<p>高阶邻接保留嵌入算法（HOPE）是一种能够有向图的不对称传递性的网络嵌入算法。</p>
<p>高阶邻近性源自不对称传递性, 采用机器学习中最优化算法最小化损失函数:
<span class="math display">$$\begin{align}
\min \left\|\boldsymbol{S}-\boldsymbol{U}^{\boldsymbol{s}}
\boldsymbol{U}^{t^{\top}}\right\|^2
\end{align}$$</span></p>
<p>公式中, <span class="math inline"><strong>S</strong></span>
是网络高阶相似性测度指标值, <span
class="math inline"><strong>U</strong><sup><em>s</em></sup></span> 和
<span class="math inline"><strong>U</strong><sup><em>t</em></sup></span>
是每个网络节点嵌入向量组成的嵌入属性矩阵。</p>
<h2 id="基于随机游走的图嵌入算法">基于随机游走的图嵌入算法</h2>
<p>矩阵嵌入的算法局限在于矩阵的运算和复杂度较高，针对大型网络表现出一定局限性。为了提升效率以及高效采样，采用随机游走的思路。</p>
<h3 id="deepwalk算法">DeepWalk算法</h3>
<p>原始文献<a href="https://arxiv.org/abs/1403.6652">DeepWalk: Online
Learning of Social Representations</a></p>
<p>DeepWalk方法通过随机游走在网络上采样，获得网络节点和连边构成的序列数据，构造不同网络节点之间的相似性特征或者领域结构特征。</p>
<figure>
<img src="./3-3.png" alt="DeepWalk" />
<figcaption aria-hidden="true">DeepWalk</figcaption>
</figure>
<figure>
<img src="./3-4.png" alt="DeepWalk" />
<figcaption aria-hidden="true">DeepWalk</figcaption>
</figure>
<p>先初始化参数矩阵<span class="math inline"><em>Φ</em></span>为<span
class="math inline">|<em>V</em>|×<em>d</em></span>的矩阵。然后随机采样，减少网络之间的关联性。</p>
<p>从网络节点<span
class="math inline"><em>v</em><sub><em>i</em></sub></span>开始，采样长度为<span
class="math inline"><em>t</em></span>的样本，表示为： <span
class="math display">$$\begin{align}
W_{v_i} =&amp; \{ v_i,v_{i+1},v_{i+2}\cdots v_{i+t}\} \\
=&amp; \text{RandomWalk}(G, v_i, t)
\end{align}$$</span>
基于随机游走进行数据的采样，算法定义节点之间的领域关系。一般而言，如果两个节<span
class="math inline"><em>u</em><sub><em>k</em></sub>, <em>v</em><sub><em>j</em></sub></span>相邻，其同时出现在一条随机游走路径上的概率较高，因此目标函数定义为：
<span class="math display">$$\begin{align}
J(\Phi)=&amp;-\log{P_\Phi(u_k|v_j)} \\
P_\Phi(u_k|v_j) =&amp; \frac{\exp(\Phi_k^T\Phi_j)}{\sum_{u\in
V}\exp(\Phi_u^T\Phi_j)}
\end{align}$$</span></p>
<p>将嵌入词向量之间的关联性转化为概率值P_(u_k|v_j)，计算两节点同时出现在同一序列的概率值。</p>
<h3 id="node2vec算法">Node2Vec算法</h3>
<figure>
<img src="./3-5.png" alt="Node2Vec" />
<figcaption aria-hidden="true">Node2Vec</figcaption>
</figure>
<p>如上图，从节点<span
class="math inline"><em>v</em></span>跳转到候选节点<span
class="math inline"><em>t</em>, <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub></span>的概率为：
<span class="math display">$$\begin{align}
\pi_{vx}=\alpha_{pq}(t,x)w_{vx}
\end{align}$$</span> 其中<span
class="math inline"><em>w</em><sub><em>v</em><em>x</em></sub></span>表示节点<span
class="math inline"><em>v</em></span>与<span
class="math inline"><em>x</em></span>之间的权重，<span
class="math inline"><em>α</em><sub><em>p</em><em>q</em></sub>(<em>t</em><em>x</em>)</span>表示在超参数<span
class="math inline"><em>p</em>, <em>q</em></span>的情形下，从节点<span
class="math inline"><em>v</em></span>跳转到<span
class="math inline"><em>x</em></span>的非归一化概率，如上图所示。</p>
<p>其中包含超参数<span
class="math inline"><em>p</em>, <em>q</em></span>，如上图所示，<span
class="math inline"><em>v</em></span>节点是从<span
class="math inline"><em>t</em></span>转移过来跳回原节点<span
class="math inline"><em>t</em></span>的概率利用超参数<span
class="math inline"><em>p</em></span>设置概率为<span
class="math inline">$\frac{1}{p}$</span>；如果跳的节点是<span
class="math inline"><em>t</em></span>的二阶相邻节点，则利用超参数<span
class="math inline"><em>q</em></span>设置概率为<span
class="math inline">$\frac{1}{q}$</span>；如果同时是<span
class="math inline"><em>v</em>, <em>t</em></span>的超参数，则直接设为<span
class="math inline">1</span>。</p>
<p>由于这种超参数的设置，使得整个网络的采样是有偏的。</p>
<h1 id="图神经网络">图神经网络</h1>
<p>图神经网络（Graph Neural Networks,
GNN）模型的目标是挖掘图特征数据或信息，解构蕴含于复杂图结构和图网络以及相关特征属性的信息。在机器学习领域，该过程抽象的表达为表示学习过程，将高维、复杂信息抽象为低维的特征向量，获得特征向量更适应接下来的任务处理。本质上图神经网络就是图表示学习（Graph
Representation Learning, GPL）的有效方法。</p>
<h2 id="消息传递神经网络">消息传递神经网络</h2>
<p>消息传递神经网络（Message Passing Neural Networks,
MPNN），汇聚邻居节点的信息以更新网络节点的特征向量。主要包含领域信息汇聚函数、信息更新函数和信息池化函数等。本身还是出于消息传递算法的特性，考虑较短的圈，忽略更远节点的影响。</p>
<p>这种直接在图上进行信息处理的方法，也被叫做空间域图卷积神经网络。其中信息的聚合与信息的池化，对应卷积神经网络里面的卷积与池化。</p>
<h3 id="领域信息汇聚函数">领域信息汇聚函数</h3>
<p>网络 <span
class="math inline"><em>G</em> = (<em>V</em>, <em>E</em>)</span>
作为图神经网络模型的输入数据, 用于计算节点的隐含特征表示向量 <span
class="math inline"><strong>x</strong><sub><em>i</em></sub><sup>(<em>k</em>)</sup></span>
。节点 <span class="math inline"><em>i</em></span>
的特征属性表示向量是基于节点 <span class="math inline"><em>i</em></span>
的邻居节点集合 <span class="math inline">𝒩<sub><em>i</em></sub></span>
计算而来，具体计算公式如下： <span class="math display">$$\begin{align}
\boldsymbol{x}_{-i}^{(k)}=\text { Aggregate }_{j \in \mathcal{N}_i}
f^{(k)}\left(\boldsymbol{x}_j^{(k-1)}\right)
\end{align}$$</span></p>
<p>其中， <span
class="math inline"><strong>x</strong><sub>−<em>i</em></sub><sup>(<em>k</em>)</sup></span>
表示聚合了节点 <span class="math inline"><em>i</em></span>
的邻居节点集合 <span class="math inline">𝒩<sub><em>i</em></sub></span>
的属性表示，但是不包括网络节点 <span
class="math inline"><em>i</em></span> 自身的属性信息。</p>
<p>其中上标<span
class="math inline">(<em>k</em>)</span>表示汇聚节点深度，例如<span
class="math inline"><em>k</em> = 1</span>表示计算最近邻。聚合函数
Aggregate
可以有多种形式，除了传统的求和、均值等，也可以是神经网络。另一方面，信息处理函数<span
class="math inline"><em>f</em><sup>(<em>k</em>)</sup></span>也可以嵌套一些深度神经网络模型。</p>
<h3 id="信息更新函数">信息更新函数</h3>
<p>在图神经网络模型中，节点 <span class="math inline"><em>i</em></span>
聚合了邻居信息后，可以更新节点 <span
class="math inline"><em>i</em></span> 自身的特征属性: <span
class="math display"><strong>x</strong><sub><em>i</em></sub><sup>(<em>k</em>)</sup> = Update (<strong>x</strong><sub><em>i</em></sub><sup>(<em>k</em> − 1)</sup>, <strong>x</strong><sub>−<em>i</em></sub><sup>(<em>k</em>)</sup>),</span></p>
<p>其中， <span
class="math inline"><strong>x</strong><sub><em>i</em></sub><sup>(<em>k</em> − 1)</sup></span>
表示第 <span class="math inline"><em>k</em> − 1</span> 次迭代计算中节点
<span class="math inline"><em>i</em></span> 的特征表示。 Update
函数可以有多种形式, 最常见的更新函数 Update 是深度神经网络模型,
也可以是简单的求和函数 (SUM) 或者平均值函数 (MEAN) 等,
或者进行简单的向量拼接。</p>
<h3 id="图信息池化函数">图信息池化函数</h3>
<p>对节点属性进行池化操作: <span
class="math display"><strong>x</strong><sup>(<em>K</em>)</sup> = Pool<sub>∀<em>i</em></sub>(<strong>x</strong><sub><em>i</em></sub><sup>(<em>K</em>)</sup>).</span></p>
<p>最简单的池化操作就是将所有网络节点属性求和： <span
class="math display"><strong>x</strong><sup>(<em>K</em>)</sup> = SUM<sub>∀<em>i</em></sub>(<strong>x</strong><sub><em>i</em></sub><sup>(<em>K</em>)</sup>)</span></p>
<h3 id="应用">应用</h3>
<p>使用MPNN进行分子性质预测的项目<a
href="https://github.com/chemprop/chemprop/tree/main">chemprop</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AtomMessagePassing</span>(MessagePassing, HyperparametersMixin):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;A :class:`AtomMessagePassing` encodes a batch of molecular graphs by passing messages along</span></span><br><span class="line"><span class="string">    atoms.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It implements the following operation:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        h_v^&#123;(0)&#125; &amp;= \tau \left( \mathbf&#123;W&#125;_i(x_v) \right) \\</span></span><br><span class="line"><span class="string">        m_v^&#123;(t)&#125; &amp;= \sum_&#123;u \in \mathcal&#123;N&#125;(v)&#125; h_u^&#123;(t-1)&#125; \mathbin\Vert e_&#123;uv&#125; \\</span></span><br><span class="line"><span class="string">        h_v^&#123;(t)&#125; &amp;= \tau\left(h_v^&#123;(0)&#125; + \mathbf&#123;W&#125;_h m_v^&#123;(t-1)&#125;\right) \\</span></span><br><span class="line"><span class="string">        m_v^&#123;(T)&#125; &amp;= \sum_&#123;w \in \mathcal&#123;N&#125;(v)&#125; h_w^&#123;(T-1)&#125; \\</span></span><br><span class="line"><span class="string">        h_v^&#123;(T)&#125; &amp;= \tau \left (\mathbf&#123;W&#125;_o \left( x_v \mathbin\Vert m_&#123;v&#125;^&#123;(T)&#125; \right)  \right),</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`\tau` is the activation function; :math:`\mathbf&#123;W&#125;_i`, :math:`\mathbf&#123;W&#125;_h`, and</span></span><br><span class="line"><span class="string">    :math:`\mathbf&#123;W&#125;_o` are learned weight matrices; :math:`e_&#123;vw&#125;` is the feature vector of the</span></span><br><span class="line"><span class="string">    bond between atoms :math:`v` and :math:`w`; :math:`x_v` is the feature vector of atom :math:`v`;</span></span><br><span class="line"><span class="string">    :math:`h_v^&#123;(t)&#125;` is the hidden representation of atom :math:`v` at iteration :math:`t`;</span></span><br><span class="line"><span class="string">    :math:`m_v^&#123;(t)&#125;` is the message received by atom :math:`v` at iteration :math:`t`; and</span></span><br><span class="line"><span class="string">    :math:`t \in \&#123;1, \dots, T\&#125;` is the number of message passing iterations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    d_v : int, default=DEFAULT_ATOM_FDIM</span></span><br><span class="line"><span class="string">        the feature dimension of the vertices</span></span><br><span class="line"><span class="string">    d_e : int, default=DEFAULT_BOND_FDIM</span></span><br><span class="line"><span class="string">        the feature dimension of the edges</span></span><br><span class="line"><span class="string">    d_h : int, default=DEFAULT_HIDDEN_DIM</span></span><br><span class="line"><span class="string">        the hidden dimension during message passing</span></span><br><span class="line"><span class="string">    bias : bool, defuault=False</span></span><br><span class="line"><span class="string">        if `True`, add a bias term to the learned weight matrices</span></span><br><span class="line"><span class="string">    depth : int, default=3</span></span><br><span class="line"><span class="string">        the number of message passing iterations</span></span><br><span class="line"><span class="string">    undirected : bool, default=False</span></span><br><span class="line"><span class="string">        if `True`, pass messages on undirected edges</span></span><br><span class="line"><span class="string">    dropout : float, default=0.0</span></span><br><span class="line"><span class="string">        the dropout probability</span></span><br><span class="line"><span class="string">    activation : str, default=&quot;relu&quot;</span></span><br><span class="line"><span class="string">        the activation function to use</span></span><br><span class="line"><span class="string">    d_vd : int | None, default=None</span></span><br><span class="line"><span class="string">        the dimension of additional vertex descriptors that will be concatenated to the hidden features before readout</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        d_v: <span class="built_in">int</span> = DEFAULT_ATOM_FDIM,</span></span><br><span class="line"><span class="params">        d_e: <span class="built_in">int</span> = DEFAULT_BOND_FDIM,</span></span><br><span class="line"><span class="params">        d_h: <span class="built_in">int</span> = DEFAULT_HIDDEN_DIM,</span></span><br><span class="line"><span class="params">        bias: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        depth: <span class="built_in">int</span> = <span class="number">3</span>,</span></span><br><span class="line"><span class="params">        dropout: <span class="built_in">float</span> = <span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        activation: <span class="built_in">str</span> | Activation = Activation.RELU,</span></span><br><span class="line"><span class="params">        undirected: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        d_vd: <span class="built_in">int</span> | <span class="literal">None</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        V_d_transform: ScaleTransform | <span class="literal">None</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        graph_transform: GraphTransform | <span class="literal">None</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        <span class="comment"># layers_per_message: int = 1,</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.save_hyperparameters()</span><br><span class="line">        <span class="variable language_">self</span>.hparams[<span class="string">&quot;cls&quot;</span>] = <span class="variable language_">self</span>.__class__</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.W_i, <span class="variable language_">self</span>.W_h, <span class="variable language_">self</span>.W_o, <span class="variable language_">self</span>.W_d = <span class="variable language_">self</span>.setup(d_v, d_e, d_h, d_vd, bias)</span><br><span class="line">        <span class="variable language_">self</span>.depth = depth</span><br><span class="line">        <span class="variable language_">self</span>.undirected = undirected</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.tau = get_activation_function(activation)</span><br><span class="line">        <span class="variable language_">self</span>.V_d_transform = V_d_transform <span class="keyword">if</span> V_d_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        <span class="variable language_">self</span>.graph_transform = graph_transform <span class="keyword">if</span> graph_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">output_dim</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.W_d.out_features <span class="keyword">if</span> <span class="variable language_">self</span>.W_d <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="variable language_">self</span>.W_o.out_features</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        d_v: <span class="built_in">int</span> = DEFAULT_ATOM_FDIM,</span></span><br><span class="line"><span class="params">        d_e: <span class="built_in">int</span> = DEFAULT_BOND_FDIM,</span></span><br><span class="line"><span class="params">        d_h: <span class="built_in">int</span> = DEFAULT_HIDDEN_DIM,</span></span><br><span class="line"><span class="params">        d_vd: <span class="built_in">int</span> | <span class="literal">None</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        bias: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="built_in">tuple</span>[nn.Module, nn.Module, nn.Module, nn.Module | <span class="literal">None</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;setup the weight matrices used in the message passing update functions</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        d_v : int</span></span><br><span class="line"><span class="string">            the vertex feature dimension</span></span><br><span class="line"><span class="string">        d_e : int</span></span><br><span class="line"><span class="string">            the edge feature dimension</span></span><br><span class="line"><span class="string">        d_h : int, default=300</span></span><br><span class="line"><span class="string">            the hidden dimension during message passing</span></span><br><span class="line"><span class="string">        d_vd : int | None, default=None</span></span><br><span class="line"><span class="string">            the dimension of additional vertex descriptors that will be concatenated to the hidden</span></span><br><span class="line"><span class="string">            features before readout, if any</span></span><br><span class="line"><span class="string">        bias: bool, default=False</span></span><br><span class="line"><span class="string">            whether to add a learned bias to the matrices</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        W_i, W_h, W_o, W_d : tuple[nn.Module, nn.Module, nn.Module, nn.Module | None]</span></span><br><span class="line"><span class="string">            the input, hidden, output, and descriptor weight matrices, respectively, used in the</span></span><br><span class="line"><span class="string">            message passing update functions. The descriptor weight matrix is `None` if no vertex</span></span><br><span class="line"><span class="string">            dimension is supplied</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        W_i = nn.Linear(d_v, d_h, bias)</span><br><span class="line">        W_h = nn.Linear(d_e + d_h, d_h, bias)</span><br><span class="line">        W_o = nn.Linear(d_v + d_h, d_h)</span><br><span class="line">        W_d = nn.Linear(d_h + d_vd, d_h + d_vd) <span class="keyword">if</span> d_vd <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> W_i, W_h, W_o, W_d</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">initialize</span>(<span class="params">self, bmg: BatchMolGraph</span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;initialize the message passing scheme by calculating initial matrix of hidden features&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.W_i(bmg.V[bmg.edge_index[<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">message</span>(<span class="params">self, H: Tensor, bmg: BatchMolGraph</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Calculate the message matrix&quot;&quot;&quot;</span></span><br><span class="line">        H = torch.cat((H, bmg.E), dim=<span class="number">1</span>)</span><br><span class="line">        index_torch = bmg.edge_index[<span class="number">1</span>].unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, H.shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="built_in">len</span>(bmg.V), H.shape[<span class="number">1</span>], dtype=H.dtype, device=H.device).scatter_reduce_(</span><br><span class="line">            <span class="number">0</span>, index_torch, H, reduce=<span class="string">&quot;sum&quot;</span>, include_self=<span class="literal">False</span></span><br><span class="line">        )[bmg.edge_index[<span class="number">0</span>]]          <span class="comment">#按照指定方式进行归约，本值为aggregate函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, M_t, H_0</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Calcualte the updated hidden for each edge&quot;&quot;&quot;</span></span><br><span class="line">        H_t = <span class="variable language_">self</span>.W_h(M_t)           <span class="comment">#通过神经网络从其它节点处传来的Message</span></span><br><span class="line">        H_t = <span class="variable language_">self</span>.tau(H_0 + H_t)     <span class="comment">#结合该节点，并进行激活函数激活</span></span><br><span class="line">        H_t = <span class="variable language_">self</span>.dropout(H_t)       <span class="comment">#dropout层，增加表达能力</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> H_t</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, bmg: BatchMolGraph, V_d: Tensor | <span class="literal">None</span> = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Encode a batch of molecular graphs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        bmg: BatchMolGraph</span></span><br><span class="line"><span class="string">            a batch of :class:`BatchMolGraph`s to encode</span></span><br><span class="line"><span class="string">        V_d : Tensor | None, default=None</span></span><br><span class="line"><span class="string">            an optional tensor of shape ``V x d_vd`` containing additional descriptors for each atom</span></span><br><span class="line"><span class="string">            in the batch. These will be concatenated to the learned atomic descriptors and</span></span><br><span class="line"><span class="string">            transformed before the readout phase.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        Tensor</span></span><br><span class="line"><span class="string">            a tensor of shape ``V x d_h`` or ``V x (d_h + d_vd)`` containing the encoding of each</span></span><br><span class="line"><span class="string">            molecule in the batch, depending on whether additional atom descriptors were provided</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        bmg = <span class="variable language_">self</span>.graph_transform(bmg)   <span class="comment">#输入图</span></span><br><span class="line">        H_0 = <span class="variable language_">self</span>.initialize(bmg)        <span class="comment">#初始化</span></span><br><span class="line"></span><br><span class="line">        H = <span class="variable language_">self</span>.tau(H_0)                 <span class="comment">#激活函数/池化层</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="variable language_">self</span>.depth):    <span class="comment">#按照设定深度进行采样</span></span><br><span class="line">            M = <span class="variable language_">self</span>.message(H, bmg)      <span class="comment">#消息传递</span></span><br><span class="line">            H = <span class="variable language_">self</span>.update(M, H_0)       <span class="comment">#更新</span></span><br><span class="line"></span><br><span class="line">        index_torch = bmg.edge_index[<span class="number">1</span>].unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, H.shape[<span class="number">1</span>])</span><br><span class="line">        M = torch.zeros(<span class="built_in">len</span>(bmg.V), H.shape[<span class="number">1</span>], dtype=H.dtype, device=H.device).scatter_reduce_(</span><br><span class="line">            <span class="number">0</span>, index_torch, H, reduce=<span class="string">&quot;sum&quot;</span>, include_self=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.finalize(M, bmg.V, V_d)</span><br></pre></td></tr></table></figure>
<h2 id="谱图卷积神经网络">谱图卷积神经网络</h2>
<p><font color='red'>这部分需要独立出来作为完整的一节重构</font></p>
<p>参考资料： * <a
href="http://www.cs.yale.edu/homes/spielman/sgta/SpectTut.pdf">Spectral
Graph Theory and its Applications</a> * <a
href="https://zhuanlan.zhihu.com/p/554955092">读书报告 | 谱图理论 Ch1:
Introduction</a> * <a
href="https://zhuanlan.zhihu.com/p/290755442">图卷积网络原来是这么回事（一）——拉普拉斯矩阵初探</a>
* <a
href="https://zhuanlan.zhihu.com/p/292935670">图卷积网络原来是这么回事（二）——图傅里叶变换及案例分析</a>
* <a
href="https://zhuanlan.zhihu.com/p/297613044">图卷积网络原来是这么回事（三）——图滤波器与图卷积的设计</a>
* <a href="https://mathweb.ucsd.edu/~fan/research/revised.html">SPECTRAL
GRAPH THEORY</a> * <a href="https://arxiv.org/abs/1312.6203">Spectral
Networks and Locally Connected Networks on Graphs</a></p>
<p>谱图理论（Spectral Graph
Theory）是分析图的拉普拉斯矩阵特征值和特征向量及其对应图性质的理论。</p>
<p>考虑一个邻接矩阵，</p>
<figure>
<img src="./4-2-1.png" alt="邻接矩阵例子" />
<figcaption aria-hidden="true">邻接矩阵例子</figcaption>
</figure>
<p>可以写为 <span class="math display">$$
A=\left(\begin{array}{llll}
0 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 0
\end{array}\right)
$$</span> 不难发现矩阵<span
class="math inline"><em>A</em></span>是厄米矩阵<span
class="math inline"><em>A</em> = <em>A</em><sup>†</sup></span>，因此其具有共轭正交的本征矢，<span
class="math inline">∥<em>x</em>∥ = 1</span> 并且 <span
class="math inline"><em>x</em><sup><em>T</em></sup><em>A</em><em>x</em> = <em>λ</em></span>。将其记为：
<span class="math display">$$\begin{align}
A\mathbf{X} =&amp; \lambda \mathbf{X} \\
y=&amp;A x \\
y(i)= &amp; \sum_{j:(i, j) \in E} x(j) \\
x^T A x=&amp;\sum_{(i, j) \in E} 2x(i) x(j) \label{4-2-1} \\
\end{align}$$</span> 公式<span
class="math inline">$\eqref{4-2-1}$</span>中乘以2的原因在于<span
class="math inline">(<em>i</em>, <em>j</em>) ∈ <em>E</em></span>连接的边只计算一次，但是对称矩阵中计算两次。</p>
<p>接下来使用拉普拉斯矩阵衡量本征矢的平整性，这是因为拉普拉斯矩阵是图上的平方损失。
<span class="math display">$$\begin{align}
\boldsymbol{L}_G \stackrel{\text { def }}{=}&amp;
\boldsymbol{D}_G-\boldsymbol{M}_G \\
\boldsymbol{x}^T \boldsymbol{L}_G \boldsymbol{x}=&amp;\sum_{(a, b) \in
E} w_{a, b}(\boldsymbol{x}(a)-\boldsymbol{x}(b))^2
\end{align}$$</span> 如果<span
class="math inline"><strong>x</strong></span>没有很大的跳跃，值将会小。同时另一方面可以看出，拉普拉斯矩阵是半正定的。接下来假定拉普拉斯矩阵<span
class="math inline"><strong>L</strong><sub><em>G</em></sub></span>的本征值<span
class="math inline"><em>λ</em></span>按照从小到大排列 <span
class="math inline">0 = <em>λ</em><sub>1</sub> &lt; <em>λ</em><sub><em>k</em> − 1</sub> &lt; <em>λ</em><sub><em>k</em></sub></span>，代表从低频到高频的本征值信息。</p>
<p>为什么用“频”？接下来用一个实验说明，上面考虑的是包含4个节点的直链，接下来考虑10个节点直链，从左到右依次编号。计算其拉普拉斯矩阵的本征值与本征态并按照约定从小到大排序。其中<span
class="math inline"><em>λ</em> = 0</span>，其对应的本征态<span
class="math inline"><em>v</em><sub>0</sub></span>为一个常数。</p>
<figure>
<img src="./4-2-2.png" alt="说明频率问题" />
<figcaption aria-hidden="true">说明频率问题</figcaption>
</figure>
<p>上图中为横坐标为本征态的位置，因为有10个节点，本征态的长度为10，纵坐标表示每一个位置上代表的值的大小。其中<span
class="math inline"><em>v</em>2, <em>v</em>3, <em>v</em>4</span>分别对应<span
class="math inline"><em>λ</em><sub>1</sub>, <em>λ</em><sub>2</sub>, <em>λ</em><sub>3</sub></span>的本征态。可以看出<span
class="math inline"><em>v</em>2</span>较缓慢的变化，<span
class="math inline"><em>v</em>3, <em>v</em>4</span>的变化幅度增大。</p>
<figure>
<img src="./4-2-3.png" alt="v10频率" />
<figcaption aria-hidden="true">v10频率</figcaption>
</figure>
<p>上图是<span
class="math inline"><em>v</em>10</span>的变化情况，可以看出其变化幅度十分剧烈。</p>
<p>可以发现，以上表现出来的模式很相一根弦的振动模式，可以理解为这直链图是一根弦的离散化表示，而它的拉普拉斯矩阵是拉普拉斯算子的离散化。<font color='red'>有空补一下转化过程。</font>将把低频率的特征值与连通性联系起来。相比之下，最高频率的特征值在每个顶点处正负交替出现。高频率的特征向量可能与图着色问题和寻找独立集的问题相关。<font color='red'>如何与优化问题联系起来的，需要详细说明。</font></p>
<h3 id="图卷积">图卷积</h3>
<p>回到最开始的原因，为什么要在图结构上进行卷积操作，这里引用<a
href="https://arxiv.org/abs/1312.6203">最早提出图卷积方法文献</a>中给出的动机（该文章作者中有卷积神经网络的提出者之一Yann
LeCun）。</p>
<blockquote>
<p>Convolutional Neural Networks are extremely efficient architectures
in image and audio recognition tasks, thanks to their ability to exploit
the local translational invariance of signal classes over their
domain.</p>
</blockquote>
<p>为什么平时的卷积方式不能直接用在图上呢？</p>
<blockquote>
<p>Convolutional Neural Networks (CNNs) have been extremely succesful in
machine learning problems where the coordinates of the underlying data
representation have a grid structure (in 1, 2 and 3 dimensions), and the
data to be studied in those coordinates has translational
equivariance/invariance with respect to this grid.</p>
<p>In many contexts, however, one may be faced with data defined over
coordinates which lack some, or all, of the above geometrical
properties. Graphs offer a natural framework to generalize the
low-dimensional grid structure, and by extension the notion of
convolution.</p>
</blockquote>
<p>解决方法是什么呢？</p>
<blockquote>
<p>The other construction, which we call spectral construction, draws on
the properties of convolutions in the Fourier domain. This equivalence
is given through the graph Laplacian, an operator which provides an
harmonic analysis on the graphs.</p>
</blockquote>
<p>拉普拉斯矩阵是一个厄米矩阵，其本征值与频率有关，那么其本征态处于希尔伯特动量空间（频率空间，物理中称为动量空间、倒空间），给图加入了基矢与结构化数据。</p>
<p>虽然确定方向，但还需要解决两个问题： 1. 卷积真正的内核是什么？ 2.
如何构建在动量空间搭建卷积操作？</p>
<p>卷积, 在泛函中, 指使用一个算符(函数),
与另一个函数运算产生第三个函数。若对连续函数 <span
class="math inline"><em>f</em>(<em>x</em>), <em>g</em>(<em>x</em>)</span>
使用该运算, 可以使用积分公式: <span
class="math display">∫<sub>−∞</sub><sup>+∞</sup><em>f</em>(<em>τ</em>)<em>g</em>(<em>x</em> − <em>τ</em>)<em>d</em><em>τ</em></span></p>
<p>在卷积神经网络中, 我们见到的是离散二维卷积： <span
class="math display">$$
f[x, y] * g[x, y]=\sum_{i=-\infty}^{+\infty} \sum_{j=-\infty}^{+\infty}
f[i, j] \cdot g[x-i, y-j]
$$</span></p>
<p>上面神经网络的卷积通常是对欧式空间数据的操作,
例如图片数据。这样的二维离散卷积可以看作是在固定形状的矩阵 <span
class="math inline"><em>g</em>[<em>x</em>, <em>y</em>]<sub><em>M</em> × <em>N</em></sub></span>
上使用固定的卷积核 <span
class="math inline"><em>f</em>[<em>x</em>, <em>y</em>]<sub><em>a</em> × <em>b</em></sub></span>
进行对应位置相乘再求和(积分)。再联想CNN中的卷积操作，由此可见卷积的核心在于找到卷积核定义方式（也是需要学习参数的地方），给定一种移动方式（这个在CNN中是超参）。</p>
<p>接下来搭建动量空间的卷积操作。首先将图数据投影到动量空间。定义动量空间的基矢量：</p>
<p><span class="math display">$$
\begin{equation}
L=U \Lambda U^T=\left[\begin{array}{cccc}
\mid &amp; \mid &amp; \ldots &amp; \mid \\
\boldsymbol{v}_1 &amp; \boldsymbol{v}_2 &amp; \ldots &amp;
\boldsymbol{v}_n \\
\mid &amp; \mid &amp; \ldots &amp; \mid
\end{array}\right]\left[\begin{array}{cccc}
\lambda_1 &amp; &amp; &amp; \\
&amp; \lambda_2 &amp; &amp; \\
&amp; &amp; \ldots &amp; \\
&amp; &amp; &amp; \lambda_n
\end{array}\right]
\left[
\begin{array}{ccc}
- &amp; \boldsymbol{v}_1^T &amp; - \\
- &amp; \boldsymbol{v}_2^T &amp; - \\
- &amp; \ldots &amp; - \\
- &amp; \boldsymbol{v}_n^T &amp; -
\end{array}
\right] \label{laplace}
\end{equation}
$$</span></p>
<p><font color='yellow'>这里笔者犯了一个很蠢的错误，把邻接矩阵当作图数据。但是邻接矩阵是用于刻画相互作用关系的，每一个节点如何取值才是图数据。</font>定义图数据<span
class="math inline"><strong>x</strong> = [<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>⋯<em>x</em><sub><em>n</em></sub>]</span>，每一个数据<span
class="math inline"><em>x</em><sub><em>i</em></sub></span>为在图上<span
class="math inline"><em>i</em></span>节点的值。对于频率为<span
class="math inline"><em>λ</em><sub><em>j</em></sub></span>的基矢投影得到：</p>
<p><span class="math display">$$\begin{align}
F(\lambda_j) = \langle\boldsymbol{x}, \boldsymbol{v_j}\rangle =
\sum_{j=1}^n x_j v_i(j)
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>v</em><sub><em>i</em></sub>(<em>j</em>)</span>表示第<span
class="math inline"><em>i</em></span>个特征向量的第<span
class="math inline"><em>j</em></span>个分量，自然这个投影过程就是图傅里叶变换（Graph
Fourier Transform，GFT） <span class="math display">$$\begin{align}
\tilde{x}_k=\left\langle\boldsymbol{x}, \boldsymbol{v}_k\right\rangle,
k=1,2, \ldots, n
\end{align}$$</span></p>
<p>用矩阵形式可以计算出所有的傅里叶系数: <span
class="math display">$$\begin{align}
\tilde{\boldsymbol{x}}=U^T \boldsymbol{x}=\left[\begin{array}{ccc}
- &amp; \boldsymbol{v}_1^T &amp; - \\
- &amp; \boldsymbol{v}_2^T &amp; - \\
- &amp; \ldots &amp; - \\
- &amp; \boldsymbol{v}_n^T &amp; -
\end{array}\right] \quad \boldsymbol{x}, \quad \tilde{\boldsymbol{x}}
\in \boldsymbol{R}^n
\end{align}$$</span></p>
<p>由于拉普拉斯矩阵的特征向量 <span
class="math inline"><em>U</em></span> 是一组 <span
class="math inline"><em>n</em></span> 维空间中的完备的正交基, 对上式左乘
<span class="math inline"><em>U</em></span>, 则有 <span
class="math display">$$\begin{align}
\boldsymbol{x}= &amp; U \tilde{\boldsymbol{x}}=\left[\begin{array}{cccc}
\mid &amp; \mid &amp; \ldots &amp; \mid \\
\boldsymbol{v}_1 &amp; \boldsymbol{v}_2 &amp; \ldots &amp;
\boldsymbol{v}_n \\
\mid &amp; \mid &amp; \ldots &amp; \mid
\end{array}\right]\left[\begin{array}{c}
\tilde{x}_1 \\
\tilde{x}_2 \\
\ldots \\
\tilde{x}_n
\end{array}\right] \\
=&amp;\tilde{x}_1 \boldsymbol{v}_1+\tilde{x}_2
\boldsymbol{v}_2+\ldots+\tilde{x}_n \boldsymbol{v}_n \\
=&amp; \sum_{k=1}^n \tilde{x}_k \boldsymbol{v}_k
\end{align}$$</span></p>
<p>上式是一种矩阵形式的逆图傅里叶变换 (IGFT)。</p>
<p>接下来考虑在动量空间的卷积操作。给定图 <span
class="math inline"><em>G</em></span> 上的两组图信号 <span
class="math inline"><strong>x</strong><sub>1</sub></span> 和 <span
class="math inline"><strong>x</strong><sub>2</sub></span>,
图卷积运算的定义如下: <span class="math display">$$\begin{align}
\boldsymbol{x}_1 *
\boldsymbol{x}_2=\operatorname{IGFT}\left(\operatorname{GFT}\left(\boldsymbol{x}_1\right)
\odot \operatorname{GFT}\left(\boldsymbol{x}_2\right)\right)
\end{align}$$</span></p>
<p>其中, <span class="math inline">⊙</span>
表示两个向量的逐元素相乘。这里的定义与离散信号处理中的卷积定义是一样的，时域中的卷积运算等价于频域中的乘法运算。</p>
<p>继续对上式进行推导: <span class="math display">$$\begin{align}
\boldsymbol{x}_1 * \boldsymbol{x}_2=&amp; U\left[\left(U^T
\boldsymbol{x}_1\right) \odot\left(U^T \boldsymbol{x}_2\right)\right] \\
=&amp; U\left[\tilde{\boldsymbol{x}}_1 \odot\left(U^T
\boldsymbol{x}_2\right)\right] \\
=&amp; U \operatorname{diag}\left(\tilde{\boldsymbol{x}}_1\right)
\mathrm{U}^{\mathrm{T}} \boldsymbol{x}_2
\end{align}$$</span></p>
<p>令<span class="math inline">$H_{\tilde{\boldsymbol{x}}_1}=U
\operatorname{diag}\left(\tilde{\boldsymbol{x}}_1\right)
\mathrm{U}^{\mathrm{T}} \boldsymbol{x}_2$</span>
为卷积核，图信号的卷积操作为: <span class="math display">$$\begin{align}
\boldsymbol{x}_1 * \boldsymbol{x}_2=H_{\tilde{\boldsymbol{x}}_1}
\boldsymbol{x}_2
\end{align}$$</span></p>
<p><span
class="math inline">H = U<em>Λ</em><sub>h</sub>U<sup>T</sup></span>
的核心在于中间对角化的本征值 <span
class="math inline"><em>Λ</em><sub><em>h</em></sub></span>，那么对应于CNN的卷积核，自然会想到<span
class="math inline">H</span>进行参数化,
本质是通过训练的方式得出一组<span
class="math inline"><em>Λ</em><sub><em>h</em></sub></span>，来让图卷积自动地调整和取舍不同本征态的信息，从而提取出有用的图特征。</p>
<p>故设计如下的图卷积层: <span class="math display">$$\begin{align}
Y=&amp;\sigma\left(U\left[\begin{array}{cccc}
\theta_1 &amp; &amp; &amp; \\
&amp; \theta_2 &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; \theta_n
\end{array}\right] \quad U^T X\right) \label{theta}\\
=&amp;\sigma\left(U \operatorname{diag}(\theta) \mathrm{U}^{\mathrm{T}}
\mathrm{X}\right)\\
=&amp;\sigma(\Theta \mathrm{X})
\end{align}$$</span></p>
<p>其中, <span
class="math inline"><em>X</em> ∈ <em>R</em><sup><em>n</em> × <em>f</em></sup></span>
是输入的图信号矩阵, <span
class="math inline"><em>θ</em> = [<em>θ</em><sub>1</sub>, <em>θ</em><sub>2</sub>, …<em>θ</em><sub><em>n</em></sub>]</span>
是需要学习的参数, <span class="math inline"><em>Θ</em></span>
是需要学习的图卷积核, <span class="math inline"><em>σ</em>(⋅)</span>
是激活函数, 而 <span class="math inline"><em>Y</em></span>
是输出的图信号矩阵。</p>
<p>回顾以上思路，首先面对图类型数据由于没有结构化的表示方法，因此将其转入动量空间中进行表示（此操作称为傅里叶变化）；接下来利用基矢量的正交归一性，定义动量空间的卷积操作；然后参数化构造卷积核；最后再将其动量空间转化为图数据，增加激活函数。</p>
<p>但是这样的操作也存在一些缺点，卷积层的参数个数与节点数相等。在上亿节点数规模的图训练任务重，这样的网络极易发生过拟合问题，因为Yann
以及后续的大部分成果都指出：图的有效信息往往蕴含在低频段，没有必要为每一个频段训练一个参数。<font color='red'>有待进一步调研</font></p>
<h3 id="k阶截断多项式算子">K阶截断多项式算子</h3>
<ul>
<li><a href="https://arxiv.org/abs/0912.3848">Wavelets on Graphs via
Spectral Graph Theory</a></li>
</ul>
<p>为了解决参数过多过拟合的问题，那就通过截断将参数规模下降，在数学上称为小波变换（Wavelet
Transform）。</p>
<p>观察<span class="math inline">$\eqref{laplace}$</span>和<span
class="math inline">$\eqref{theta}$</span>，其中<span
class="math inline"><em>λ</em></span>是由拉普拉斯矩阵得出，而<span
class="math inline"><em>θ</em></span>是卷积学习参数，可以通过用<span
class="math inline"><em>λ</em></span>的<span
class="math inline"><em>K</em></span>阶表示<span
class="math inline"><em>θ</em></span>来减少学习参数： <span
class="math display">$$\begin{align}
\Lambda(\boldsymbol{h})=&amp;\sum_{i=0}^K h_i \lambda_k^i \\
=&amp;\left[\begin{array}{cccc}
\sum_{i=0}^K h_i \lambda_1^i &amp; &amp; &amp; \\
&amp; \sum_{i=0}^K h_i \lambda_2^i &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; \sum_{i=0}^K h_i \lambda_N^i
\end{array}\right]
\end{align}$$</span></p>
<p>这样通过学习参数<span
class="math inline"><strong>h</strong></span>减少学习的参数量。该变换在空域同样具有含义，指的是，对于一个节点仅仅考虑第<span
class="math inline"><em>K</em></span>阶近邻的节点。</p>
<h3 id="切比雪夫多项式滤波算子">切比雪夫多项式滤波算子</h3>
<ul>
<li><a href="https://arxiv.org/abs/1606.09375">Convolutional Neural
Networks on Graphs with Fast Localized Spectral Filtering</a></li>
</ul>
<p>在<span
class="math inline"><em>K</em></span>阶截断多项式算子中，将本征值作为一种基进行展开，然后学习<span
class="math inline"><em>K</em></span>个参数<span
class="math inline"><em>h</em></span>得到最后的卷积核。那么本征值的选取只要是负数且不同的即可，并没有必要准确计算出来。因此，使用切比雪夫多项式结果代替本征值。</p>
<p>切比雪夫多项式来源于 <span class="math inline"><em>n</em></span>
倍角公式, 其每一项可以通过迭代的方式求出来。其第 <span
class="math inline"><em>k</em></span> 项满足: <span
class="math display">$$\begin{align}
T_k(x)=&amp;2 x T_{k-1}(x)-T_{k-2}(x)\\
T_0=&amp;1 \quad T_1=x
\end{align}$$</span> 由于切比雪夫多项式<span
class="math inline"><em>x</em> ∈ [−1, 1]</span>，因此对拉普拉斯矩阵进行变换（<span
class="math inline"><em>λ</em><sub>max</sub></span>为最大的本征值）：
<span class="math display">$$\begin{align}
\bar{\Lambda}=&amp;\frac{2 \Lambda}{\lambda_{\max }}-I \\
\bar{L}=&amp;\frac{2 L}{\lambda_{\max }}-I
\end{align}$$</span></p>
<p>于是, 我们用截断的 <span class="math inline"><em>K</em></span>
阶切比雪夫多项式来表示图卷积操作:</p>
<p><span class="math display">$$\begin{align}
\Lambda(\boldsymbol{h})=&amp;\sum_{i=0}^K h_i T_i(\bar{\Lambda})
\end{align}$$</span></p>
<h3 id="图卷积神经网络">图卷积神经网络</h3>
<ul>
<li><a href="https://arxiv.org/abs/1609.02907">Semi-Supervised
Classification with Graph Convolutional Networks</a></li>
</ul>
<p>图卷积神经网络（Graph Convolutional Network, GCN）</p>
<p>在实际运用中，如果<span
class="math inline"><em>K</em></span>值取得过高，则对于包含度数较大的节点的图来说，一个卷积层的感受野很有可能直接覆盖了几乎整张图。这样的话，即使堆叠几层卷积层，后续的卷积层的感受野仍然是整张图，重复执行全局平均操作，最终导致输出的图信号过平滑。</p>
<p>为此进一步近似，令<span
class="math inline"><em>K</em> = 1, <em>λ</em><sub>max</sub> = 2</span>，得到一阶切比雪夫多项式近似：</p>
<p><span class="math display">$$\begin{align}
\Lambda(\boldsymbol{h})=&amp;\sum_{i=0}^2 h_i T_i(\bar{\Lambda}) \\
=&amp; h_0 T_0(\bar{\Lambda}) + h_1 T_1(\bar{\Lambda}) \\
=&amp; h_0 \boldsymbol{I} + h_1 \bar{\Lambda} \\
=&amp; h_0 \boldsymbol{I} + h_1
(\frac{2\Lambda}{\lambda_\max}-\boldsymbol{I}) \\
\end{align}$$</span></p>
<p>利用<span
class="math inline">$\eqref{normal_laplace}$</span>，可以将输出转化为：
<span class="math display">$$\begin{align}
\boldsymbol{y}=&amp;h_0 \boldsymbol{x}+h_1\left(L-I_n\right)
\boldsymbol{x} \\
=&amp;h_0 \boldsymbol{x}-h_1 D^{-\frac{1}{2}} W D^{-\frac{1}{2}}
\boldsymbol{x}
\end{align}$$</span> 设 <span
class="math inline"><em>h</em><sub>0</sub> = −<em>h</em><sub>1</sub> = <em>h</em></span>,
则 <span class="math display">$$\begin{align}
\boldsymbol{y}=h\left(I_n+D^{-\frac{1}{2}} W D^{-\frac{1}{2}}\right)
\boldsymbol{x}=H \boldsymbol{x}
\end{align}$$</span></p>
<p>因为 <span class="math inline">$I_n+D^{-\frac{1}{2}} W
D^{-\frac{1}{2}}=2 I_n-\bar{L}$</span> 的特征值范围是 <span
class="math inline">[0, 2]</span>,
连续堆叠这样的卷积层相当于引入了频率响应函数 <span
class="math inline">(2 − <em>λ̄</em><sub><em>i</em></sub>)<sup><em>K</em></sup></span>,
会容易过度放大 <span
class="math inline"><em>λ̄</em><sub><em>i</em></sub> &lt; 1</span>
频段的信号, 进而可能会引发某些参数梯度爆炸而另外一些参数梯度消失。</p>
<p><font color='red'>未完待续</font></p>
<h1 id="图网络几何等变性">图网络几何等变性</h1>
<ul>
<li><a href="https://arxiv.org/abs/2202.07230">Geometrically Equivariant
Graph Neural Networks: A Survey</a></li>
</ul>
<p>这篇综述给出研究几何等变性的动机十分精确： &gt;
在物理学和化学等领域中，许多问题需要处理形式为几何图形的数据 [Bronstein
et al.,
2021]。与一般的图形数据不同，几何图形不仅为每个节点分配一个特征，还分配一个几何向量。例如，可以将分子/蛋白质视为一个几何图形，其中原子的3D位置坐标是几何向量；或者在一个一般的多体物理系统中，粒子的3D状态（位置、速度或自旋）是几何向量。值得注意的是，几何图形展现出平移、旋转和/或反射的对称性。这是因为控制原子（或粒子）动态的物理定律无论我们如何将分子（或一般的物理系统）从一个地方平移或旋转到另一个地方都是相同的。在处理这类数据时，将对称性的归纳偏差融入到模型设计中是至关重要的，这激发了对具有几何等变性的图神经网络（GNNs）的研究。</p>
<p>然后这篇综述描述了在消息传递下的等变性问题。</p>
<p>令<span class="math inline">𝒳</span>和<span
class="math inline">𝒴</span>分别为输入和输出向量空间，它们都赋予了一组变换<span
class="math inline"><em>G</em> : <em>G</em> × 𝒳 → 𝒳</span>和<span
class="math inline"><em>G</em> × 𝒴 → 𝒴</span>。如果函数<span
class="math inline"><em>ϕ</em> : 𝒳 → 𝒴</span>在我们对输入应用任何变换时，输出也通过相同的变换或某种可预测的行为发生变化，则称该函数相对于<span
class="math inline"><em>G</em></span>是<strong>等变</strong>的。具体来说，有：</p>
<p><strong>定义 1（等变性）</strong> 函数<span
class="math inline"><em>ϕ</em> : 𝒳 ↦ 𝒴</span>是<span
class="math inline"><em>G</em></span>-等变的，如果它与<span
class="math inline"><em>G</em></span>中的任何变换交换， <span
class="math display">$$\begin{align}
\phi\left(\rho_{\mathcal{X}}(g) x\right)=\rho_{\mathcal{Y}}(g) \phi(x),
\quad \forall g \in G \label{transformation}
\end{align}$$</span> 其中<span
class="math inline"><em>ρ</em><sub>𝒳</sub></span>和<span
class="math inline"><em>ρ</em><sub>𝒴</sub></span>分别是输入空间和输出空间中的群表示。特别是，如果<span
class="math inline"><em>ρ</em><sub>𝒴</sub></span>是恒等的，则称<span
class="math inline"><em>ϕ</em></span>是<strong>不变</strong>的。</p>
<p><strong>定义 2（群）</strong> 群<span
class="math inline"><em>G</em></span>是一组变换，具有满足以下属性的二元操作“<span
class="math inline">⋅</span>“在关联复合下是封闭的，存在一个单位元，且<span
class="math inline"><em>G</em></span>中的每个元素必须有一个逆元。</p>
<p>根据群的定义，我们在这里提供一些例子： - <span
class="math inline">O(<em>n</em>)</span>是一个<span
class="math inline"><em>n</em></span>维正交群，由旋转和反射组成。 -
<span
class="math inline">SO(<em>n</em>)</span>是一个特殊正交群，仅由旋转组成。
- <span class="math inline">E(<em>n</em>)</span>是一个<span
class="math inline"><em>n</em></span>维欧几里得群，由旋转、反射和平移组成。
- <span
class="math inline">SE(<em>n</em>)</span>是一个特殊欧几里得群，由旋转和平移组成。
-
李群是其元素构成可微流形的群。实际上，上面所有的群都是李群的具体例子。</p>
<p><strong>群表示</strong> 一个群的表示是一个可逆的线性映射<span
class="math inline"><em>ρ</em>(<em>g</em>) : <em>G</em> ↦ 𝒱</span>，它接受群元素<span
class="math inline"><em>g</em> ∈ <em>G</em></span>作为输入，并在向量空间<span
class="math inline">𝒱</span>上作用。同时，它是线性的：<span
class="math inline"><em>ρ</em>(<em>g</em>)<em>ρ</em>(<em>h</em>) = <em>ρ</em>(<em>g</em> ⋅ <em>h</em>), ∀<em>g</em>, <em>h</em> ∈ <em>G</em></span>。例如，对于<span
class="math inline">O(<em>n</em>)</span>的矩阵表示是正交矩阵<span
class="math inline"><strong>O</strong> ∈ ℝ<sup><em>n</em></sup></span>，满足<span
class="math inline"><strong>O</strong><sup>⊤</sup><strong>O</strong> = <strong>I</strong></span>。在<span
class="math inline">O(3)</span>上实例化 <span
class="math inline">$\eqref{transformation}$</span> 变为<span
class="math inline"><em>ϕ</em>(<strong>O</strong><em>x</em>) = <strong>O</strong><em>ϕ</em>(<em>x</em>)</span>，如果输入和输出空间共享相同的表示。对于平移等价性，我们有<span
class="math inline"><em>ϕ</em>(<em>x</em> − <strong>t</strong>) = <em>ϕ</em>(<em>x</em>) − <strong>t</strong></span>，其中<span
class="math inline"><em>t</em> ∈ ℝ<sup><em>n</em></sup></span>。</p>
<figure>
<img src="./5-1.png"
alt="An illustration of the geometrically equivariant" />
<figcaption aria-hidden="true">An illustration of the geometrically
equivariant</figcaption>
</figure>
<p>上图是在消息传递图网络上，展示旋转等变性。左边是通常的消息传递过程，首先进行消息传递，然后进行聚合得出最后的结果。有边的图最开始是由左图进行旋转得到，然后同样经过消息传递与聚合操作，得出的结果与左图进行旋转结果相同。由此说明了消息传递的图神经网络的几何等变性。</p>
<p>在消息传递GNN中，实现等变性，主要有以下三种思路： *
使用群论中的不可约表示，例如对于<span
class="math inline">SO(3)</span>群，使用 Wigner-D matrices 进行表示。 *
通过李群和李代数，对之前参见的操作进行修改。例如将卷积修改为李卷积(LieConv)。
*
通过标量化，将几何向量首先被转换为不变的标量，然后通过几个多层感知机（MLPs）来控制大小，最后在原始方向上相加以获得等变性。</p>
<h1 id="hypergraph">Hypergraph</h1>
<p>超图（Hypergraph）是<strong>一种高维的图形结构</strong>，用于表示对象之间的复杂关系。</p>
<p>超图的核心概念在于它包含的“超边”，这是与普通图的边不同的概念。在普通图中，一条边连接两个顶点，而在超图中，一条<strong>超边可以连接多个顶点</strong>（包括两个以上）。这种结构使得超图能够表示更为复杂的关系，比如社交网络中一个人可能同时属于多个社交圈，或者在生态系统中一个物种可能在多个栖息地中出现。</p>
<p>超图由两部分组成：一组顶点集合 <span class="math inline">𝒱</span>
和一组超边集合 <span class="math inline">ℰ</span>
，超图可以表示为这些集合的组合 <span
class="math inline">ℋ = (𝒱, ℰ)</span> 。在超图中，如果一个顶点 <span
class="math inline"><em>v</em></span> 出现在一个超边 <span
class="math inline"><em>e</em></span> 中，那么我们说顶点 <span
class="math inline"><em>v</em></span> 与超边 <span
class="math inline"><em>e</em></span> 相关联。</p>
<h2 id="special-hypergraph-neural-networks">Special Hypergraph Neural
Networks</h2>
<ul>
<li><a
href="https://dl.acm.org/doi/abs/10.1609/aaai.v33i01.33013558">Hypergraph
Neural Networks</a> <a
href="https://github.com/iMoonLab/HGNN">代码仓库Github</a> <a
href="https://zhuanlan.zhihu.com/p/653917458">文章解读</a></li>
<li><a
href="https://ieeexplore.ieee.org/abstract/document/6287378">Learning
with Hypergraphs: Clustering, Classification, and Embedding</a> <img
src="./6-1.png" alt="Hypergraph" /></li>
</ul>
<p>针对超图这种复杂的结构，作者提出了 hypergraph neural networks (HGNN)
框架。</p>
<figure>
<img src="./6-2.png"
alt="The comparison between graph and hypergraph" />
<figcaption aria-hidden="true">The comparison between graph and
hypergraph</figcaption>
</figure>
<p>由上图可见，左边的<span
class="math inline"><em>W</em></span>为邻接矩阵，右边表示含有超边的图。该图的每一行代表一个节点，每一列表示一条超边，例如<span
class="math inline"><em>e</em><sub>1</sub></span>这条超边与<span
class="math inline"><em>n</em><sub>2</sub>, <em>n</em><sub>4</sub>, <em>n</em><sub>8</sub></span>相连，在矩阵上只有这三行为<span
class="math inline">1</span>。这种数据结构可以将任意的超边包含在内。<font color='red'>这种方式十分有效，但是之后如何进行嵌入呢？这样看只能在空域运算；在谱域，没有邻接矩阵，如何得到拉普拉斯矩阵？</font></p>
<p>首先将超图定义为<span
class="math inline">𝒢 = (𝒱，ℰ, <strong>W</strong>)</span>，其中顶点集合为<span
class="math inline">𝒱</span>，超边集为<span
class="math inline">ℰ</span>，每一个超边给予一个权重<span
class="math inline"><strong>W</strong></span>。超图<span
class="math inline">𝒢</span>可以通过<span
class="math inline">|𝒱|×|ℰ|</span>的矩阵<span
class="math inline"><strong>H</strong></span>定义为：</p>
<p><span class="math display">$$\begin{align}
h(v, e)= \begin{cases}1, &amp; \text { if } v \in e \\ 0, &amp; \text {
if } v \notin e,\end{cases}
\end{align}$$</span></p>
<p>接下来考虑节点的分类任务，参考<a
href="https://ieeexplore.ieee.org/abstract/document/6287378">Learning
with Hypergraphs: Clustering, Classification, and
Embedding</a>，利用正则化框架： <span class="math display">$$
\begin{align}
\arg \min _f\left\{\mathcal{R}_{\text{e m p}}(f)+\Omega(f)\right\},
\end{align}
$$</span></p>
<p>其中 <span class="math inline"><em>Ω</em>(<em>f</em>)</span>
是正则化项，<span class="math inline">ℛ<sub>emp</sub>(<em>f</em>)</span>
是经验损失，<span class="math inline"><em>f</em>(⋅)</span>
表示分类函数。</p>
<p><span class="math display">$$
\begin{align}
\Omega(f) = \frac{1}{2} \sum_{e \in \mathcal{E}} \sum_{\substack{u, v
\in \mathcal{V}}}
\frac{w(e)\, h(u, e)\, h(v, e)}{\delta(e)}
\left( \frac{f(u)}{\sqrt{d(u)}} - \frac{f(v)}{\sqrt{d(v)}} \right)^2
\end{align}
$$</span></p>
<p>令<span
class="math inline"><em>θ</em> = <strong>D</strong><sub><em>v</em></sub><sup>−1/2</sup><strong>H</strong><strong>W</strong><strong>D</strong><sub><em>e</em></sub><sup>−1</sup><strong>H</strong><sup>⊤</sup><strong>D</strong><sub><em>v</em></sub><sup>−1/2</sup></span>
和 <span
class="math inline"><strong>Δ</strong> = <strong>I</strong> − <strong>Θ</strong></span>，那么正则化项<span
class="math inline"><em>Ω</em>(<em>f</em>)</span> 可以写为： <span
class="math display"><em>Ω</em>(<em>f</em>) = <em>f</em><sup>⊤</sup><strong>Δ</strong>,</span>
其中<span class="math inline"><strong>Δ</strong></span>
是版正定的通常被叫为超图上的拉普拉斯矩阵（hypergraph Laplacian）。</p>
<p>剩下的操作和之前的一致，找到其本征态定义为傅里叶变换矩阵，然后利用本征值作为滤波函数，引入学习参数。为了简化，将滤波函数用切比雪夫多项式表示。最后完成特征提取。</p>
<h2 id="hypergraph-embedding-based-on-random-walk">Hypergraph Embedding
Based on Random Walk</h2>
<h2 id="random-walks-on-random-hypergraph">Random Walks on Random
Hypergraph</h2>
<h2
id="random-walks-and-laplacians-on-hypergraphs-when-do-they-match">Random
walks and Laplacians on hypergraphs: When do they match?</h2>
<h2 id="message-passing-on-hypergraphs">Message-Passing on
Hypergraphs</h2>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Graphs</tag>
        <tag>Complex Network</tag>
        <tag>Laplacian Matrix</tag>
        <tag>Hypergraph Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>统计物理读书笔记</title>
    <url>/2024/01/28/book/statistical_physics_CMB/statistical_physics_CMB/</url>
    <content><![CDATA[<p>本笔记为阅读陈敏伯《统计物理》的读书笔记，包含大量基本概念与公式推导。按照本书前言所著，这本书大量使用变分原理进行推理基本公式。</p>
<span id="more"></span>
<figure>
<img src="./0-1.png" alt="脉络体系" />
<figcaption aria-hidden="true">脉络体系</figcaption>
</figure>
<h1 id="第一章引言">第一章：引言</h1>
<p>量子力学用以构建原子分子的微观性质，统计力学根据此微观性质进行解释从而得到理解体系的宏观性质，这就是统计力学的任务所在。</p>
<h2 id="宏观量的统计性质">宏观量的统计性质</h2>
<p>体系的宏观量（可直接或者间接观测的物理量）：</p>
<ul>
<li>热力学变量（温度、密度、压强等）</li>
<li>热力学函数（内能、熵等）</li>
<li>体系的电磁性质</li>
<li>输运性质（扩散、粘度、热传导等）</li>
<li>速度分布和流体密度相关函数</li>
</ul>
<p>宏观量应该是相应微观量的统计平均值。宏观量应该具备两个特征：</p>
<ul>
<li>在空间尺度上，宏观量在宏观上足够小从而可以看出在宏观体系上的不均匀性质；但同时在微观上应该足够大，包含足够多的粒子，从而统计平均富有意义。</li>
<li>在时间尺度上，宏观量在宏观尺度上应该足够小，可以反应出在宏观尺度上随时间变化的情况；另一方在微观上应该是足够长的，包含了足够多次的微观变化。</li>
</ul>
<h2 id="基本概念">基本概念</h2>
<p><strong>体系</strong>：物质世界中普遍存在着相互作用，从相互作用的众多物体中划分出来进行研究的那一部分称为体系。</p>
<p><strong>环境</strong>：所有与上述体系存在相互作用而又不属于该体系的物体，统称为环境。体系与环境的划分由如何解决问题的方便决定。</p>
<p><strong>体系分类</strong>： *
孤立体系：体系与环境无物质与能量的交换。 *
封闭体系：体系与环境仅有能量的交换，无物质交换。 *
开放体系：体系与环境存在物质与能量的交换。</p>
<p><strong>广义坐标</strong>：描述问题的最小变量。即确定体系的独立变量，称为广义坐标（广义位置、lagrange位形）。记为：
<span
class="math display"><em>q</em>(<em>t</em>) = (<em>q</em><sub>1</sub>(<em>t</em>), <em>q</em><sub>2</sub>(<em>t</em>)⋯<em>q</em><sub><em>s</em></sub>(<em>t</em>))</span></p>
<p><strong>广义动量</strong>：为确定整个体系在<span
class="math inline"><em>t</em></span>时刻的力学状态，除了知道广义动量坐标以外还需要知道其下一时刻的运动方向，称为广义运动向量,记作<span
class="math inline"><em>q̇</em>(<em>t</em>) = (<em>q̇</em><sub>1</sub>(<em>t</em>), <em>q̇</em><sub>2</sub>(<em>t</em>)⋯<em>q̇</em><sub><em>s</em></sub>(<em>t</em>))</span>。Hamilton更深刻的反应力学本质，除广义位移以外引入广义动量，定义为：</p>
<p><span class="math display">$$\begin{equation}p(t)=\frac{\partial
L}{\partial \dot{q}(t)}\end{equation}$$</span></p>
<p><strong>相空间</strong>：由<span
class="math inline"><em>x</em> = (<em>q</em>, <em>q̇</em>)</span>构成的空间称为相空间，记为<span
class="math inline"><em>Γ</em></span>。描述一个粒子的相空间称为<span
class="math inline"><em>μ</em></span>空间。</p>
<p><strong>力学量</strong>：体系中任意可观测量（记为<span
class="math inline"><em>B</em></span>）在体系的每个微观状态下都有确定的数值。</p>
<p><strong>体系的宏观量</strong>：描述体系的平衡态实际上只要用少数宏观量即可。体积、压强、温度、能量、外磁场、外电场、电极化强度、磁极化强度等。</p>
<p><strong>涨落</strong>：由于微观状态在剧烈变化，宏观会在平均值附近随机变动，这种变动称为涨落。</p>
<p><strong>非平衡定态</strong>：经验表明，体系处于环境不变的情况下，经过一段时间后，体系必将达到一个宏观上不随时间变化的状态，尽管不一定是平衡态，但体系将长久的保持这种状态，这种状态称为非平衡定态（定态、稳态、定常态stationary
state），描述为：<span class="math inline">$\frac{\partial \langle
B\rangle}{\partial t}=
0$</span>（注意，只是时间的偏导，而不是全导），例如：在两个具有稳定温差中形成的热流，是定态。</p>
<p><strong>平衡态</strong>：处于定态体系，并且其环境的宏观状态宏观状态也不变，则这个体系称为处于平衡态。体系的所有宏观性质随时间不变化：<span
class="math inline">$\frac{\partial \langle B\rangle}{\partial
t}=0$</span>。处于平衡态的体系，内部允许出现某种微观不均匀，但是不允许出现流（热流、粒子流等）。</p>
<p><strong>非平衡态</strong>：体系所处的宏观状态，非上述的平衡态，均处于非平衡态。</p>
<p><strong>弛豫过程</strong>：处于非平衡态的体系有自发趋向平衡态的趋势，这种从非平衡态到平衡态的过程称为弛豫过程。</p>
<p><strong>广延量与强度量</strong>：描述体系所需的所有宏观参量。与体系质量有关的称为广延量，与体系质量无关的称为强度量。</p>
<p><strong>外参量与内参量</strong>：宏观参量的另一种分类方法，只取决于环境而与体系无关的宏观量称为外参量，例如：体系体积、体系外形、外电场、外磁场；取决于体系内部粒子的特性以及运动状态的宏观参量，例如：体系的压力。</p>
<p><strong>物态方程</strong>：系统处于平衡态时，内参量依赖于外参量。例如气体存在的关系：<span
class="math inline"><em>f</em>(<em>p</em>, <em>V</em>, <em>T</em>) = 0</span>。物质在其他形态下的物态方程，在工程界有时候也称为本构方程。</p>
<h2
id="统计力学中体系力学描述的三种不同层次">统计力学中体系力学描述的三种不同层次</h2>
<p>Bogoliubov提出关于空间、时间上大致有三种不同尺度的描述方法，又称为三种标度。粒子间的相互作用力程<span
class="math inline"><em>L</em><sub><em>i</em><em>n</em><em>t</em></sub></span>，粒子平均自由程<span
class="math inline"><em>L</em><sub><em>τ</em></sub></span>和体系温度、密度等非均匀的量程<span
class="math inline"><em>L</em><sub><em>h</em></sub></span>。</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr>
<th>描述层次</th>
<th>特征长度</th>
<th>特征时间</th>
<th>以常温、常压氢气为例</th>
</tr>
</thead>
<tbody>
<tr>
<td>微观层次</td>
<td>粒子间作用力程<span
class="math inline"><em>L</em><sub><em>i</em><em>n</em><em>t</em></sub></span></td>
<td>相互作用持续时间<span
class="math inline"><em>τ</em><sub><em>i</em><em>n</em><em>t</em></sub></span></td>
<td><span
class="math inline"><em>L</em><sub><em>i</em><em>n</em><em>t</em></sub> = 10<sup>−8</sup>cm, <em>τ</em> = 5 × 10<sup>−14</sup>s</span></td>
</tr>
<tr>
<td>动理学层次</td>
<td>粒子平均自由程<span
class="math inline"><em>L</em><sub><em>τ</em></sub></span></td>
<td>弛豫时间<span class="math inline"><em>τ</em></span></td>
<td><span
class="math inline"><em>L</em><sub><em>τ</em></sub> = 10<sup>−5</sup>cm, <em>τ</em> = 5 × 10<sup>−11</sup>s</span></td>
</tr>
<tr>
<td>流体力学层次</td>
<td>非均匀性量程<span
class="math inline"><em>L</em><sub><em>h</em></sub></span></td>
<td>非均匀性特征时间<span
class="math inline"><em>τ</em><sub><em>h</em></sub></span></td>
<td><span
class="math inline"><em>L</em><sub><em>h</em></sub> = 1cm, <em>τ</em><sub><em>h</em></sub> = 5 × 10<sup>−6</sup>s</span></td>
</tr>
</tbody>
</table>
<h1 id="第二章经典动力学">第二章：经典动力学</h1>
<h2 id="lagrange函数">Lagrange函数</h2>
<p><span
class="math display"><em>L</em>(<em>q</em>, <em>q̇</em>, <em>t</em>) = <em>T</em> − <em>U</em></span></p>
<p>以下讨论保守力体系，即<span
class="math inline"><em>F</em> = −∇<em>U</em></span>，Lagrange函数可以写为：
<span class="math display">$$\begin{align}
L(q,\dot q)&amp;=T(\dot q)-U(q)=\frac{1}{2}\sum_{i=1}^{3N} m_i
\dot{x_i}^2-U \\
&amp;= \frac{1}{2}\sum_{i=1}^{3N}\sum_{k,l=1}^{s^2} m_i \frac{\partial
x_i}{\partial q_k}\frac{\partial x_i}{\partial
q_l}\dot{q}_k\dot{q}_l-U(q) \\
&amp;= \frac{1}{2}\sum_{k,l=1}^{s^2} M_{kl} \dot{q}_k\dot{q}_l-U(q) \\
\end{align}$$</span></p>
<p>其中<span
class="math inline">{<em>x</em>}</span>称为d’Alembert位形，广义质量<span
class="math inline"><em>N</em><sub><em>k</em><em>l</em></sub></span>定义为：
<span class="math display">$$M_{kl}= \sum_{i=1}^{3N}m_i\frac{\partial
x_i}{\partial q_k}\frac{\partial x_i}{\partial q_l}$$</span></p>
<h2 id="最小作用量原理和lagrange方程">最小作用量原理和lagrange方程</h2>
<p>从最小作用量原理（Hamilton）导出Lagrange形式的经典力学。在相空间中，设体系在<span
class="math inline"><em>t</em><sub>1</sub></span>时刻从A点出发，经过某路径<span
class="math inline"><em>q</em>(<em>t</em>)</span>在<span
class="math inline"><em>t</em><sub>2</sub></span>时刻到达点B。对于每条可能的路径<span
class="math inline"><em>q</em>(<em>t</em>)</span>，可以定义作用量（action）为：</p>
<p><span
class="math display"><em>S</em>[<em>q</em>(<em>t</em>)] = ∫<sub><em>t</em><sub>1</sub></sub><sup><em>t</em><sub>2</sub></sup><em>L</em>(<em>q̇</em>, <em>q</em>)𝕕<em>t</em></span></p>
<p>可以看出作用量是一个标量，为路径的泛函。</p>
<p><strong>最小作用量原理</strong>在理论上有很多条路径可以实现从A点到B点的目的，但是实际上只存在一条真是的路径，这条路径应当满足如下要求：
<span
class="math display"><em>q</em><sub><em>t</em><em>r</em><em>u</em><em>e</em></sub> = argmin<sub><em>q</em></sub>{<em>S</em>[<em>q</em>(<em>t</em>)]}</span></p>
<p>从物理上理解，因为从A到B需要在相空间上需要满足能量最低原理。因此最小作用量原理本质上就是满足前者要求。</p>
<p>故而此问题成为一个有约束的变分问题。考虑路径上可能存在一个虚位移<span
class="math inline"><em>q</em>(<em>t</em>) → <em>q</em>(<em>t</em>) + <em>δ</em><em>q</em>(<em>t</em>)</span>，在A、B两点由于固定的原因，不存在虚位移<span
class="math inline"><em>δ</em><em>q</em>(<em>t</em><sub><em>A</em></sub>) = <em>δ</em><em>q</em>(<em>t</em><sub><em>B</em></sub>) = 0</span>。对作用量公式进行变分：</p>
<p><span class="math display">$$\begin{align}
\delta S &amp;=\int_{t_1}^{t_2}\mathbb{d}t\sum_{i=1}^{s}\{
\frac{\partial L}{\partial q_i}\delta q_i+\frac{\partial L}{\partial
\dot{q}_i}\delta \dot{q}_i\} \\
&amp;= \int_{t_1}^{t_2}\mathbb{d}t\sum_{i=1}^{s}\{ \frac{\partial
L}{\partial q_i}\delta q_i+\frac{\partial L}{\partial \dot{q}_i}\delta
\frac{d q_i}{dt}\} \\
&amp;= \delta\frac{\partial L}{\partial \dot{q}_i} \frac{d
q_i}{dt}|_{t_1}^{t_2} + \int_{t_1}^{t_2}\mathbb{d}t\sum_{i=1}^{s}\{
\frac{\partial L}{\partial q_i}\delta q_i-\frac{d}{dt}\frac{\partial
L}{\partial \dot{q}_i}\delta q_i \} \\
&amp;= \int_{t_1}^{t_2}\mathbb{d}t\sum_{i=1}^{s}\{ \frac{\partial
L}{\partial q_i}-\frac{d}{dt}\frac{\partial L}{\partial \dot{q}_i} \}
\delta q_i\\
\end{align}$$</span></p>
<p>得到最终的Lagrange方程： <span class="math display">$$\frac{\partial
L}{\partial q_i}-\frac{d}{dt}\frac{\partial L}{\partial \dot{q}_i}=0,
\forall i=1,2,\cdots s$$</span></p>
<p>如果受到外力<span
class="math inline"><em>f</em><sub><em>i</em></sub><sup><em>e</em><em>x</em><em>t</em></sup></span>，只需要在右边加上外力项：
<span class="math display">$$\frac{\partial L}{\partial
q_i}-\frac{d}{dt}\frac{\partial L}{\partial \dot{q}_i}=f^{ext}_i,
\forall i=1,2,\cdots s$$</span></p>
<h2 id="hamilton正则方程">Hamilton正则方程</h2>
<p>利用勒让德变换（Legendre），将一组变量转化为另一组变量。将<span
class="math inline"><em>L</em>(<em>q̇</em>, <em>q</em>, <em>t</em>)</span>中的变量<span
class="math inline"><em>q̇</em></span>变为<span class="math inline">$p_i
= \frac{\partial L}{\partial \dot q_i}$</span>，记<span
class="math inline"><em>p</em> = {<em>p</em><sub><em>i</em></sub>}</span>，定义哈密顿量（Hamiltonian）：</p>
<p><span
class="math display">$$\begin{equation}H(q,p,t)=\sum_{t=1}^{s}\dot
q_ip_i - L(\dot q, q, t) \end{equation}$$</span></p>
<p>存在以下关系： <span class="math display">$$\begin{align}
\frac{\partial H}{\partial t}&amp;=\frac{\partial (\dot
q_ip_i)}{\partial t} -\frac{\partial L}{\partial t} \\
&amp;= -\frac{\partial L}{\partial t} \\
\frac{\partial H}{\partial q_i}&amp;=-\frac{\partial L}{\partial q_i} \\
\frac{\partial H}{\partial \dot q_i}&amp;=0 =  p_i -\frac{\partial
L}{\partial \dot q_i} \\
\frac{\partial H}{\partial p_i}&amp;= \dot q_i\\
\end{align}$$</span></p>
<p>将前两式代入Lagrange方程，可以得到： <span
class="math display">$$\begin{align}
-\frac{\partial H}{\partial q_i}-\frac{d}{dt}p_i&amp;=0, \forall
i=1,2,\cdots s  \\
\dot p_i &amp;= -\frac{\partial H}{\partial q_i}
\end{align}$$</span></p>
<p>得到Hamiltonian正则方程： <span class="math display">$$\begin{align}
\dot p_i &amp;= -\frac{\partial H}{\partial q_i} \\
\dot q_i &amp;= \frac{\partial H}{\partial p_i}
\end{align}$$</span></p>
<h2
id="最小作用量原理与hamilton正则方程">最小作用量原理与Hamilton正则方程</h2>
<p>Hamilton正则方程的导出，同样可以利用最小作用量原理得到。由式（7）、（8）和（13）可以得到：</p>
<p><span class="math display">$$\begin{align}
\delta S &amp;= \delta\int_{t_1}^{t_2}L\mathbb{d}t \\
&amp;= \delta\int_{t_1}^{t_2}(\sum_{i=1}^{s}\dot q_ip_i-H)\mathbb{d}t \\
&amp;= \int_{t_1}^{t_2}\mathbb{d}t\left( \sum_{i=1}^{s}\left(\delta \dot
q_ip_i+\dot q_i\delta p_i\right)-\delta H\right) \\
&amp;= \int_{t_1}^{t_2}\mathbb{d}t\left( \sum_{i=1}^{s}\left(\delta \dot
q_ip_i+\dot q_i\delta p_i\right)-\left(\frac{\delta H}{\delta p}\delta
p+\frac{\delta H}{\delta q}\delta q\right)\right) \\
&amp;= \int_{t_1}^{t_2}\mathbb{d}t \sum_{i=1}^{s}\left(\delta \dot
q_ip_i+\dot q_i\delta p_i-\frac{\delta H}{\delta p_i}\delta
p_i-\frac{\delta H}{\delta q_i}\delta q_i\right)\\
&amp;= \int_{t_1}^{t_2}\mathbb{d}t \sum_{i=1}^{s}\left(\delta \dot
q_ip_i+\dot q_i\delta p_i-\dot q_i\delta p_i-\frac{\delta H}{\delta
q_i}\delta q_i\right)\\
&amp;= \int_{t_1}^{t_2}\mathbb{d}t \sum_{i=1}^{s}\left(\delta \dot
q_ip_i-\frac{\delta H}{\delta q_i}\delta q_i\right)\\
&amp;= \int_{t_1}^{t_2}\mathbb{d}t \sum_{i=1}^{s}\left(-\delta q_i\dot
p_i-\frac{\delta H}{\delta q_i}\delta q_i\right)\\
\end{align}$$</span></p>
<p>根据<span
class="math inline"><em>δ</em><em>S</em> = 0</span>可以得出： <span
class="math display">$$\dot p_i=-\frac{\delta H}{\delta
q_i}$$</span></p>
<h2 id="概率分布函数与liouville方程">概率分布函数与Liouville方程</h2>
<p>把香农熵（shannon）作为统计力学基础，自然的把纯态和混合态的概念联合在一起。</p>
<h3 id="经典力学的纯态和混合态">经典力学的纯态和混合态</h3>
<p>在经典力学中有两种描述方法：</p>
<ul>
<li>纯态方法：将微观状态看为是相空间中的的一个相点，体系随时间的演化就是相空间中的一条曲线。这种观点把体系瞬时间所处的微观状态看成是单个纯粹的微观状态。</li>
<li>混合态方法：引入概率的思想，将体系瞬时间所处的状态，认为是以概率分布在一组微观状态的概率云上，用相空间的概率密度函数<span
class="math inline"><em>f</em>(<em>q</em>(<em>t</em>), <em>p</em>(<em>t</em>), <em>t</em>)</span>来描述。体系随时间的变化，就是这团概率云（概率密度函数）随时间的变化。</li>
</ul>
<p>混合态描述是纯态描述的推广，纯态描述是混合态描述的特例。</p>
<p>从混合态的角度分析，概率的分布应该存在不确定度<span
class="math inline"><em>H</em></span>。如果是纯态，那么必然是确定的<span
class="math inline"><em>H</em><sub><em>m</em><em>i</em><em>n</em></sub> = 0</span>，不存在不确定度；如果是均匀的混合态，那么此时不确定度应改是最大的。因此不确定度应改满足以下的条件：</p>
<ol type="1">
<li>最小条件。当<span
class="math inline">{<em>p</em><sub><em>i</em></sub>}</span>中只有一个结果的概率为<span
class="math inline">1</span>，其余均为<span
class="math inline">0</span>时，得到最大的信息量，最小的不确定度<span
class="math inline"><em>H</em><sub><em>m</em><em>i</em><em>n</em></sub> = 0</span>。</li>
<li>最大值条件。当<span
class="math inline"><em>p</em><sub><em>i</em></sub></span>均为常数，信息量必为<span
class="math inline">0</span>，即最大不确定度<span
class="math inline"><em>H</em> = <em>H</em><sub><em>m</em><em>a</em><em>x</em></sub></span>。</li>
<li>对称条件。若交换两个结果的次序，不确定度不发生改变： <span
class="math display"><em>H</em>(⋯<em>p</em><sub><em>i</em></sub>⋯<em>p</em><sub><em>j</em></sub>⋯) = <em>H</em>(⋯<em>p</em><sub><em>j</em></sub>⋯<em>p</em><sub><em>i</em></sub>⋯)</span></li>
<li>加性条件。若将两个独立的随机变量A和B看成是一个联合事件，则联合事件给出的不确定性应当是各随机事件不确定性之和：
<span
class="math display"><em>H</em>(<em>A</em> ∪ <em>B</em>) = <em>H</em>(<em>A</em>) + <em>H</em>(<em>B</em>)</span></li>
</ol>
<p>针对以上条件shannon证明唯一解是： <span
class="math display">$$H=-K\sum_{j=1}^{n}p_j\ln p_j$$</span> <span
class="math inline"><em>K</em></span>为任意常数。</p>
<h3 id="概率分布函数">概率分布函数</h3>
<p>测不准原理表明，粒子微观状态在相空间中一个自由度上是占有一个宽度的<span
class="math inline">△<em>p</em><sub><em>i</em></sub> △ <em>q</em><sub><em>i</em></sub> = <em>h</em></span>。对于自由度为<span
class="math inline"><em>s</em></span>的粒子，一个粒子在微观状态中占有的体积，即“相胞”体积为<span
class="math inline">$\prod_{i=1}^{s}\bigtriangleup p_i\bigtriangleup
q_i=h^s$</span>。只要粒子落入这个相胞内，便无法区分。</p>
<p>对于三维体系中<span
class="math inline"><em>N</em></span>个粒子的体系，其体系相空间的相胞为<span
class="math inline"><em>h</em><sup>3<em>N</em></sup></span>，再因为是全同的体系，可以得到一个相体积元代表的微观粒子数：</p>
<p><span
class="math display">$$\mathbb{d}\Gamma=\frac{dpdq}{h^{3N}N!}=\frac{1}{h^{3N}N!}\prod_{1}^{N}dp_i
dq_i$$</span></p>
<p>设<span class="math inline"><em>t</em></span>时刻体系微观状态在<span
class="math inline"><em>Γ</em></span>相空间的相体积元<span
class="math inline"><em>d</em><em>Γ</em></span>中出现的概率为：</p>
<p><span
class="math display"><em>d</em><em>w</em> = <em>f</em>(<em>q</em>, <em>p</em>, <em>t</em>)<em>d</em><em>Γ</em></span></p>
<p>其中<span
class="math inline"><em>f</em>(<em>q</em>, <em>p</em>, <em>t</em>)</span>称为<span
class="math inline"><em>Γ</em></span>相空间体系状态的<strong>概率分布函数</strong>，也称为<strong>系综分布函数</strong>，简称<strong>分布函数</strong>或者<strong>概率密度</strong>。</p>
<p><span class="math display">$$\begin{align}
\int_{\Gamma}f(p,q,t)d\Gamma &amp;= 1 \\
&lt;A(t)&gt;&amp;=\int_{\Gamma}A(p,q)f(p,q,t)d\Gamma
\end{align}$$</span></p>
<h3 id="liouvile方程">Liouvile方程</h3>
<p><strong>Liouvile方程</strong>：概率密度函数<span
class="math inline"><em>f</em>(<em>p</em>, <em>q</em>, <em>t</em>)</span>随时间的变化规律。</p>
<p>相点的速度为 <span class="math display">$$v=\sum_{i=1}^{3N}\dot
q_i\hat{q}_i+\sum_{i=1}^{3N}\dot p_i\hat{p}_i$$</span> 其中<span
class="math inline"><em>q̂</em>, <em>p̂</em></span>是方向向量。</p>
<p>固定区域<span class="math inline"><em>V</em></span>内的状态点的增速：
<span class="math display">$$\frac{\partial}{\partial t}\int_V f
d\Gamma$$</span></p>
<p>从固定区域<span
class="math inline"><em>V</em></span>中流出的状态点速度： <span
class="math display">∮<sub><em>S</em></sub><em>d</em><em>s</em> ⋅ <em>v</em><em>f</em> = ∫<sub><em>V</em></sub><em>d</em><em>Γ</em>∇ ⋅ (<em>f</em><em>v</em>)</span>
其中<span
class="math inline"><em>d</em><em>s</em></span>是表面向量，规定方向为外法向。</p>
<p>因为<span
class="math inline"><em>Γ</em></span>空间中不存在生成状态的源，也不存在消灭状态的黑洞。因此在一个区域中，流出的状态与增加的状态应该是一样的。因此存在关系：
<span class="math display">$$0=\frac{\partial}{\partial t}\int_V f
d\Gamma+\int_V d\Gamma \nabla\cdot(fv)=\int_V
d\Gamma\left(\frac{\partial f}{\partial
t}+\nabla\cdot(fv)\right)$$</span></p>
<p>对于任意区域均成立，需要满足： <span
class="math display">$$\frac{\partial f}{\partial
t}+\nabla\cdot(fv)=0$$</span></p>
<p>而<span class="math inline">$\frac{\partial f}{\partial
t}=-\nabla\cdot(fv)=-\sum_{i=1}^{3N}\left(\frac{\partial (f\dot
q_i)}{\partial q_i}+\frac{\partial (f\dot p_i)}{\partial
p_i}\right)=-\sum_{i=1}^{3N}\left(\frac{\dot q_i\partial f}{\partial
q_i}+\frac{f\partial \dot q_i}{\partial q_i}+\frac{\dot p_i\partial
f}{\partial p_i}+\frac{f\partial \dot p_i}{\partial
p_i}\right)=-\sum_{i=1}^{3N}\left(\frac{\dot q_i\partial f}{\partial
q_i}+\frac{\dot p_i\partial f}{\partial
p_i}\right)$</span>再根据Hamilton正则方程可以得到：</p>
<p><span class="math display">$$\begin{align}
\frac{\partial f}{\partial t} &amp;= -\sum_{i=1}^{3N}\left(\frac{\dot
q_i\partial f}{\partial q_i}+\frac{\dot p_i\partial f}{\partial
p_i}\right) \\
&amp;= -\sum_{i=1}^{3N}\left(\frac{ \partial f}{\partial
q_i}\frac{\partial H}{\partial p_i}-\frac{\partial f}{\partial
p_i}\frac{\partial H}{\partial q_i}\right) \\
&amp;= \{H,f\}
\end{align}$$</span></p>
<p>上述方程称为Liouvile方程。其中Poisson括号定义为<span
class="math inline">$\{A, B\}=\sum_{i=1}^{3N}\left(\frac{ \partial
A}{\partial q_i}\frac{\partial B}{\partial p_i}-\frac{\partial
A}{\partial p_i}\frac{\partial B}{\partial q_i}\right)$</span></p>
<h3 id="经典liouville算符的形式解">经典Liouville算符的形式解</h3>
<p>形式解就是解析解。如果经典Liouville算符<span
class="math inline"><em>L</em></span>与时间无关，并且<span
class="math inline"><em>f</em>(<em>q</em>, <em>p</em>, <em>t</em> = 0)</span>已知，可以得到<span
class="math inline">$\frac{\partial f}{\partial
t}|_{t=0}=(-iL)f|_{t=0}$</span>，重复计算<span
class="math inline">∞</span>次得到<span
class="math inline">$\frac{\partial^n f}{\partial t^n}|_{t=0}=(-iL)^n
f|_{t=0}$</span>，接下来将<span
class="math inline"><em>f</em>(<em>q</em>, <em>p</em>, <em>t</em>)</span>展开：</p>
<p><span class="math display">$$\begin{align}
f(q,p,t)&amp;=\sum_{n=0}^{\infty}\frac{t^n}{n!}(\frac{\partial^n
f}{\partial t^n})_{t=0} \\
&amp;=\sum_{n=0}^{\infty}\frac{t^n}{n!}(-iL)^nf|_{t=0} \\
&amp;= e^{-iLt}f(q,p,0)
\end{align}$$</span></p>
<p>因此：</p>
<p><span
class="math display"><em>f</em>(<em>q</em>, <em>p</em>, <em>t</em>) = <em>e</em><sup>−<em>i</em><em>L</em><em>t</em></sup><em>f</em>(<em>q</em>, <em>p</em>, 0)</span></p>
<h3 id="力学量的时间演化">力学量的时间演化</h3>
<p>任意力学量<span
class="math inline"><em>A</em> = <em>A</em>(<em>q</em>(<em>t</em>), <em>p</em>(<em>t</em>), <em>t</em>)</span>的时间演化可以写为：</p>
<p><span class="math display">$$\begin{align}
\frac{d A}{dt}&amp;=\frac{\partial A}{\partial
t}+\sum_{i=1}^{3N}(\frac{\partial A}{\partial q_i}\dot q_i +
\frac{\partial A}{\partial p_i}\dot p_i) \\
&amp;= \frac{\partial A}{\partial t}+\sum_{i=1}^{3N}(\frac{\partial
A}{\partial q_i}\frac{\partial H}{\partial p_i} + \frac{\partial
A}{\partial p_i}\frac{\partial H}{\partial q_i}) \\
&amp;= \frac{\partial A}{\partial t}+ \{A, H\} \\
&amp;= \frac{\partial A}{\partial t}+ iLA
\end{align}$$</span></p>
<h3 id="任意力学量的平均值">任意力学量的平均值</h3>
<p><span class="math display">$$\begin{align}
\frac{d \langle A\rangle}{dt}=\langle \frac{dA}{dt} \rangle
\end{align}$$</span></p>
<h3 id="不显含时间的物理量">不显含时间的物理量</h3>
<p>如果<span
class="math inline"><em>A</em> = <em>A</em>(<em>q</em>(<em>t</em>), <em>p</em>(<em>t</em>))</span>那么：</p>
<p><span class="math display">$$\begin{align}
\frac{d A}{d t}&amp;=iLA \\
A(q(t), p(t)) &amp;= e^{iLt}A(q(0),p(0))
\end{align}$$</span></p>
<h2 id="经典演化算符时间反演对称性">经典演化算符、时间反演对称性</h2>
<p>针对三维<span
class="math inline"><em>N</em></span>粒子体系，Hamilton写为：</p>
<p><span class="math display">$$H(p,q)=\sum_{j=1}^{N}
\frac{P_j^2}{2m_j}+U(q)$$</span></p>
<ol type="1">
<li>Liouville算符的Hermite性 <span
class="math display"><em>L</em><sup>†</sup> = <em>L</em></span></li>
<li>典微观状态的时间演化。</li>
</ol>
<p>演化状态的解析解为： <span
class="math display"><em>x</em>(<em>t</em>) = <em>e</em><sup><em>i</em><em>L</em><em>t</em></sup><em>x</em>(0)</span>
其中，将<span
class="math inline"><em>e</em><sup><em>i</em><em>L</em><em>t</em></sup></span>称为经典传播子（classical
prograpator），与量子力学中的<span
class="math inline"><em>e</em><sup>−<em>i</em><em>H</em><em>t</em>/ℏ</sup></span>相对应。传播子又称为演化算符。</p>
<p>记演化算符<span
class="math inline"><em>U</em>(<em>t</em><sub>1</sub>, <em>t</em><sub>2</sub>) = <em>U</em>(<em>t</em><sub>2</sub> − <em>t</em><sub>1</sub>) = <em>e</em><sup><em>i</em><em>L</em>(<em>t</em><sub>2</sub> − <em>t</em><sub>1</sub>)</sup></span>，即：
<span
class="math display"><em>U</em>(<em>t</em>) = <em>e</em><sup><em>i</em><em>L</em><em>t</em></sup></span>
于是有<span
class="math inline"><em>x</em>(<em>t</em>) = <em>U</em>(<em>t</em>)<em>x</em>(0)</span></p>
<ol start="3" type="1">
<li><p>演化算符<span
class="math inline"><em>U</em>(<em>t</em>)</span>为酉算符。 <span
class="math display"><em>U</em><sup>†</sup>(<em>t</em>)<em>U</em>(<em>t</em>) = 1</span></p></li>
<li><p>时间反演对称性。</p></li>
</ol>
<p>时间反演对称：时间从<span
class="math inline">0 → <em>t</em></span>的演化过程，与时间从<span
class="math inline"><em>t</em> → 0</span>的过程，满足相同的运动方程，称为具有时间反演对称性。</p>
<p>Hamilton方程具有时间反演对称性。传播子与时间反演对称性等价。
考虑正向过程<span
class="math inline"><em>x</em>(<em>t</em>) = <em>U</em>(<em>t</em>, 0)<em>x</em>(0)</span>，在考虑反向的传播过程：
<span
class="math display"><em>x</em>(0) = <em>U</em>(0, <em>t</em>)<em>x</em>(<em>t</em>) = <em>U</em>(−<em>t</em>)<em>U</em>(<em>t</em>)<em>x</em>(0) = <em>U</em><sup>†</sup>(<em>t</em>)<em>U</em>(<em>t</em>)<em>x</em>(0) = <em>x</em>(0)</span></p>
<h2 id="约化分布函数">约化分布函数</h2>
<h2 id="全同粒子体系的力学量的平均值">全同粒子体系的力学量的平均值</h2>
<p><span
class="math display">⟨<em>B</em>(<em>t</em>)⟩ = ∫<em>B</em>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>⋯<em>x</em><sub><em>N</em></sub>)<em>f</em>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>⋯<em>x</em><sub><em>N</em></sub>)<em>d</em><em>x</em><sub>1</sub><em>d</em><em>x</em><sub>2</sub>⋯<em>d</em><em>x</em><sub><em>N</em></sub></span>
在处理全同体系的时候，任意交换两个粒子的时候，体系不变，具备如下的条件：</p>
<p><span
class="math display"><em>B</em>(<em>x</em><sub>1</sub>⋯<em>x</em><sub><em>i</em></sub>⋯<em>x</em><sub><em>j</em></sub>⋯<em>x</em><sub><em>N</em></sub>) = <em>B</em>(<em>x</em><sub>1</sub>⋯<em>x</em><sub><em>j</em></sub>⋯<em>x</em><sub><em>i</em></sub>⋯<em>x</em><sub><em>N</em></sub>), ∀<em>i</em> ≠ <em>j</em></span></p>
<p>这种交换对称性可以展开为： <span
class="math display">$$\begin{equation}
B(x_1, x_2 \cdots x_N) = \sum_{n=0}^N B_n(x_1,x_2\cdots x_n)
\end{equation}$$</span></p>
<p>其中： <span class="math display">$$\begin{align}
B_0 &amp;= b_0 \\
B_1 &amp;= b_1(x_1)+b_1(x_2)+\cdots +b_1(x_N)=Nb_1(x) \\
B_2 &amp;= b_2(x_1, x_2)+\cdots+b_2(x_1, x_N)+b_2(x_2,
x_3)+\cdots+b_2(x_2, x_N)+\cdots+b_2(x_{N-1}, x_N) \\
&amp;= C_N^2 b_2(x_1, x_2)=\frac{N!}{2!(N-1)!}b_2(x_1, x_2) \\
B_n &amp;= \frac{N!}{n!(N-n)!}b_n(x_1,x_2\cdots x_n)
\end{align}$$</span></p>
<p><span
class="math inline"><em>b</em><sub>2</sub>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>)</span>是不能再分解为<span
class="math inline"><em>x</em><sub>1</sub></span>或者<span
class="math inline"><em>x</em><sub>2</sub></span>的单变量函数，直到，<span
class="math inline"><em>b</em><sub><em>n</em></sub>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>⋯<em>x</em><sub><em>n</em></sub>)</span>不能再分解为更少变量的函数。通过以上的构造，可以理解为转化为一种具备轮换对称性的基底，进行函数的展开。</p>
<p>在计算力学量平均值的时候： <span class="math display">$$\begin{align}
\langle B(t)\rangle &amp;=  \sum_{n=0}^N \langle B_n(x_1,x_2\cdots
x_n)\rangle \\
&amp;= \sum_{n=0}^N \frac{N!}{n!(N-n)!}\langle b_n(x_1,x_2\cdots
x_n)\rangle \\
&amp;= \sum_{n=0}^N \frac{N!}{n!(N-n)!} \int b_n(x_1, x_2\cdots
x_n)f_n(x_1, x_2\cdots x_n)dx_1 dx_2\cdots dx_n
\end{align}$$</span></p>
<h2
id="bogoliubov-born-green-kirkwood-yvon级联方程">Bogoliubov-Born-Green-Kirkwood-Yvon级联方程</h2>
<p>Liouvile方程是关于N粒子的运动方程，涉及<span
class="math inline">3<em>N</em> + 1</span>个变量，蕴含的信息远超过宏观观测量需要的信息，有必要进行进一步的提取关键信息，即从分布函数<span
class="math inline"><em>f</em></span>的规律求导<span
class="math inline"><em>n</em></span>阶约化分布函数的<span
class="math inline"><em>f</em><sub><em>n</em></sub></span>的规律。</p>
<p><span
class="math inline"><em>H</em><sub><em>N</em></sub></span>为该<span
class="math inline"><em>N</em></span>粒子体系的Hamilton量，即： <span
class="math display">$$\begin{align}
H_N=\sum_{i=1}^{N}\left [ \frac{p_i^2}{2m}+V_{ext}(r_i)\right
]+\sum_{i&lt;j&gt;}^{N}V_{i j}(r_i,r_j)
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>V</em><sub><em>e</em><em>x</em><em>t</em></sub>(<em>r</em><sub><em>i</em></sub>)</span>为第i个粒子受到的外势，<span
class="math inline"><em>V</em><sub><em>i</em><em>j</em></sub></span>表示存在一个两体的相互作用。以下导出<span
class="math inline"><em>n</em></span>阶约化分布函数<span
class="math inline"><em>f</em><sub><em>n</em></sub></span>的时间演化规律。</p>
<p><span class="math display">$$\begin{align}
\frac{\partial f_n}{\partial t} &amp;= \frac{\partial }{\partial t}\int
f_N(x_1, x_2 \cdots x_N, t)d x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;= \int \frac{\partial }{\partial t}f_N(x_1, x_2 \cdots x_N, t)d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;= \int \left \{ H_N, f_N\right \}d x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;= \int \left \{ \sum_{i=1}^{N}\left [
\frac{p_i^2}{2m}+V_{ext}(r_i)\right ], f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} +\int \left \{ \sum_{i&lt;j}^{N}V_{i
j}(r_i,r_j), f_N\right \}d x_{n+1}dx_{n+2}\cdots dx_{N}
\end{align}$$</span></p>
<p>其中第一项有<span
class="math inline">$\sum_{i=1}^N(\cdot)=\sum_{i=1}^n(\cdot)+\sum_{i=n+1}^N(\cdot)$</span>，将上式第一项进行化简：
<span class="math display">$$\begin{align*}
&amp;\int \left \{ \sum_{i=1}^{N}\left [
\frac{p_i^2}{2m}+V_{ext}(r_i)\right ], f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;=\int \left \{ \sum_{i=1}^{n}\left [
\frac{p_i^2}{2m}+V_{ext}(r_i)\right ]+\sum_{i=n+1}^{N}\left [
\frac{p_i^2}{2m}+V_{ext}(r_i)\right ], f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;= \int \left \{ \sum_{i=1}^{n}\left [
\frac{p_i^2}{2m}+V_{ext}(r_i)\right ], f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;+ \sum_{i=n+1}^{N}\int \left \{ \left [
\frac{p_i^2}{2m}+V_{ext}(r_i)\right ], f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;= \int \left \{ \sum_{i=1}^{n}\left [
\frac{p_i^2}{2m}+V_{ext}(r_i)\right ], f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;+ \sum_{i=n+1}^{N}\int \left \{ V_{ext}(r_i), f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;+ \sum_{i=n+1}^{N}\int \left \{ \frac{p_i^2}{2m}, f_N\right \} d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;= \int \left \{ \sum_{i=1}^{n}\left [
\frac{p_i^2}{2m}+V_{ext}(r_i)\right ], f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;+ \sum_{i=n+1}^{N}\int \sum_{k=1}^{N} \left [ \nabla_{r_k}
V_{ext}(r_i)\nabla_{p_k}f_N - \nabla_{p_k}
V_{ext}(r_i)\nabla_{r_k}f_N\right ]d x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;+ \sum_{i=n+1}^{N}\int \sum_{k=1}^{N}\left [
\nabla_{r_k}\frac{p_i^2}{2m}\nabla_{p_k}f_N-\nabla_{p_k}\frac{p_i^2}{2m}\nabla_{r_k}f_N\right
] d x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;= \int \left \{ \sum_{i=1}^{n}\left [
\frac{p_i^2}{2m}+V_{ext}(r_i)\right ], f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;+ \sum_{i=n+1}^{N}\int \sum_{k=1}^{N}\left [ \nabla_{r_k}
V_{ext}(r_i)\nabla_{p_k}f_N
-\nabla_{p_k}\frac{p_i^2}{2m}\nabla_{r_k}f_N\right ] d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;= \int \left \{ \sum_{i=1}^{n}\left [
\frac{p_i^2}{2m}+V_{ext}(r_i)\right ], f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;+ \sum_{i=n+1}^{N}\int \left [ \nabla_{r_i}
V_{ext}(r_i)\nabla_{p_i}f_N -\frac{p_i}{m}\nabla_{r_i}f_N\right ] d
x_{n+1}dx_{n+2}\cdots dx_{N} \text{使用Guass公式消去}\\
&amp;= \int \left \{ \sum_{i=1}^{n}\left [
\frac{p_i^2}{2m}+V_{ext}(r_i)\right ], f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
\end{align*}$$</span></p>
<p>接下来讨论第二项，存在<span class="math inline">$\sum_{i &lt;
j}^N(\cdot)=\sum_{i &lt;
j}^n(\cdot)+\sum_{i=1}^n\sum_{j=n+1}^{N}(\cdot)$</span>，那么：</p>
<p><span class="math display">$$\begin{align*}
&amp;\int \left \{ \sum_{i&lt;j}^{N}V_{i j}(r_i,r_j), f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;= \int \left \{ \sum_{i&lt;j}^{N}V_{i j}(r_i,r_j), f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;+ \int \left \{ \sum_{i=1}^{n}\sum_{j=n+1}^{N}V_{i j}(r_i,r_j),
f_N\right \}d x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;= \int \left \{ \sum_{i&lt;j}^{N}V_{i j}(r_i,r_j), f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;+ \int  \sum_{i=1}^{n}\sum_{j=n+1}^{N}\nabla_{r_i}V_{i
j}(r_i,r_j)\cdot\nabla_{p_i}f_Nd x_{n+1}dx_{n+2}\cdots dx_{N} \\
\end{align*}$$</span></p>
<p>最后得到： <span class="math display">$$\begin{align}
\frac{\partial f_n}{\partial t} &amp;= \int \left \{ \sum_{i=1}^{n}\left
[ \frac{p_i^2}{2m}+V_{ext}(r_i)\right ], f_N\right \}d
x_{n+1}dx_{n+2}\cdots dx_{N} + \int \left \{ \sum_{i&lt;j}^{N}V_{i
j}(r_i,r_j), f_N\right \}d x_{n+1}dx_{n+2}\cdots dx_{N} +
\int  \sum_{i=1}^{n}\sum_{j=n+1}^{N}\nabla_{r_i}V_{i
j}(r_i,r_j)\cdot\nabla_{p_i}f_Nd x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;= \int \left \{ H_n, f_N\right \}d x_{n+1}dx_{n+2}\cdots dx_{N} +
\int  \sum_{i=1}^{n}\sum_{j=n+1}^{N}\nabla_{r_i}V_{i
j}(r_i,r_j)\cdot\nabla_{p_i}f_Nd x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;= \left \{ H_n, f_n\right \} +
(N-n)\int  \sum_{i=1}^{n}\nabla_{r_i}V_{i,j=(n+1)}\cdot\nabla_{p_i}f_Nd
x_{n+1}dx_{n+2}\cdots dx_{N} \\
&amp;= \left \{ H_n, f_n\right \} +
(N-n)\int  \sum_{i=1}^{n}\nabla_{r_i}V_{i,j=(n+1)}\cdot\nabla_{p_i}f_{n+1}dx_{n+1}
\\
\end{align}$$</span></p>
<p>上述方程为级联方程，其将<span
class="math inline"><em>f</em><sub><em>n</em></sub></span>与<span
class="math inline"><em>f</em><sub><em>n</em> + 1</sub></span>联系在一起。</p>
<h1 id="平衡态系综">平衡态系综</h1>
<p>平衡态的系综原理，解决达到平衡态的体系热力学性质的微观解释。</p>
<h2 id="微正则系综">微正则系综</h2>
<h3 id="等概率原理和微正则系综">等概率原理和微正则系综</h3>
<p><strong>孤立系统</strong>：体系与环境不存在任何形式的相互作用。</p>
<p><strong>等概率原理</strong>（平衡态统计力学的唯一假设）：处于平衡态下的孤立系统，体系的各个可能的微观状态出现的概率相等。表现为：
<span class="math display">$$\begin{align}
f=C\delta(E-H(q,p))
\end{align}$$</span> 其中<span
class="math inline"><em>C</em></span>的定义为： <span
class="math display">$$\begin{align}
1=\int_{\tau}fdpdq=\lim_{\Delta E\to 0}C\int_{\Delta E} dqdp
\end{align}$$</span></p>
<p>任意力学量的<span
class="math inline">⟨<em>B</em>(<em>q</em>, <em>p</em>)⟩</span>的系综平均值为：
<span class="math display">$$\begin{align}
\langle B(q,p)\rangle=\int_{\tau}fB(q,p)dpdq=\lim_{\Delta E\to
0}C\int_{\Delta E} B(q,p)dqdp
\end{align}$$</span></p>
<h3 id="poinacre定理">Poinacre定理</h3>
<p><strong>Poinacre回归定理</strong>：对于等能面内的体积有限大的宏观系统，假定其Hamilton量有界，则其<span
class="math inline">|<em>p</em>|,|<em>q</em>|</span>均有界。若时间<span
class="math inline"><em>t</em> = 0</span>时，体系从<span
class="math inline"><em>τ</em></span>相空间的某点<span
class="math inline"><em>P</em><sub>0</sub></span>出发，则体系在一有限的时间<span
class="math inline"><em>T</em></span>内，必然会经过<span
class="math inline"><em>P</em><sub>0</sub></span>点足够近的邻近点<span
class="math inline"><em>P</em><sup>′</sup><sub>0</sub></span>，其距离小于任意小的预设正数<span
class="math inline"><em>ϵ</em></span>，即<span
class="math inline">|<em>P</em><sub>0</sub><em>P</em><sup>′</sup><sub>0</sub>| &lt; <em>ϵ</em></span>。</p>
<h3 id="等概率原理和最大熵原理">等概率原理和最大熵原理</h3>
<p>本节主要证明：最大熵原理等价于孤立系统达到平衡态后，体系微观状态在等能面上呈等概率分布的假设。</p>
<p>时刻<span class="math inline"><em>t</em></span>系综处于<span
class="math inline"><em>i</em></span>态的样本数目<span
class="math inline"><em>N</em><sub><em>i</em></sub></span>在<span
class="math inline"><em>d</em><em>t</em></span>时间中，离开与增加的变化量以及净变化量：
<span class="math display">$$\begin{align}
(dN_i)_{-} &amp;= \sum_{j(\neq i)}N_i P_{i\to j}dt \\
(dN_i)_{+} &amp;= \sum_{j(\neq i)}N_j P_{j\to i}dt \\
dN_i &amp;= (dN_i)_{+} - (dN_i)_{-} \\
&amp;= \sum_{j(\neq i)}(N_j-N_i) P_{j\to i}dt \\
\frac{dN_i}{dt} &amp;= \sum_{j(\neq i)}(N_j-N_i) P_{j\to i} \\
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>P</em><sub><em>i</em> → <em>j</em></sub></span>表示从<span
class="math inline"><em>i</em></span>跃迁到<span
class="math inline"><em>j</em></span>的概率。同样可以得到： <span
class="math display">$$\begin{align}
\dot p_i = \frac{dp_i}{dt} = \sum_{j(\neq i)}(p_j-p_i) P_{j\to i}
\end{align}$$</span> 根据信息熵的定义，定义体系的量<span
class="math inline"><em>S</em></span>:</p>
<p><span class="math display">$$\begin{align}
S(t) &amp;= -k_B\sum_i p_i(t)\ln p_i(t)=-k_B \langle \ln p_i(t) \rangle
\\
\dot S &amp;= -k_B\sum_i \dot p_i（\ln p_i+1）\\
&amp;=-k_B\sum_i \dot p_i\ln p_i \\
&amp;= -k_B\sum_i \sum_{j(\neq i)}(p_j-p_i) P_{j\to i}\ln p_i \\
&amp;= -k_B\sum_i \sum_{j}(p_j-p_i) \ln p_i P_{j\to i}\\
&amp;= \frac{-k_B}{2}\sum_i \sum_{j} \left [ (p_j-p_i) \ln p_i P_{j\to
i} +(p_i-p_j) \ln p_j P_{i\to j}\right ]\\
&amp;= \frac{k_B}{2}\sum_i \sum_{j} (p_i-p_j) (\ln p_i -\ln p_j)P_{i\to
j}\\
\end{align}$$</span></p>
<p>因为<span
class="math inline"><em>P</em><sub><em>i</em> → <em>j</em></sub> &gt; 0</span>，可知<span
class="math inline"><em>Ṡ</em> &gt; 0</span>，<span
class="math inline"><em>S</em></span>是一个恒增的量。当任意的跃迁量之间满足:
<span class="math display">$$\begin{align}
p_i=p_j=\cdots=\frac{1}{\Omega}
\end{align}$$</span> 就会有<span
class="math inline"><em>Ṡ</em> = 0</span>，达到最终的平衡态。即得到<span
class="math inline"><em>S</em></span>的最大值，Gibbs熵： <span
class="math display">$$\begin{align}
S=k_B \ln \Omega
\end{align}$$</span></p>
<h2 id="正则系综">正则系综</h2>
<p><strong>正则系综</strong>：体积<span
class="math inline"><em>V</em></span>、粒子数<span
class="math inline"><em>N</em></span>和温度<span
class="math inline"><em>T</em></span>恒定的体系，也称为<span
class="math inline">(<em>N</em>, <em>V</em>, <em>T</em>)</span>体系。</p>
<p>设<span
class="math inline">{<em>H</em><sub><em>i</em></sub>}</span>为体系的Hamilton量，对所有样本求和得到系综的Hamilton量<span
class="math inline"><em>H</em><sub><em>e</em><em>n</em><em>s</em></sub></span>：
<span class="math display">$$\begin{align}
H_{ens}=\sum_{i=1}^{N}H_i+\text{热交换项}
\end{align}$$</span></p>
<p>当体系粒子数不太少，热交换项可以小到忽略。设样本的体系第<span
class="math inline"><em>j</em></span>种状态的能量为<span
class="math inline"><em>E</em><sub><em>j</em></sub></span>，又设系综中同属于第<span
class="math inline"><em>j</em></span>种状态的样本体系数为<span
class="math inline"><em>n</em><sub><em>j</em></sub></span>。因此，对所有体系状态求和得到系综的总能量：
<span class="math display">$$\begin{align}
\epsilon=\sum_{j}^{\text{state}}n_j E_j
\end{align}$$</span></p>
<p>因为整个正则体系是孤立的，因此系综的能量<span
class="math inline"><em>ϵ</em></span>是不变的。又对所有的体系状态<span
class="math inline"><em>n</em><sub><em>j</em></sub></span>求和，得到系综中的样本样本体系总数<span
class="math inline"><em>N</em></span>，即： <span
class="math display">$$\begin{align}
N=\sum_j^{\text{state}}n_j
\end{align}$$</span></p>
<h3 id="正则系综的最可几分布">正则系综的最可几分布</h3>
<p>在给定样本总数<span
class="math inline"><em>N</em></span>和系综能量<span
class="math inline"><em>ϵ</em></span>的约束条件下，寻找分布<span
class="math inline">{<em>n</em><sub><em>j</em></sub>}</span>可以使得正则系综的微观状态<span
class="math inline"><em>W</em></span>达到最大。按照排列组合的原理，给定分布<span
class="math inline">{<em>n</em><sub><em>j</em></sub>}</span>，<span
class="math inline"><em>N</em></span>个样本构成的正则系综的微观状态数为：
<span class="math display">$$\begin{align}
W=\frac{N!}{\prod_j n_j !}
\end{align}$$</span></p>
<p>本质是解决一个变分问题，在满足能量与样本数约束的条件下，寻找<span
class="math inline">{<em>n</em><sub><em>i</em></sub>}</span>分布，使得<span
class="math inline"><em>W</em></span>最大，可以写为： <span
class="math display">$$\begin{align}
\Omega = \ln W-\alpha (\sum_j n_j-N)-\beta (\sum_j n_j E_j-\epsilon)
\end{align}$$</span></p>
<p>利用Lagrange乘子法，极值时应满足： <span
class="math display">$$\begin{align}
\frac{\delta \Omega}{\delta n_j}&amp;=0,\forall j \\
\ln W &amp;= \ln \left (\frac{N!}{\prod_j n_j !}\right) \\
&amp;\simeq N\ln N-N-\sum_j (n_j\ln n_j-n_j) \\
\frac{\partial \ln W}{\partial n_j} &amp;= -\ln n_j \\
\frac{\delta \Omega}{\delta n_j}&amp;= -\ln n_j-\alpha-\beta E_j =0  \\
n_j^* &amp;= e^{-\alpha-\beta E_j} \\
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>n</em><sub><em>j</em></sub><sup>*</sup></span>表示最可几分布。因此，体系处于第<span
class="math inline"><em>j</em></span>个态的概率为：</p>
<p><span class="math display">$$\begin{align}
P_j &amp;= \frac{n_j}{N} = \frac{e^{-\alpha-\beta
E_j}}{\sum_i^{\text{state}}e^{-\alpha-\beta E_i}} \\
&amp;= \frac{e^{-\beta E_j}}{\sum_i^{\text{state}}e^{-\beta E_i}} \\
&amp;= \frac{e^{-\beta E_j}}{Z}
\end{align}$$</span></p>
<p>其中<span class="math inline"><em>Z</em></span>是配分函数。
具体的物理量计算可以写为： <span class="math display">$$\begin{align}
\langle B\rangle = \sum_j P_j B_j = \frac{1}{Z}\sum_j B_j e^{-\beta E_j}
\end{align}$$</span></p>
<h3 id="正则系综中的热力学关系">正则系综中的热力学关系</h3>
<ol type="1">
<li>体系平均能量 <span class="math display">$$\begin{align}
\langle E\rangle  = \frac{1}{Z}\sum_j E_j e^{-\beta E_j} =
-\left(\frac{\partial \ln Z}{\partial \beta}\right)_{N,V}
\end{align}$$</span></li>
<li>Helmholtz自由能 <span class="math display">$$\begin{align}
\langle F\rangle  = -\frac{1}{\beta}\ln Z
\end{align}$$</span><br />
</li>
<li>平均压强 <span class="math display">$$\begin{align}
\langle p\rangle  = \frac{1}{\beta}\left(\frac{\partial \ln Z}{\partial
V}\right)_{N,T}
\end{align}$$</span></li>
<li>熵的期望值 <span class="math display">$$\begin{align}
\langle S\rangle  &amp;= \frac{\langle E\rangle}{Z}+k_B\ln Z =
\frac{1}{\beta}\left(\frac{\partial \ln Z}{\partial T}\right)_{N,V} +
k_B\ln Z \\
&amp;= k_B \left [\frac{\partial (T\ln Z)}{\partial T}\right]_{N,V}
\end{align}$$</span></li>
<li>正则系综的微观状态数 <span class="math display">$$\begin{align}
W^*  &amp;=\frac{N!}{\prod_j n_j^* !}= Z^N e^{-\beta \epsilon}
\end{align}$$</span></li>
<li>化学势 <span class="math display">$$\begin{align}
\mu = \frac{-1}{\beta}\left(\frac{\partial \ln Z}{\partial
N}\right)_{V,T}
\end{align}
$$</span></li>
</ol>
<h2 id="巨正则系综">巨正则系综</h2>
<p><strong>巨正则系综</strong>：体积<span
class="math inline"><em>V</em></span>固定，与环境存在能量与粒子数交换。体系的不变量为体积<span
class="math inline"><em>V</em></span>、温度<span
class="math inline"><em>T</em></span>、化学势<span
class="math inline"><em>μ</em></span>。</p>
<p>设<span
class="math inline"><em>H</em>(<em>N</em>)</span>为该粒子数<span
class="math inline"><em>N</em></span>的体系的Hamilton量，<span
class="math inline">|<em>N</em>, <em>j</em>⟩</span>为该体系第<span
class="math inline"><em>j</em></span>个本征态，对应的能量为<span
class="math inline"><em>E</em><sub><em>N</em>, <em>j</em></sub></span>，存在：
<span class="math display">$$\begin{align}
H(N)|N,j\rangle = |N,j\rangle E_{N,j}
\end{align}$$</span></p>
<p>设<span
class="math inline">{<em>H</em><sub><em>i</em></sub>}</span>为样本体系的Hamilton量，对所有样本求和得到系综的Hamilton量<span
class="math inline"><em>H</em><sub><em>G</em><em>C</em><em>E</em></sub></span>为：</p>
<p><span class="math display">$$\begin{align}
H_{GCE}=\sum_{i=1}^A+\text{相互作用}
\end{align}$$</span>
当样本足够大的时候，可以认为相互作用项小到可以忽略。</p>
<p>设巨正则系综中同属于体系粒子数为<span
class="math inline"><em>N</em></span>的第<span
class="math inline"><em>j</em></span>个状态（能量为<span
class="math inline"><em>E</em><sub><em>N</em>, <em>j</em></sub></span>）的样本体系个数为<span
class="math inline"><em>a</em><sub><em>N</em>, <em>j</em></sub></span>，于是在巨正则系综中的所有样本都可以归属于不同的<span
class="math inline">(<em>N</em>, <em>j</em>)</span>值，所有的“GCE的量子态”可以用分布<span
class="math inline">{<em>a</em><sub><em>N</em>, <em>j</em></sub>}</span>来表征。GCE态可以记为<span
class="math inline">|{<em>a</em><sub><em>N</em>, <em>j</em></sub>}⟩</span>。由于整个巨正则系综可以看成一个孤立体系，所以根据统计力学假定，各个GCE的量子态<span
class="math inline">|{<em>a</em><sub><em>N</em>, <em>j</em></sub>}⟩</span>出现的概率是相等的。</p>
<h3 id="单组分gce的最可几分布">单组分GCE的最可几分布</h3>
<p>单组分体系构成巨正则含有的微观状态数为： <span
class="math display">$$W(\{a_{N,j}\})=\frac{A!}{\prod_N\prod_j
a_{N,j}!}$$</span></p>
<p>需要解决的问题是在怎样的分布下<span
class="math inline"><em>W</em></span>达到最大？
已经巨正则守恒条件（系综样本数<span
class="math inline">𝒜</span>、系综能量<span
class="math inline"><em>ϵ</em></span>、系综总粒子数<span
class="math inline">𝒩</span>）为： <span
class="math display">$$\begin{align}
\sum_N \sum_j a_{N,j} &amp;= \mathcal A \\
\sum_N \sum_j a_{N,j} E_{N,j} &amp;= \epsilon \\
\sum_N \sum_j a_{N,j}N &amp;= \mathcal N
\end{align}$$</span></p>
<p>数学目标为： <span
class="math display">max<sub><em>a</em><sub><em>N</em>, <em>j</em></sub></sub>(ln <em>W</em>)</span></p>
<p>Lagrange乘子法： <span class="math display">$$\begin{align}
\ln \Omega &amp;= \ln W -\alpha\left( \sum_{N,j}a_{N,j}-\mathcal
A\right)-\beta \left( \sum_N \sum_j a_{N,j} E_{N,j} - \epsilon\right)
-\gamma \left( \sum_N \sum_j a_{N,j}N - \mathcal N\right) \\
\frac{\delta \ln \Omega}{\delta a_{N,j}} &amp;= 0 \quad \forall N,j \\
\frac{\delta \ln W}{\delta a_{N,j}} &amp;= \frac{\delta }{\delta
a_{N,j}} \left[ \ln \mathcal A!-\sum_{N,j} (a_{N,j}\ln
a_{N,j}-a_{N,j})\right]=-\ln a_{N,j} \\
\frac{\delta \ln \Omega}{\delta a_{N,j}} &amp;= -\ln a_{N,j}-\alpha
-\beta E_{N,j}-\gamma N = 0 \quad \forall N,j \\
\end{align}$$</span></p>
<p>因此最可几分布为：</p>
<p><span
class="math display"><em>a</em><sub><em>N</em>, <em>j</em></sub><sup>*</sup> = <em>e</em><sup>−(<em>α</em> + <em>β</em><em>E</em><sub><em>N</em>, <em>j</em></sub> + <em>γ</em><em>N</em>)</sup>  ∀<em>N</em>, <em>j</em></span></p>
<p>其中利用样本数守恒计算<span
class="math inline"><em>e</em><sup>−<em>α</em></sup></span>： <span
class="math display">$$\begin{align}
\sum_{N,j} a_{N,j} &amp;= \mathcal A=e^{-\alpha}\sum_{N,j} e^{-(\beta
E_{N,j}+\gamma N)} \\
e^{-\alpha} &amp;= \frac{\mathcal A}{\sum_{N,j} e^{-(\beta
E_{N,j}+\gamma N)}} \\
\end{align}$$</span></p>
<p>定义巨配分函数<span class="math inline"><em>Ξ</em></span>为： <span
class="math display"><em>Ξ</em>(<em>V</em>, <em>β</em>, <em>γ</em>) = ∑<sub><em>N</em>, <em>j</em></sub><em>e</em><sup>−(<em>β</em><em>E</em><sub><em>N</em>, <em>j</em></sub> + <em>γ</em><em>N</em>)</sup></span></p>
<p>因此系综的最可几分布： <span class="math display">$$a_{N,j} =
\mathcal A \frac{e^{-(\beta E_{N,j}+\gamma N)}}{\Xi}$$</span>
粒子数为<span class="math inline"><em>N</em></span>的第<span
class="math inline"><em>j</em></span>个状态出现的概率为： <span
class="math display">$$P_{N,j} = \frac{e^{-(\beta E_{N,j}+\gamma
N)}}{\Xi}$$</span></p>
<h3 id="多组分gce的最可几分布">多组分GCE的最可几分布</h3>
<h3 id="lagrange待定乘子beta的确定">Lagrange待定乘子<span
class="math inline"><em>β</em></span>的确定</h3>
<p><span class="math display">$$\beta = \frac{1}{k_B T}$$</span> ###
Lagrange待定乘子<span class="math inline"><em>γ</em></span>的确定 <span
class="math display">$$\gamma_i = \frac{-\mu_i}{k_B T}$$</span> ##
等温等压系综</p>
<h1 id="近独立子体系的统计热力学">近独立子体系的统计热力学</h1>
<p>系综理论具体应用：使用系综理论处理独立子体系和近独立子体系。</p>
<h2 id="独立子体系和近独立子体系">独立子体系和近独立子体系</h2>
<p>从组成粒子的三个角度对宏观体系进行分类，粒子是单组份还是多组分、粒子的位置是否固定、粒子之间是否存在相互作用：</p>
<ol type="1">
<li>按组成粒子的种类分类。单组分为全同粒子体系，多组分为混合粒子体系。</li>
<li>按组成粒子的位置是否固定。位置不固定的称为离域子体系，例如气体；位置固定称为定域子体系，例如晶体。</li>
<li>按有无相互作用分类。粒子之间没有相互作用称为独立子体系；微粒之间存在相互作用称为相倚子体系；粒子之间相互作用非常小，例如气体，称为近独立子体系。</li>
</ol>
<h2 id="粒子的配分函数">粒子的配分函数</h2>
<p>对于只有一个粒子的体系，可以认为是一个正则系综: <span
class="math display">$$\begin{align}
q\equiv \sum_j^{\text{state}}e^{-\beta \epsilon_j} =
\sum_k^{\text{level}}g_k e^{-\beta \epsilon_k}
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>g</em><sub><em>k</em></sub></span>为能级简并度，前一个加和是对状态的加和，后一个加和是对能级的加和。</p>
<p>现在讨论由<span
class="math inline"><em>N</em></span>个单组分全同粒子组成的近独立子体系。有离域与定域两种情况，首先讨论定域的情况。
既然不存在相互作用，每个格点处都有<span
class="math inline"><em>q</em></span>个花样，因此体系的配分函数为：
<span
class="math display"><em>Q</em> = <em>q</em><sup><em>N</em></sup></span>
对于离域的情况，由于粒子间可以轮换，因此需要排除重复计算的项： <span
class="math display">$$Q=\frac{q^N}{N!}$$</span></p>
<p>由上面可知，分析配分函数的关键是知道<span
class="math inline">{<em>ϵ</em><sub><em>k</em></sub>, <em>g</em><sub><em>k</em></sub>}</span>（粒子能谱）。接下来分析单个粒子的运动</p>
<h3
id="分支骨架的运动状态简单体系的量子力学解">分支骨架的运动状态、简单体系的量子力学解</h3>
<p>分子的运动包括分子质心的平动运动、分子绕质心的转动、分子内部组成原子之间的振动和体系电子状态跃迁。
首先考虑电子运动，电子质量远小于核，因此当核发生变化的时候，电子可以快速运动适应核的变化，但核对电子的运动却不敏感。因此Born-Oppenheimer近似将运动其分为两部分，分别是核的薛定谔方程与固定核骨架之后的解电子的薛定谔方程。</p>
<p>对于分子的平动、转动和振动都是核的运动，可以用经典力学或者量子力学求解（在经典近似之后可以得到经典的结果，离散状态变为连续状态）。这三种运动无论多么复杂都可以归结为三种基本的模型：三维盒中的只有平动粒子、刚性转子和一维简谐运动。</p>
<p>此处省略对这三种情况的量子力学解。</p>
<h3 id="分子配分函数的析因子性">分子配分函数的析因子性</h3>
<p>假定分子的各种运动形态之间互为独立，则能量的加和性和简并度的乘积性会使子的配分函数<span
class="math inline"><em>q</em></span>总可以写成各种运动形态的（子的）配分函数<span
class="math inline"><em>q</em><sub><em>k</em></sub></span>的乘积，<span
class="math inline"><em>q</em> = ∏<sub><em>k</em></sub><em>q</em><sub><em>k</em></sub></span>，称之为子的配分函数的析因子性。</p>
<p>根据子的配分函数析因子性，只要用量子力学求得分子的每一种运动形态的能谱<span
class="math inline">{<em>ϵ</em><sub><em>j</em></sub>}</span>，继而依次求得该种运动的子的配分函数<span
class="math inline"><em>q</em><sub><em>k</em></sub></span>和子的配分函数<span
class="math inline"><em>q</em></span>。再求得独立定域（离域）子体系的配分函数<span
class="math inline"><em>Q</em></span>。然后求出所有物理量的系综均值。</p>
<h3 id="粒子平动振动转动的配分函数">粒子平动、振动、转动的配分函数</h3>
<p>讨论粒子平动、振动、转动的配分函数。</p>
<h3 id="bose子fermi子和boltzmann子">Bose子、Fermi子和Boltzmann子</h3>
<ul>
<li>Bose子：每个能量状态上没有容纳粒子数限制，光子、声子。</li>
<li>Fermi子：每个能量状态上最多只能容纳一个粒子，电子、质子、中子。</li>
<li>全同离域Boltzmann子（Boltzon）：当能级简并度远大于占有该能级的粒子数，此时Bose子与Fermi子之间的差距已经不起作用，两者趋同。即量子效应消失，这时就是经典情况。</li>
</ul>
<h2 id="配分函数的经典表述">配分函数的经典表述</h2>
<h3 id="三维平动子配分函数的经典表述">三维平动子配分函数的经典表述</h3>
<h3 id="刚性转子配分函数的经典表述">刚性转子配分函数的经典表述</h3>
<h3 id="一维谐振子配分函数的经典表述">一维谐振子配分函数的经典表述</h3>
<h2 id="平动子体系的分布函数">平动子体系的分布函数</h2>
<h2 id="理想气体的热力学量">理想气体的热力学量</h2>
<h2
id="晶体的定容热容einstein与debye模型">晶体的定容热容、Einstein与Debye模型</h2>
<h2
id="双原子分子的运动成分及其对称性">双原子分子的运动成分及其对称性</h2>
<h2
id="能均分定理双原子分子气体的热容">能均分定理、双原子分子气体的热容</h2>
<h2
id="多原子分子气体的运动和配分函数">多原子分子气体的运动和配分函数</h2>
<h2 id="多原子分子气体的分布函数">多原子分子气体的分布函数</h2>
<h2 id="化学平衡的统计理论">化学平衡的统计理论</h2>
<h2 id="反应速度理论中的统计理论">反应速度理论中的统计理论</h2>
<h1 id="平衡态系综原理在化学中的应用">平衡态系综原理在化学中的应用</h1>
<h2 id="固体的状态方程">固体的状态方程</h2>
<h2 id="外磁场中的气体">外磁场中的气体</h2>
<h2 id="气固吸附">气固吸附</h2>
<h2 id="吸附竞争">吸附竞争</h2>
<h2 id="非理想气体">非理想气体</h2>
<h1 id="相关函数">相关函数</h1>
<ul>
<li>相关或关联（correlation）：物理量在时间或者空间上存在相互联系。</li>
<li>空间相关函数（space correlation function,
SCF）：在体系的某一空间位置处的某种性质通过粒子间的相互作用，对另外一个空间位置处的另一种（或同一种）性质造成影响的程度，体现这两个量在空间上的联系。</li>
<li>相关长度（correlation length）：表征一空间相关性的空间尺度。</li>
<li>时间相关函数（time correlation function,
TCF）：体系在受到环境的某种作用，造成对体系某时刻的某个物理量与此后某时刻的另一个（或同一个）物理量之间的相互影响。</li>
<li>相关时间(correlation time)：表征时间相关性的时间尺度。</li>
<li>空间自相关函数（auto-SCF）：与另一处的同一物理量的联系。</li>
<li>空间交叉相关函数（cross-SCF）：与另一处的<strong>不同</strong>一物理量的联系。</li>
</ul>
<h2 id="空间相关函数">空间相关函数</h2>
<p>体系处于平衡态，物理量的涨落为： <span
class="math display">$$\begin{align}
\Delta A(r) \equiv A(r) - \langle A(r) \rangle
\end{align}$$</span> 显然处于平衡态的体系涨落为零： <span
class="math display">$$\begin{align}
\langle \Delta A(r) \rangle = \langle A(r) - \langle A(r) \rangle
\rangle = 0
\end{align}$$</span> 但是涨落平方的系综平均值具有确定数值，设为<span
class="math inline"><em>a</em></span>，即： <span
class="math display">$$\begin{align}
\langle (\Delta A(r))^2 \rangle = \langle [A(r) - \langle A(r)
\rangle]^2 \rangle = a
\end{align}$$</span>
假定不存在外场，则处于平衡态的体系应当是均匀的，于是<span
class="math inline"><em>a</em></span>与位置无关。</p>
<p>对于体系内的两个空间位置<span
class="math inline"><em>r</em></span>与<span
class="math inline"><em>r</em><sup>′</sup></span>，位置<span
class="math inline"><em>r</em></span>处的物理量<span
class="math inline"><em>A</em>(<em>r</em>)</span>和另一个位置<span
class="math inline"><em>r</em><sup>′</sup></span>处的另一个物理量<span
class="math inline"><em>B</em>(<em>r</em><sup>′</sup>)</span>的乘积的系综平均值为：
<span class="math display">$$\begin{align}
\langle A(r) B(r')\rangle \equiv C_{AB}(r,r')
\end{align}$$</span>
称为这两个量的空间相关函数，描述不同位置上的两个物理量之间的联系，“乘积”反映某种平均化后的特征。
其中一个特例，如果描述的两个物理量相同，称之为空间的自相关函数： <span
class="math display">$$\begin{align}
\langle A(r) A(r')\rangle \equiv C_{AA}(r,r')
\end{align}$$</span></p>
<p>对于平衡态来说，因为具有均匀性，相关函数的绝对位置是不重要的，重要的是相对位置（具有空间位置平移不变性）：
<span class="math display">$$\begin{align}
\langle A(r)B(r')\rangle = \langle A(r-r')B(0) \rangle
\end{align}$$</span></p>
<p>进一步，如果体系是各向同性，则相关函数与位置向量无关，仅与两点间距<span
class="math inline">|<em>r</em> − <em>r</em><sup>′</sup>|</span>有关，一般来说，在一个距离<span
class="math inline"><em>ξ</em></span>内，空间相关性<span
class="math inline"><em>C</em><sub><em>A</em><em>B</em></sub></span>值不小，而大于<span
class="math inline"><em>ξ</em></span>之后相关性变得很小，则长度<span
class="math inline"><em>ξ</em></span>可作为空间相范围的度量，称之为<strong>相关长度</strong>。</p>
<h3
id="位置的概率密度动量的概率密度">位置的概率密度、动量的概率密度</h3>
<h3 id="数密度及其涨落的空间相关函数">数密度及其涨落的空间相关函数</h3>
<p><span
class="math inline"><em>N</em></span>个全同粒子体系的俄数密度<span
class="math inline"><em>ρ</em>(<em>r</em>)</span>指位置<span
class="math inline"><em>r</em></span>处单位体积中的粒子个数，即<span
class="math inline"><em>ρ</em>(<em>r</em>)<em>d</em><em>r</em></span>为体积元<span
class="math inline"><em>d</em><em>r</em></span>中的粒子个数，经典力学意义上可以表示为：
<span
class="math display">$$\rho(r)=\sum_{i=1}^N\delta(r-r_i)$$</span></p>
<p>体系数密度<span
class="math inline"><em>ρ</em>(<em>r</em>)</span>的系综平均值为： <span
class="math display">$$\begin{align}
\langle \rho(r)\rangle &amp;= \int dr\rho(r)f_N(r_1\cdots r_N)=\int dr
\sum_{i=1}^N\delta(r-r_i)f_N(r_1\cdots r_N) \\
&amp;= N\int dr_1 \delta(r-r_i) \int dr_2\cdots r_N f_N(r_1\cdots r_N)
\\
\end{align}$$</span> 引入定义： <span
class="math display"><em>f</em><sub>1</sub>(<em>r</em><sub>1</sub>) ≡ <em>V</em>∫<em>d</em><em>r</em><sub>2</sub>⋯<em>r</em><sub><em>N</em></sub><em>f</em><sub><em>N</em></sub>(<em>r</em><sub>1</sub>⋯<em>r</em><sub><em>N</em></sub>)</span>
可得： <span class="math display">$$\begin{align}
\langle \rho(r)\rangle &amp;= N \int
dr_1\delta(r-r_1)\frac{1}{V}f_1(r_1)=\frac{N}{V}f_1(r) \\
&amp;= \rho f_1(r)
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>f</em><sub>1</sub>(<em>r</em>)</span>的物理含义为；不管其它粒子的分布情况，有一个粒子出现在<span
class="math inline"><em>r</em></span>处的概率再乘以体积。对于均匀体系，这个概率分布显然与位置无关并且等于<span
class="math inline">$\frac{1}{V}$</span>，即对于均匀系： <span
class="math display">$$\begin{align*}
\int d_2\cdots d_N f_N(r_1\cdots r_N)&amp;=\frac{1}{V} \\
f_1(r) &amp;= 1
\end{align*}$$</span></p>
<p>接下来讨论数密度的空间相关性。 <span
class="math display">$$\begin{align}
\langle \rho(r)\rho(r')\rangle &amp;= \int dr \rho(r)\rho (r')
f_N(r_1\cdots r_N) \\
&amp;= \int dr_1\cdots dr_N
\sum_{i=1}^N\sum_{j=1}^N\delta(r-r_i)\delta(r'-r_j)f_N(r_1\cdots r_N) \\
&amp;= \int dr_1\cdots dr_N \left[
\sum_{i=1}^N\delta(r-r_i)\delta(r'-r_i)+\sum_{i\neq
j}\delta(r-r_i)\delta(r-r_j)\right]f_N(r_1\cdots r_N) \\
&amp;= \rho \delta(r-r')+N(N-1)\int dr_1 dr_2
\delta(r-r_1)\delta(r'-r_2) \int dr_3\cdots dr_N f_N(r_1\cdots r_N) \\
&amp;= \rho \delta(r-r')+\frac{N(N-1)}{V^2}\int dr_1 dr_2
\delta(r-r_1)\delta(r'-r_2) f_2(r_1, r_2) \\
&amp;= \rho \delta(r-r') + \rho^2 f_2(r_1, r_2)
\end{align}$$</span></p>
<p>其中定义<span class="math inline"><em>f</em><sub>2</sub></span>为：
<span
class="math display"><em>f</em><sub>2</sub>(<em>r</em><sub>1</sub>, <em>r</em><sub>2</sub>) ≡ <em>V</em><sup>2</sup>∫<em>d</em><em>r</em><sub>3</sub>⋯<em>d</em><em>r</em><sub><em>N</em></sub><em>f</em><sub><em>N</em></sub>(<em>r</em><sub>1</sub>⋯<em>r</em><sub><em>N</em></sub>)</span></p>
<ol type="1">
<li>如果两点距离很大，可认为两点是独立的，即: <span
class="math display">lim<sub>|<em>r</em><sub>1</sub> − <em>r</em><sub>2</sub>| → ∞</sub><em>f</em><sub>2</sub>(<em>r</em><sub>1</sub>, <em>r</em><sub>2</sub>) = 1</span></li>
<li>在均匀体系中有： <span
class="math display"><em>f</em><sub>2</sub>(<em>r</em><sub>1</sub>, <em>r</em><sub>2</sub>) = <em>f</em><sub>2</sub>(<em>r</em><sub>2</sub>, <em>r</em><sub>1</sub>)</span>
进一步为各项同性体系： <span
class="math display"><em>f</em><sub>2</sub>(<em>r</em><sub>1</sub>, <em>r</em><sub>2</sub>) = <em>f</em><sub>2</sub>(|<em>r</em><sub>1</sub>, <em>r</em><sub>2</sub>|)</span></li>
<li>均匀体系数密度涨落的空间自相关函数。均匀体系的数密度涨落为： <span
class="math display">$$\begin{align}
\Delta \rho(r)&amp;\equiv \rho(r)-\rho \\
\langle \Delta\rho(r)\Delta\rho(r')\rangle &amp;=\langle
\rho(r)\rho(r')\rangle-\rho^2 \\
&amp;= \rho \delta(r-r') + \rho^2 [f_2(r_1, r_2)-1]
\end{align}$$</span></li>
</ol>
<h2 id="正则系综中的空间相关函数">正则系综中的空间相关函数</h2>
<p>具体分析<span
class="math inline"><em>N</em></span>个子的正则离域体系，其中其Hamilton为：
<span class="math display">$$H =
\frac{1}{2m}\sum_{i=1}^Np_i^2+U_N+\sum_{i=1}^N\epsilon$$</span>
其中第一项为总动能，第二项为相互作用，第三项为粒子内部的能量项。 ###
约化分布函数 ### 径向分布函数 ### 直接相关函数和Ornstein-Zernike方程</p>
<h2 id="时间相关函数">时间相关函数</h2>
<ul>
<li><strong>时间相关函数</strong>（TCF）：描述前后不同时刻两个物理量之间的相关关系。</li>
<li>相关时间（correlation time）：表征相关性在时间上的尺度。</li>
</ul>
<p>定义时间相关函数<span
class="math inline">⟨<em>B</em>(0)<em>C</em>(<em>t</em>)⟩</span>: <span
class="math display">$$\begin{align}
\langle B(0)C(t)\rangle \equiv \lim_{T\to \infty}\frac{1}{T} d\tau
B(\tau) C(\tau+t)
\end{align}$$</span></p>
<p>时间相关函数表述物质运动在时间先后上的相关关系，具备如下的性质： 1.
由于人一物理量在本质上必须是实数，因此时间相关函数必须是实数。 2. <span
class="math inline">lim<sub><em>t</em> → ∞</sub>⟨<em>B</em>(0)<em>C</em>(0)⟩ = 0</span>，两个时间间隔无穷远的事件自然不相关了。
<span
class="math inline"><em>表</em><em>示</em><em>怀</em><em>疑</em>，<em>真</em><em>的</em><em>是</em><em>这</em><em>样</em><em>么</em>？</span></p>
<h3 id="非平衡定态时的时间相关函数">非平衡定态时的时间相关函数</h3>
<p>非平衡定时，还具备如下的性质： 1.
时间平移不变性。物理量的相关性在该种状态下，只与相对的时间差有关，而与具体的时刻无关：
<span class="math display">$$\begin{align}
   \langle B(t_1)C(t_2) \rangle &amp;= \langle B(t_1-\tau)C(t_2-\tau)
\rangle \\
   \langle B(t_1)C(t_2) \rangle &amp;= \langle B(0)C(t_2-t_1) \rangle
   \end{align}$$</span> 2. 非平衡定态。 <span
class="math display">$$\begin{align}
   \langle \dot B(t)C(0)\rangle = -\langle B(t)\dot C(0)\rangle
   \end{align}$$</span></p>
<h3 id="平衡态时间自相关函数的性质">平衡态时间自相关函数的性质</h3>
<p>平衡态时的时间自相关函数 <span
class="math inline">⟨<em>C</em><sub><em>B</em><em>B</em></sub>(<em>t</em><sub>1</sub>, <em>t</em><sub>2</sub>) ≡ ⟨<em>B</em>(<em>t</em><sub>1</sub>)<em>B</em>(<em>t</em><sub>2</sub>)⟩</span>，具有如下性质：
1. 时间平移不变性。自相关函数只与时间间隔<span
class="math inline"><em>τ</em> ≡ <em>t</em><sub>1</sub> − <em>t</em><sub>2</sub></span>有关:
<span class="math display">$$\begin{align*}
   C_{BB}(t_1, t_2) &amp;\equiv \langle B(t_1)B(t_2)\rangle = \langle
B(0)B(t_2-t_1)\rangle \\
   &amp;= \rangle = \langle B(0)B(\tau)\rangle \equiv C_{BB}(\tau)
   \end{align*}$$</span> 2. <span
class="math inline"><em>C</em><sub><em>B</em><em>B</em></sub>(0) &gt; 0</span>。
3. 对于任意的时间<span class="math inline"><em>τ</em></span>，<span
class="math inline"><em>C</em><sub><em>B</em><em>B</em></sub>(<em>τ</em>)</span>的绝对值不可能大于<span
class="math inline"><em>τ</em></span>，<span
class="math inline"><em>C</em><sub><em>B</em><em>B</em></sub>(0)</span>：
<span
class="math display">|<em>C</em><sub><em>B</em><em>B</em></sub>(<em>τ</em>)| &lt; <em>C</em><sub><em>B</em><em>B</em></sub>(0)</span>
4. <span
class="math inline"><em>C</em><sub><em>B</em><em>B</em></sub>(−<em>τ</em>) = <em>C</em><sub><em>B</em><em>B</em></sub>(<em>τ</em>)</span>
5. 相关时间<span
class="math inline"><em>τ</em><sub><em>B</em></sub></span>，当间隔时间远大于相关时间之后，时间相关性趋于0：
<span
class="math display">lim<sub><em>τ</em> &gt;  &gt; <em>τ</em><sub><em>B</em></sub></sub><em>C</em><sub><em>B</em><em>B</em></sub>(<em>τ</em>) = 0</span></p>
<h3 id="时间相关函数的应用">时间相关函数的应用</h3>
<h1 id="量子动力学">量子动力学</h1>
<h1 id="连续介质力学">连续介质力学</h1>
<p>非平衡态统计力学分为多种形式理论： * 微观层次 * 动理学层次 *
流体力学层次
以上三个层次，逐渐粗粒化。在流体力学层次，先将处理的体系设想为连续介质，然后用连续介质来研究这一层次的问题。</p>
<h2 id="基本概念-1">基本概念</h2>
<h3 id="压强张量和应力张量">压强张量和应力张量</h3>
<p>粘性液体或者固体中任意单位截面上受到的力不一定在该截面的法向上，需要拓展为张量利用9个标量描述，称为压强张量<span
class="math inline"><em>P</em> ≡ {<em>p</em><sub><em>α</em><em>β</em>|<em>α</em>, <em>β</em> = <em>x</em>, <em>y</em>, <em>z</em>}</sub></span>，在直角坐标系下写为：
<span class="math display">$$\begin{align}
P=
\begin{bmatrix}
p_{xx} &amp; p_{xy} &amp; p_{xz}\\
p_{yx} &amp; p_{yy} &amp; p_{yz}\\
p_{zx} &amp; p_{zy} &amp; p_{zz}\\
\end{bmatrix}
\end{align}$$</span></p>
<p>应力张量<span class="math inline"><em>σ</em></span>取反方向：<span
class="math inline"><em>σ</em> ≡ −<em>P</em></span>，定义法向量<span
class="math inline">$\bf n$</span>，该界面上面的力<span
class="math inline">$\bf\sigma_n$</span>为： <span
class="math display">$$\bf{\sigma_n} =\sigma \cdot \bf n$$</span>
同时，应力张量为：</p>
<p><span class="math display">$$\sigma = \sum_{i,j=1}^3\sigma_{ij}e_ie_j
\triangleq \sigma_{ij}e_ie_j$$</span></p>
<p>其中<span class="math inline">≜</span>是Einstein求和符号。</p>
<h3 id="应变张量">应变张量</h3>
<p>以下讨论物体受力后体内粒子位置发生的相对改变。</p>
<p>体内无限接近的两点<span
class="math inline"><em>P</em>  <em>Q</em></span>，形变前向量<span
class="math inline">$\overrightarrow{PQ}\equiv dr =
\sum_{i=1}^3dx_ie_i$</span>，形变后成为向量<span
class="math inline">$\overrightarrow{P'Q'}\equiv dr'=dr+du =
\sum_{i=1}^3(dx_i+du_i)e_i$</span>，位移变量<span
class="math inline"><em>d</em><em>u</em> = <em>d</em><em>r</em><sup>′</sup> − <em>d</em><em>r</em></span>。</p>
<p><span class="math display">$$\begin{align}
|dr'|^2 &amp;= \sum_{i=1}^3(dx_i+du_i)^2 =
\sum_{i=1}^3(dx_i+\sum_{k=1}^3\frac{\partial u_i}{\partial x_k}dx_k)^2
\\
&amp;= \sum_{i=1}^3 \left[ (dx_i)^2 +2dx_i\sum_{k=1}^3\frac{\partial
u_i}{\partial x_k}dx_k+\left(\sum_{k=1}^3\frac{\partial u_i}{\partial
x_k}dx_k\right)^2\right] \\
&amp;= |dr|^2 + 2\sum_{i=1}^3dx_i \sum_{k=1}^3\frac{\partial
u_i}{\partial x_k}dx_k + \sum_{i,k,l=1}^3\frac{\partial u_i}{\partial
x_k}\frac{\partial u_i}{\partial x_l}dx_kdx_l \\
&amp;= |dr|^2 + 2\frac{\partial u_i}{\partial x_k}dx_kdx_i +
\frac{\partial u_i}{\partial x_k}\frac{\partial u_i}{\partial
x_l}dx_kdx_l \\
&amp;= |dr|^2 + \left(\frac{\partial u_i}{\partial x_k}+\frac{\partial
u_k}{\partial x_i}+\frac{\partial u_l}{\partial x_l}\frac{\partial
u_i}{\partial x_i}\right) dx_kdx_l \\
&amp;= |dr|^2 + 2\epsilon_{ij} dx_kdx_l
\end{align}$$</span></p>
<p>其中令： <span class="math display">$$\epsilon_{ij}\equiv
\left(\frac{\partial u_i}{\partial x_k}+\frac{\partial u_k}{\partial
x_i}+\frac{\partial u_l}{\partial x_l}\frac{\partial u_i}{\partial
x_i}\right)$$</span></p>
<p>对角元称为正向应变，非对角元称为切向应变或者剪应变。在应变比较小的情况下，二阶微分项远小于一阶相，可以忽略。因此：
<span class="math display">$$\begin{align}
\epsilon_{ij} &amp;\approx \left(\frac{\partial u_i}{\partial
x_k}+\frac{\partial u_k}{\partial x_i}\right) \\
\epsilon &amp;= \frac{1}{2}\left[\nabla u+\nabla u^T\right]
\end{align}$$</span></p>
<h3 id="广义hooke定律">广义Hooke定律</h3>
<p>在连续形变之后，应力张量<span
class="math inline"><em>σ</em></span>可普世的认为是应变张量的俄<span
class="math inline"><em>ϵ</em></span>函数，即<span
class="math inline"><em>σ</em>(<em>ϵ</em>)</span>。在无初应力且应变较小的情况下，略去高次项得：
<span class="math display">$$\sigma_{ij}=\left( \frac{\partial
\sigma_{ij}}{\partial \epsilon_{kl}}\right)_0\epsilon_{kl}$$</span></p>
<p>其中下标为0表示在应变为0处取值。</p>
<p>当形变较小的时候，饰演的应力与应变值之间服从Hooke定律，将偏微分关系变为线性关系：
<span
class="math display"><em>σ</em><sub><em>i</em><em>j</em></sub> = <em>C</em><sub><em>i</em><em>j</em><em>k</em><em>l</em></sub><em>ϵ</em><sub><em>l</em><em>k</em></sub></span></p>
<h3 id="形变能">形变能</h3>
<h3 id="各项同性介质的形变能">各项同性介质的形变能</h3>
<h3 id="各项同性介质的应力张量">各项同性介质的应力张量</h3>
<h2 id="流体力学">流体力学</h2>
<p>流体力学就是连续介质力学。Newton粘性公式，剪切应力<span
class="math inline">$\tau\equiv \frac{f}{\delta
S}$</span>正比于切向速度： <span class="math display">$$\tau=\eta
\frac{dv}{dy}$$</span></p>
<h3 id="流体的运动方程">流体的运动方程</h3>
<p>流体的连续性方程：</p>
<p><span class="math display">$$\begin{align}
\frac{\partial \rho_m}{\partial t}+\nabla\cdot \bf j &amp;=0 \\
\bf{j}(r,t) &amp;= \rho_m \bf m \\
\end{align}$$</span></p>
<p>理想流体的运动方程： <span class="math display">$$\rho_m
\frac{Dv}{Dt}=\rho_m(\frac{v}{t}-v\cdot\nabla v)=-\nabla\cdot
P$$</span></p>
<h2 id="连续介质的导热">连续介质的导热</h2>
<p>Fourier唯象导热定律： <span class="math display">$$\begin{align}
q=-\lambda\nabla T
\end{align}$$</span></p>
<h1 id="非平衡热力学基础">非平衡热力学基础</h1>
<h2 id="局域平衡近似">局域平衡近似</h2>
<p>非平衡态的体系有相新的平衡态趋近的趋势，这样的过程称为弛豫过程，弛豫时间记为<span
class="math inline"><em>τ</em><sub><em>m</em><em>a</em><em>c</em></sub></span>。</p>
<p><strong>局域平衡近似</strong>是将非平衡的体系分为很多的小部分，其中每一个小部分具备宏观小微观大的特点，宏观小足够反省非平衡态在宏观上的不均匀性，微观大包含足够多的粒子。其中每一个小块，可以认为是这一个小块是均匀的，可以用平衡态的状态参量描述。不同小区域与邻近区域之间的相互作用很弱，微区与微区之间不满足平衡条件，记微区的弛豫时间为<span
class="math inline"><em>τ</em><sub><em>m</em><em>i</em><em>c</em></sub></span>。</p>
<p>局域平衡近似成立有两个条件： * <span
class="math inline"><em>τ</em><sub><em>m</em><em>i</em><em>c</em></sub> ≪ <em>τ</em><sub><em>m</em><em>a</em><em>c</em></sub></span>
* <span
class="math inline"><em>l</em><sub><em>m</em><em>i</em><em>c</em></sub> ≪ <em>l</em><sub><em>m</em><em>a</em><em>c</em></sub></span>，其中<span
class="math inline"><em>l</em><sub><em>m</em><em>a</em><em>c</em></sub></span>为局域平衡成立的空间微观程度，<span
class="math inline"><em>l</em><sub><em>m</em><em>a</em><em>c</em></sub></span>为外力作用造成体系热力学量的波动宏观尺度。</p>
<h2 id="不可逆过程中的平衡方程">不可逆过程中的平衡方程</h2>
<p>使用连续介质力学的方案得到非平衡态热力学的平衡（即衡算）方程。在一定的尺度范围内，可以将任何不平衡体系看成连续介质。</p>
<p>设有一个宏观体系，体积为<span
class="math inline"><em>V</em></span>，边界为<span
class="math inline"><em>Σ</em></span>，讨论体系宏观广延量<span
class="math inline"><em>F</em>(<em>t</em>)</span>随时间的演化：</p>
<p><span
class="math display"><em>F</em>(<em>t</em>) = ∫<sub><em>V</em></sub><em>ρ</em><sub><em>m</em></sub>(<em>r</em>, <em>t</em>)<em>f</em>(<em>r</em>, <em>t</em>)<em>d</em><em>r</em></span></p>
<p>其中<span
class="math inline"><em>ρ</em><sub><em>m</em></sub>(<em>r</em>, <em>t</em>)</span>为密度质量，<span
class="math inline"><em>f</em>(<em>r</em>, <em>t</em>)</span>为单位质量的广延量，因此<span
class="math inline"><em>ρ</em><sub><em>m</em></sub><em>f</em></span>为单位体积的广延量。</p>
<figure>
<img src="./9-1.png" alt="9-1" />
<figcaption aria-hidden="true">9-1</figcaption>
</figure>
<p>讨论的体系包含化学反应，因此体系中有源和汇，设<span
class="math inline"><em>σ</em><sub><em>F</em></sub>(<em>r</em>)</span>是体系的源密度（<span
class="math inline"><em>r</em></span>处产生<span
class="math inline"><em>F</em></span>的量/体积/时间），正表示源，负表示汇。有以下的平衡方程：
<span class="math display">$$\begin{align}
\frac{dF(t)}{dt} &amp;= P[F] + \phi[F] \\
P[F] &amp;\equiv \int_V \sigma_f dr \\
\phi &amp;\equiv -\oint_{\Sigma}j_F \cdot dS \\
\frac{dF(t)}{dt} &amp;= \frac{d}{dt}\int_V \rho_m (r,t)f(r,t)dr =
\int_V\frac{\partial}{\partial t}(\rho_m f)dr \\
&amp;= \int_V \sigma_F dr - \oint_{\Sigma}j_F\cdot dS \\
\int_V\frac{\partial}{\partial t}(\rho_m f)dr &amp;= \int_V \sigma_F dr
- \int_{V}\nabla \cdot j_F dv \\
\sigma_F &amp;=  \frac{\partial}{\partial t}(\rho_m f)+\nabla \cdot
j_F    \\
\end{align}$$</span></p>
<p>称为广延量<span
class="math inline"><em>F</em></span>的局域平衡方程。其中<span
class="math inline"><em>ρ</em><sub><em>m</em></sub><em>f</em></span>是单位体积的广延量<span
class="math inline"><em>F</em></span>（即<span
class="math inline"><em>F</em></span>量/体积），<span
class="math inline"><em>σ</em><sub><em>F</em></sub></span>广延量<span
class="math inline"><em>F</em></span>的源密度（<span
class="math inline"><em>F</em></span>产生的量/面积/时间），<span
class="math inline"><em>j</em><sub><em>F</em></sub></span>是广延量<span
class="math inline"><em>F</em></span>的流密度（<span
class="math inline"><em>F</em></span>的通过量/面积/时间）也称为<strong>流</strong>或者<strong>通量</strong>。</p>
<p>以下讨论几个重要的具体广延量的平衡：质量、动量、能量、角动量和熵。</p>
<h3 id="连续介质中的质量守恒">连续介质中的质量守恒</h3>
<p>从微体积净增的物质的量一定等于从它外部流入到该微体积的物质的量，称为质量平衡或者质量横算。因为无源<span
class="math inline"><em>σ</em><sub><em>F</em></sub> = 0, <em>f</em> = 1, <em>j</em><sub><em>F</em></sub> = <em>ρ</em><sub><em>m</em></sub><em>v</em></span>。</p>
<p><span class="math display">$$\begin{align}
0 &amp;=  \frac{\partial}{\partial t}(\rho_m)+\nabla \cdot (\rho_m v)
\end{align}$$</span></p>
<h3 id="连续介质中的动量平衡">连续介质中的动量平衡</h3>
<p>质量流<span
class="math inline"><em>ρ</em><sub><em>m</em></sub><em>v</em></span>可以理解为该处的动量密度，因此<span
class="math inline"><em>ρ</em><sub><em>m</em></sub><em>v</em><em>v</em></span>为该处流体内的动量流。虽然其与能量密度的量纲相同，但是动量流为张量，能量密度是标量。流体的动量为：
<span class="math display">$$\begin{align}
G=\int_v \rho_m vd^3 r
\end{align}$$</span>
受到外力后，该体积内流动的动量就增加。单位时间内动量的增值就是该体积<span
class="math inline"><em>V</em></span>所受的外力，即： <span
class="math display">$$\begin{align}
F\equiv \frac{d G}{d t}=\int_v \frac{\partial \rho_m v}{\partial t} d^3
r
\end{align}$$</span></p>
<p><font color='red'>接下来还有具体讨论，暂且跳过</font></p>
<h3 id="连续介质中的能量守恒">连续介质中的能量守恒</h3>
<p><font color='red'>核心为分析功率，总功率分为三个部分：应力、质量、传热</font></p>
<h3 id="局域熵不可逆过程的熵产生率">局域熵、不可逆过程的熵产生率</h3>
<p>数学上，散度表示流出</p>
<p><font color='red'>因为非平衡态的假设是将整个区域分割为小的平衡区域。针对每个小的平衡区域，认为熵的变化分为两部分，分别为平衡体系内熵的变化，以及不同体系内熵的流动。</font></p>
<p>局域熵的产生率<span
class="math inline"><em>σ</em><sub><em>s</em></sub></span>均可由局域热力力量<span
class="math inline"><em>X</em><sub><em>i</em></sub></span>和由它造成的（称为：与之为共轭的）局域流<span
class="math inline"><em>J</em><sub><em>i</em></sub></span>的标积，<span
class="math inline"><em>X</em><sub><em>i</em></sub></span>称为广义力，<span
class="math inline"><em>J</em><sub><em>i</em></sub></span>称为广义流。局域熵产生率<span
class="math inline"><em>σ</em><sub><em>s</em></sub></span>可表示为：</p>
<p><span class="math display">$$\begin{align}
\sigma_s = \sum_i X_i\cdot X_i
\end{align}$$</span></p>
<h2 id="onsager关系">Onsager关系</h2>
<p>将局域流<span
class="math inline"><em>J</em><sub><em>i</em></sub></span>按局域力<span
class="math inline">{<em>X</em><sub><em>j</em></sub>}</span>展开，取最低的线性近似得到：
<span class="math display">$$\begin{align}
J_i = \sum_j L_{ij} X_j
\end{align}$$</span> 其中<span
class="math inline"><em>L</em><sub><em>i</em><em>j</em></sub></span>就是各种输运系数。</p>
<p>Onsager关系为： <span class="math display">$$\begin{align}
L_{ij} = L_{ji}, L^T = L
\end{align}$$</span> <span
class="math inline"><em>L</em><sub><em>i</em><em>j</em></sub></span>为对称矩阵。</p>
<h2 id="熵产生极小定理">熵产生极小定理</h2>
<p><strong>熵产生极小定理</strong>：对处于稳态的非平衡体系，其熵产生率处于极小值。</p>
<h1 id="涨落理论">涨落理论</h1>
<p>广义涨落有两大类： *
无论在平衡态还是非平衡态，体系宏观量瞬时值在其平均值上下快速变动。为狭义的涨落。
* Brown运动。</p>
<h2 id="涨落基本概念">涨落基本概念</h2>
<p>设<span
class="math inline"><em>B</em></span>为热力学平衡态体系的任意宏观物理量。设体系处于第<span
class="math inline"><em>j</em></span>个量子态时，该物理量取值<span
class="math inline"><em>B</em><sub><em>j</em></sub></span>，所以物理量<span
class="math inline"><em>B</em></span>的宏观值就是对量子态的平均值：</p>
<p><span
class="math display"> &lt; <em>B</em> &gt;  = ∑<sub><em>j</em></sub><em>P</em><sub><em>j</em></sub><em>B</em><sub><em>j</em></sub></span></p>
<p>其中<span
class="math inline"><em>P</em><sub><em>j</em></sub></span>为该量子态出现的概率，为方便在不混淆的情况下，省略<span
class="math inline"><em>B</em><sub><em>j</em></sub></span>的下标，记为<span
class="math inline"><em>B</em></span>。于是体系处于某平衡态的偏差与相对偏差为：
<span class="math display">$$\begin{align}
\Delta B &amp;\equiv B - &lt;B&gt; \\
r_B &amp;\equiv \frac{\Delta B}{&lt;B&gt;}
\end{align}$$</span></p>
<p>因为<span
class="math inline"> &lt; <em>Δ</em><em>B</em> &gt;  = 0</span>恒成立，为了表征涨落的程度，需将偏差的平方对量子态求平均值，称为B在系综中的均方涨落，简称涨落：
<span class="math display">$$\begin{align}
&lt;(\Delta B)^2&gt; &amp;\equiv \sum_j P_j - (\Delta B)^2 =\left &lt;
(B-&lt;\Delta B&gt;)^2\right&gt; \\
&amp;= \sum_j P_j\left(B_j-\langle B\rangle\right)^2=\sum_j P_j B_j^2-2
\sum_j P_j B_j\langle B\rangle+\sum_j P_j\langle B\rangle^2 \\
&amp;= \langle B^2\rangle -\langle B\rangle^2
\end{align}$$</span></p>
<p>其中<span
class="math inline">⟨<em>B</em><sup>2</sup>⟩</span>为物理量<span
class="math inline"><em>B</em><sup>2</sup></span>的均值，或<span
class="math inline"><em>B</em></span>的均方值。同样，相对偏差平方的均值可表为：</p>
<p><span class="math display">$$\begin{equation}
\begin{aligned}
\left\langle r_B^2\right\rangle
&amp;\equiv\left\langle\left(\frac{\Delta B}{\langle
B\rangle}\right)^2\right\rangle=\frac{\left\langle(\Delta
B)^2\right\rangle}{\langle B\rangle^2}=\frac{\left\langle(B-\langle
B\rangle)^2\right\rangle}{\langle B\rangle^2} \\
&amp;= \frac{\langle B^2\rangle}{\langle B\rangle^2}-1
\end{aligned}
\end{equation}$$</span></p>
<p>称为热力学量<span
class="math inline"><em>B</em></span>的相对涨落。</p>
<p><font color='red'>在连续谱时候的情况</font></p>
<h2 id="涨落理论-1">涨落理论</h2>
<h3 id="正则系综中的涨落">正则系综中的涨落</h3>
<p>正则系综存在与环境的热交换，因此<span
class="math inline">(<em>N</em>, <em>V</em>, <em>T</em>)</span>恒定，体系存在能量<span
class="math inline"><em>E</em></span>的涨落，能量均值： <span
class="math display">$$\begin{align}
\langle E\rangle &amp;= \sum_j E_j P_j=\frac{1}{Z}\sum_j E_j (\Omega_j
e^{\frac{-E_j}{k_B T}}) \\
\langle E\rangle Z &amp;= \sum_j E_j (\Omega_j e^{\frac{-E_j}{k_B T}})
\\
\end{align}$$</span> 两边被<span class="math inline">$\left(
\frac{\partial}{\partial T}\right)_{V,N}$</span>作用得到： <span
class="math display">$$\begin{equation}
Z\left(\frac{\partial\langle E\rangle}{\partial T}\right)_{V, N}+\langle
E\rangle\left(\frac{\partial Z}{\partial T}\right)_{V,
N}=\frac{1}{k_{\mathrm{B}} T^2} \sum_j E_j^2\left(\Omega_j
\mathrm{e}^{-E_j /\left(k_{\mathrm{B}}
T\right)}\right)=\frac{Z\left\langle E^2\right\rangle}{k_{\mathrm{B}}
T^2}
\end{equation}$$</span></p>
<p>根据<span class="math inline">$\langle E\rangle=k_B T^2\left(
\frac{\partial \ln Z}{\partial T}\right)_{V, N}$</span>得到<span
class="math inline">$\left( \frac{\partial Z}{\partial T}\right)_{V,
N}=\frac{Z}{k_B T^2}\langle E\rangle$</span>，因此： <span
class="math display">$$\begin{align}
\left(\frac{\partial\langle E\rangle}{\partial T}\right)_{V, N} =
\frac{1}{k_B T^2}\left[\langle E^2\rangle-\langle E\rangle^2\right]
\end{align}$$</span></p>
<p>根据定容热容<span class="math inline">$C_V =
\left(\frac{\partial\langle E\rangle}{\partial T}\right)_{V,
N}$</span>，可以得到正则系统的能量涨落和能量的相对涨落分别为： <span
class="math display">$$\begin{equation}
\begin{gathered}
\left\langle(\Delta E)^2\right\rangle=\left\langle
E^2\right\rangle-\langle E\rangle^2=k_{\mathrm{B}}
T^2\left(\frac{\partial\langle E\rangle}{\partial T}\right)_{V,
N}=k_{\mathrm{B}} T^2 C_V \\
\left\langle r_E^2\right\rangle=\frac{\left\langle(\Delta
E)^2\right\rangle}{\langle E\rangle^2}=\frac{1}{\langle E\rangle^2}
k_{\mathrm{B}} T^2\left(\frac{\partial\langle E\rangle}{\partial
T}\right)_{V, N}=k_{\mathrm{B}} T^2 \frac{C_V}{\langle E\rangle^2}
\propto \frac{1}{N}
\end{gathered}
\end{equation}$$</span></p>
<h3
id="巨正则系综中粒子数能能量的涨落">巨正则系综中粒子数能能量的涨落</h3>
<h3
id="平衡态开放体系中的自发涨落onsager的涨落回归假设">平衡态开放体系中的自发涨落、Onsager的涨落回归假设</h3>
<p>讨论平衡态开放体系的中的任意动力学量出现的自发涨落是自然的，但是更重要的一层意义是它的讨论有助于理解非平衡态。</p>
<p>Onsager注意到有两种弛豫过程： *
平衡态在没有外场干预的条件下，内部分子的热运动引起动力学量的自发涨落：不断偏离均值再回归均值。（微观弛豫）
*
当体系受到某种外场作用，使之成为一个宏观的非平衡状态，若突然终止该外场，则体系将通过弛豫过程趋于平衡态。（宏观弛豫）</p>
<p>Onsager假设这两种弛豫过程服从相同的规律，称为涨落回归假设。后来人们意识到，Onsager的假设在力学上有深刻的原因，即1951年Callen和Welton证明涨落耗散定理。</p>
<p>以下先用经典理论讨论平衡体系中的自发涨落问题。</p>
<h4 id="非平衡态的系综平均">非平衡态的系综平均</h4>
<p>上述Onsager假设联系两种驰豫，前者是平衡态的自发涨落弛豫，其系综平均记为<span
class="math inline">⟨⋅⟩</span>；后者是非平衡态的宏观弛豫，其系综平均记为<span
class="math inline">$\overline{\left(\cdot\right)}$</span>，在时刻<span
class="math inline"><em>t</em></span>的瞬时系综平均值记为<span
class="math inline">$\overline{A}(t)$</span>。</p>
<p>考虑一个纯态，初始时刻是一个相点，将<span
class="math inline"><em>t</em></span>时刻体系的任意动力学量<span
class="math inline"><em>A</em></span>记为： <span
class="math display"><em>A</em>(<em>t</em>) = <em>A</em>[<em>r</em><sup><em>N</em></sup>(<em>t</em>), <strong>p</strong><sup><em>N</em></sup>(<em>t</em>)] ≡ <em>A</em>(<em>t</em>; <strong>r</strong><sup><em>N</em></sup>(0), <strong>p</strong><sup><em>N</em></sup>(0)) ≜ <em>A</em>(<em>t</em>; <strong>r</strong><sup><em>N</em></sup>, <strong>p</strong><sup><em>N</em></sup>)</span>
其中<span
class="math inline"><em>r</em><sup><em>N</em></sup>, <em>p</em><sup><em>N</em></sup></span>特指初始条件<span
class="math inline"><em>r</em><sup><em>N</em></sup>(0), <em>r</em><sup><em>N</em></sup>(0)</span>。</p>
<p>更普世的情况，考虑一个混合态，初始时刻是一个分布，故任意时刻<span
class="math inline"><em>A</em>(<em>t</em>)</span>的均值要用系综平均求。设<span
class="math inline"><em>ϕ</em>(<em>r</em><sup><em>N</em></sup>, <em>p</em><sup><em>N</em></sup>)</span>为非平衡态体系在相空间的分布函数，因此<span
class="math inline"><em>A</em>(<em>t</em>)</span>的系综平均值为： <span
class="math display">$$\begin{equation}
\begin{aligned}
\bar{A}(t)&amp;=\int \mathrm{d} \boldsymbol{r}^N \mathrm{~d}
\boldsymbol{p}^N \phi\left(\boldsymbol{r}^N, \boldsymbol{p}^N\right)
A\left(t ; \boldsymbol{r}^N, \boldsymbol{p}^N\right) \\
1 &amp;= \int \mathrm{d} \boldsymbol{r}^N \mathrm{~d} \boldsymbol{p}^N
\phi\left(\boldsymbol{r}^N, \boldsymbol{p}^N\right)
\end{aligned}
\end{equation}$$</span>
这里采用Heisenber绘景思想，将时间的俄影响归于力学量，而非分布函数。</p>
<p>任意动力学量<span
class="math inline"><em>A</em><em>的</em><em>瞬</em><em>时</em><em>微</em><em>观</em><em>值</em></span>A(t)<span
class="math inline"><em>偏</em><em>离</em><em>平</em><em>衡</em><em>的</em><em>量</em><em>可</em><em>定</em><em>义</em><em>为</em>：</span><span
class="math inline"><em>δ</em><em>A</em>(<em>t</em>) ≡ <em>A</em>(<em>t</em>) − ⟨<em>A</em>⟩</span>$</p>
<p><span
class="math inline"><em>A</em>(<em>t</em>)</span>的非平衡均值<span
class="math inline"><em>Ā</em>(<em>t</em>)</span>偏离平衡的量为： <span
class="math display">$$\begin{equation}
\Delta \bar{A}(t)=\int \mathrm{d} \boldsymbol{r}^N \mathrm{~d}
\boldsymbol{p}^N \phi\left(\boldsymbol{r}^N,
\boldsymbol{p}^N\right)\left[A\left(t ; \boldsymbol{r}^N,
\boldsymbol{p}^N\right)-\langle A\rangle\right]=\overline{\delta A(t)}
\end{equation}$$</span></p>
<h4 id="平衡态的自发涨落">平衡态的自发涨落</h4>
<p>在平衡态下，<span
class="math inline"><em>δ</em><em>A</em>(<em>t</em>)</span>的平均值有：
<span class="math display">⟨<em>δ</em><em>A</em>(<em>t</em>)⟩ = 0</span>
但是有意义的是平衡时不同不同偏离量之间的相关，例如<span
class="math inline"><em>δ</em><em>A</em>(<em>t</em>)</span>与<span
class="math inline"><em>δ</em><em>A</em>(0)</span>之间的相关可表示为涨落的时间自相关函数：
<span
class="math display"><em>C</em>(<em>t</em>) ≡ ⟨<em>δ</em><em>A</em>(<em>t</em>)<em>δ</em><em>A</em>(0)⟩</span>
对于经典体系： <span
class="math display"><em>C</em>(<em>t</em>) = ∫d<strong>r</strong><sup><em>N</em></sup> d<strong>p</strong><sup><em>N</em></sup><em>f</em>(<strong>r</strong><sup><em>N</em></sup>, <strong>p</strong><sup><em>N</em></sup>)<em>δ</em><em>A</em>(0; <strong>r</strong><sup><em>N</em></sup>, <strong>p</strong><sup><em>N</em></sup>)<em>δ</em><em>A</em>(<em>t</em>; <strong>r</strong><sup><em>N</em></sup>, <strong>p</strong><sup><em>N</em></sup>)</span></p>
<p>其中<span
class="math inline"><em>f</em>(<strong>r</strong><sup><em>N</em></sup>, <strong>p</strong><sup><em>N</em></sup>)</span>是相空间中平衡时的分布函数。
<span class="math inline"><em>C</em>(<em>t</em>)</span>的性质如下： 1.
<span
class="math inline"><em>C</em>(<em>t</em>) ≡ ⟨<em>δ</em><em>A</em>(<em>t</em>)<em>δ</em><em>A</em>(0)⟩ = ⟨<em>A</em>(<em>t</em>)<em>A</em>(0)⟩ − ⟨<em>A</em>⟩<sup>2</sup></span>
2. 平衡体系的时间相关函数具有时间平移不变性，即与时间原点的选取无关。 3.
若<span class="math inline"><em>A</em>(−<em>t</em>)</span>与<span
class="math inline"><em>A</em>(0)</span>两个量可以对易，则交换两者的次序得到：
<span
class="math display"><em>C</em>(<em>t</em>) = ⟨<em>δ</em><em>A</em>(−<em>t</em>)<em>δ</em><em>A</em>(0)⟩ = ⟨<em>δ</em><em>A</em>(0)<em>δ</em><em>A</em>(−<em>t</em>)⟩ = <em>C</em>(−<em>t</em>)</span>
对于经典体系这样的对易衡成立，对于量子体系则不一定。 4. <span
class="math inline"><em>C</em>(0) = ⟨<em>δ</em><em>A</em>(0)<em>δ</em><em>A</em>(0)⟩ = ⟨(<em>δ</em><em>A</em>(0))<sup>2</sup>⟩</span>
当两个时间间隔极大时，<span
class="math inline"><em>δ</em><em>A</em>(<em>t</em>)</span>与<span
class="math inline"><em>δ</em><em>A</em>(0)</span>之间的相关会趋于零。可见任意力学量<span
class="math inline"><em>A</em></span>在平衡态涨落的时间自相关函数总是呈现衰减到零的趋势，Onsager称之为涨落的“回归”。</p>
<h4
id="onsager涨落回归假设的数学描述">Onsager涨落回归假设的数学描述</h4>
<p>基于上述对于平衡涨落的时间自相关函数，现在考虑非平衡体系开始相平衡的弛豫。设<span
class="math inline"><em>t</em> = 0</span>时撤区外场让制备得到的非平衡体系开始相平衡态作自由弛豫。Onsager涨落回归假设是指在线性范围内，可以假设弛豫服从如下方程：
<span class="math display">$$\begin{align}
\frac{\Delta \bar A(t)}{\Delta\bar A(0)}=\frac{C(t)}{C(0)}
\end{align}$$</span>
左边表示非平衡体系向平衡态作宏观弛豫的衰减程度，右边表示平衡涨落的自相关行为或微观弛豫。假设这两种弛豫规律相等，这就是Onsager的涨落回归假设。换言之，对于一个近平衡体系，无法区分是自发涨落还是偏离平衡后的弛豫，那么自发涨落自相关的行为实际上应该与<span
class="math inline"><em>t</em></span>瞬间非平衡均值偏离平衡的量衰减到平衡的行为是相同的。</p>
<h4 id="非平衡分布函数phirn-pn">非平衡分布函数<span
class="math inline"><em>ϕ</em>(<em>r</em><sup><em>N</em></sup>, <em>p</em><sup><em>N</em></sup>)</span></h4>
<p>平衡态分布<span
class="math inline"><em>f</em></span>与非平衡态分布<span
class="math inline"><em>ϕ</em></span>在<strong>近平衡</strong>的范围内应当有：
<span class="math display">$$\begin{equation}
\frac{\phi\left(\boldsymbol{r}^N,
\boldsymbol{p}^N\right)}{f\left(\boldsymbol{r}^N,
\boldsymbol{p}^N\right)}=\frac{A\left(t=0 ; \boldsymbol{r}^N,
\boldsymbol{p}^N\right)}{\langle A\rangle}
\end{equation}$$</span> 即： <span class="math display">$$\begin{align}
\phi\left(\boldsymbol{r}^N, \boldsymbol{p}^N\right)=\frac{A\left(t=0 ;
\boldsymbol{r}^N, \boldsymbol{p}^N\right)}{\langle
A\rangle}f\left(\boldsymbol{r}^N, \boldsymbol{p}^N\right)
\end{align}$$</span> 得到： <span
class="math display">$$\begin{equation}
\begin{aligned}
\bar{A}(t) &amp; =\int \mathrm{d} \boldsymbol{r}^N \mathrm{~d}
\boldsymbol{p}^N \phi\left(\boldsymbol{r}^N, \boldsymbol{p}^N\right)
A\left(t ; \boldsymbol{r}^N, \boldsymbol{p}^N\right) \\
&amp; =\langle A\rangle^{-1} \int \mathrm{d} \boldsymbol{r}^N
\mathrm{~d} \boldsymbol{p}^N f\left(\boldsymbol{r}^N,
\boldsymbol{p}^N\right) A\left(0 ; \boldsymbol{r}^N,
\boldsymbol{p}^N\right) A\left(t ; \boldsymbol{r}^N,
\boldsymbol{p}^N\right)
\end{aligned}
\end{equation}$$</span> 即： <span
class="math display">$$\begin{aligned}
\bar{A}(t) =\langle A\rangle^{-1}\langle A(0)A(t)\rangle
\end{aligned}$$</span></p>
<p>得到瞬时非平衡均值偏离平衡的量： <span
class="math display">$$\begin{equation}
\begin{aligned}
\Delta \bar{A}(t) &amp;\equiv \bar{A}(t)-\langle A\rangle=\langle
A\rangle^{-1}\langle A(0) A(t)\rangle-\langle A\rangle \\
&amp;=\langle A\rangle^{-1}\left\{\langle A(0) A(t)\rangle-\langle
A\rangle^2\right\} \\
&amp;=\langle A\rangle^{-1}\langle \delta A(0) \delta A(t)\rangle \\
&amp;= \langle A\rangle^{-1}C(t) \\
C(t) &amp;= \Delta \bar{A}(t)\langle A\rangle
\end{aligned}
\end{equation}$$</span></p>
<p>左边是平衡态力学量<span
class="math inline"><em>A</em></span>的自发涨落，右边是非平衡不可逆过程的弛豫。</p>
<h2 id="涨落的准热力学理论">涨落的准热力学理论</h2>
<p>涨落的准热力学理论（quasi-thermodynamic
theory）是计算宏观热力学量涨落的普遍理论。其思路是先求出各种略微偏移平衡态的宏观态出现的概率，然后通过统计平均求出各种热力学量的均方偏差、相对涨落。</p>
<p>设体系的能量、体积和粒子数分别为<span
class="math inline"><em>E</em>, <em>V</em>, <em>N</em></span>，体系的熵为<span
class="math inline"><em>S</em> = <em>k</em><sub><em>B</em></sub>ln <em>Ω</em></span>，<span
class="math inline"><em>Ω</em></span>为微观状态数，无论系统处于平衡还是偏离平衡均适用。若该体系孤立，则当体系处于平衡时体系熵和微观状态数均达到最大。
<span
class="math display"><em>S̄</em> = <em>k</em><sub><em>B</em></sub>ln <em>Ω</em><sub>max</sub></span>
出现熵极大的概率为<span
class="math inline"><em>W</em><sub>max</sub></span>，其应该与<span
class="math inline"><em>Ω</em><sub>max</sub></span>成正比： <span
class="math display"><em>W</em><sub>max</sub> ∝ <em>Ω</em><sub>max</sub> = e<sup><em>S̄</em>/<em>k</em><sub>B</sub></sup></span>
由于涨落，熵可以偏离其极大值，体系熵为<span
class="math inline"><em>S</em></span>的概率为<span
class="math inline"><em>W</em></span>依然与为微观状态数成正比： <span
class="math display"><em>W</em> ∝ <em>Ω</em> = e<sup><em>S</em>/<em>k</em><sub>B</sub></sup></span>
熵的偏离值<span
class="math inline"><em>Δ</em><em>S</em> ≡ <em>S</em> − <em>S̄</em></span>，得到：
<span class="math display">$$\begin{align}
W = W_{\max}\mathrm{e}^{\Delta S / k_{\mathrm{B}}}
\end{align}$$</span> 称为Smoluchowski-Einstain公式。</p>
<p>将该孤立系统的分析拓展到与环境有接触的非孤立系统。把系统与环境看成是一个拓展体系，将拓展体系视为一个孤立的个体：
<span
class="math display"><em>E</em><sub><em>e</em><em>x</em></sub> = <em>E</em> + <em>E</em><sub><em>b</em></sub>, <em>V</em><sub><em>e</em><em>x</em></sub> = <em>V</em> + <em>V</em><sub><em>b</em></sub></span>
其中下标”ex”指拓展体系，“b”指环境，无下标指体系。显然存在以下关系：
<span
class="math display"><em>Δ</em><em>E</em> = −<em>Δ</em><em>E</em><sub><em>b</em></sub>, <em>Δ</em><em>V</em> = −<em>Δ</em><em>V</em><sub><em>b</em></sub>, <em>Δ</em><em>N</em> = −<em>Δ</em><em>N</em><sub><em>b</em></sub></span>
由于拓展体系是孤立体系，因此熵偏差值为： <span
class="math display"><em>Δ</em><em>S</em><sub><em>e</em><em>x</em></sub> = <em>Δ</em><em>S</em> + <em>Δ</em><em>S</em><sub><em>b</em></sub></span>
有： <span
class="math display"><em>W</em>(<em>Δ</em><em>S</em><sub><em>e</em><em>x</em></sub>) = <em>W</em><sub>max</sub>e<sup>(<em>Δ</em><em>S</em> + <em>Δ</em><em>S</em><sub><em>b</em></sub>)/<em>k</em><sub>B</sub></sup></span>
假设环境足够大，有确定的温度<span
class="math inline"><em>T</em></span>和压强<span
class="math inline"><em>p</em></span>，则有： <span
class="math display">$$\begin{align}
\Delta S_{\mathrm{b}}&amp;=\frac{1}{T}\left(\Delta E_{\mathrm{b}}+p
\Delta V_{\mathrm{b}}\right)=-\frac{1}{T}(\Delta E+p \Delta V) \\
W\left(\Delta S_{\mathrm{ex}}\right) &amp;\equiv W(\Delta S, \Delta E,
\Delta V)=W_{\max } \mathrm{e}^{\left(\Delta S+\Delta S_b\right) /
k_{\mathrm{B}}}=W_{\max } \mathrm{e}^{(T \Delta S-\Delta E-p \Delta V)
/\left(k_{\mathrm{B}} T\right)}\\
W &amp;= W_{\max } \mathrm{e}^{-(\Delta F+p \Delta V)
/\left(k_{\mathrm{B}} T\right)}
\end{align}$$</span> 这是封闭体系的Smoluchowski公式。</p>
<p><font color='red'>单组分封闭体系的涨落、开放体系涨落</font></p>
<p>Smoluchowski-Einstein理论从另一个角度把计算涨落问题与热力学量在理论上结合起来。该理论一方面，不是从体系的微观状态的概率分布为基础，因此属于唯象理论；另一方面，它能导出涨落，而涨落是纯热力学无法涉及的。因此它被称为准热力学理论</p>
<p>下面将介绍，如何利用该公式导出体系各宏观物理量的涨落以及涨落之间的关系</p>
<p><font color='red'>待补全</font></p>
<h3 id="封闭体系热力学量的涨落">封闭体系热力学量的涨落</h3>
<h3 id="开放体系热力学量的涨落">开放体系热力学量的涨落</h3>
<h3 id="临界点附近的涨落">临界点附近的涨落</h3>
<h3 id="多变量涨落的准热力学理论">多变量涨落的准热力学理论</h3>
<h1 id="动理学描述与boltzmann方程">动理学描述与Boltzmann方程</h1>
<p>近平衡态体系发生的典型过程有热传导、扩散、黏滞现象、电导等，总称为输运过程。当体系处于非平衡态时，其体系性质随时间变化而变化。本篇介绍非平衡态处于近平衡态的体系，解释这些不可逆过程的方向，求出不可逆过程中的输运系数。与平衡态类似，非平衡态问题的关键仍然是求得体系的分布函数，但是由于非平衡态的原因，分布函数是时间的函数。因此，关键问题是求得分布函数随时间变化所需满足的方程，Boltzmann方程，是求算许多输运系数的依据，奠定了非平衡统计力学的基础。</p>
<h2 id="boltzmann方程">Boltzmann方程</h2>
<h3 id="混合稀薄气体">混合稀薄气体</h3>
<p>假定在混合稀薄气体中间，气体分子之间的相互作用只是二体碰撞，不存在三体碰撞，将第<span
class="math inline"><em>j</em></span>种的物种的单粒子分布记为<span
class="math inline"><em>f</em><sub><em>j</em></sub><sup>(1)</sup>(<em>q</em><sub>1</sub>, <em>p</em><sub>1</sub>, <em>t</em>)</span>，混合稀薄气体体系的宏观性质只取决于<span
class="math inline">{<em>f</em><sub><em>j</em></sub><sup>(1)</sup>(<em>r</em>, <em>v</em><sub>1</sub>, <em>t</em>)}</span>（动量改用速度），于是<span
class="math inline"><em>f</em><sub><em>j</em></sub><sup>(1)</sup>(<em>r</em>, <em>v</em><sub>1</sub>, <em>t</em>)<em>d</em><em>v</em><sub><em>j</em></sub></span>代表<span
class="math inline"><em>t</em></span>时刻在<span
class="math inline"><em>r</em></span>处单位体积内第<span
class="math inline"><em>j</em></span>种物种粒子速度处于<span
class="math inline"><em>v</em><sub><em>j</em></sub> → <em>v</em><sub><em>j</em></sub> + <em>d</em><em>v</em><sub><em>j</em></sub></span>的粒子数；或者<span
class="math inline"><em>f</em><sub><em>j</em></sub><sup>(1)</sup>(<em>r</em>, <em>v</em><sub>1</sub>, <em>t</em>)<em>d</em><em>v</em><sub><em>j</em></sub><em>d</em><em>r</em></span>代表<span
class="math inline"><em>t</em></span>时刻在<span
class="math inline"><em>r</em></span>处单位体积内第<span
class="math inline"><em>j</em></span>种物种粒子速度处于<span
class="math inline"><em>v</em><sub><em>j</em></sub> → <em>v</em><sub><em>j</em></sub> + <em>d</em><em>v</em><sub><em>j</em></sub></span>、位置处于<span
class="math inline"><em>r</em> → <em>r</em> + <em>d</em><em>r</em></span>的粒子数，其中<span
class="math inline"><em>v</em><sub><em>j</em></sub> → <em>v</em><sub><em>j</em></sub> + <em>d</em><em>v</em><sub><em>j</em></sub></span>的粒子数，其中<span
class="math inline"><em>f</em><sub><em>j</em></sub><sup>(1)</sup>(<em>r</em>, <em>v</em><sub>1</sub>, <em>t</em>)</span>为第<span
class="math inline"><em>j</em></span>种物种粒子<span
class="math inline"><em>μ</em></span>空间的数密度。对<span
class="math inline"><em>v</em></span>积分得到粒子数密度： <span
class="math display"><em>ρ</em><sub><em>j</em></sub>(<strong>r</strong>, <em>t</em>) = ∫<em>f</em><sub><em>j</em></sub><sup>(1)</sup>(<strong>r</strong>, <strong>v</strong><sub><em>j</em></sub>, <em>t</em>)d<strong>v</strong><sub><em>j</em></sub></span>
再对位置空间积分，得到第<span
class="math inline"><em>j</em></span>种物种的粒子数为： <span
class="math display">∬<em>f</em><sub><em>j</em></sub><sup>(1)</sup>(<strong>r</strong>, <strong>v</strong><sub><em>j</em></sub>, <em>t</em>)d<strong>r</strong>d<strong>v</strong><sub><em>j</em></sub> = ∫<em>ρ</em><sub><em>j</em></sub>(<strong>r</strong>, <em>t</em>)d<strong>r</strong> = <em>N</em><sub><em>j</em></sub></span></p>
<h3 id="几种平均速度的定义">几种平均速度的定义</h3>
<ol type="1">
<li>在<span class="math inline"><em>t</em></span>时刻<span
class="math inline"><em>r</em></span>处第<span
class="math inline"><em>j</em></span>种物种粒子的平均速度为： <span
class="math display">$$\begin{equation}
\left\langle\boldsymbol{v}_j(\boldsymbol{r},
t)\right\rangle=\frac{1}{\rho_j(\boldsymbol{r}, t)} \int
\boldsymbol{v}_j f_j^{(1)}\left(\boldsymbol{r}, \boldsymbol{v}_j,
t\right) \mathrm{d} \boldsymbol{v}_j
\end{equation}$$</span> 其中对所有不同速度的第<span
class="math inline"><em>j</em></span>种物种粒子求平均，它代表第<span
class="math inline"><em>j</em></span>种物种粒子的宏观流向。设<span
class="math inline"><em>m</em><sub><em>j</em></sub></span>为第<span
class="math inline"><em>j</em></span>种物种粒子的质量，于是<span
class="math inline"><em>j</em></span>的质量密度为： <span
class="math display">(<em>ρ</em><sub><em>m</em></sub>)<sub><em>j</em></sub> ≡ <em>m</em><sub><em>j</em></sub><em>ρ</em><sub><em>j</em></sub></span>
该混合稀薄气体体系的总质量密度为： <span
class="math display"><em>ρ</em><sub><em>m</em></sub> = ∑<sub><em>j</em></sub>(<em>ρ</em><sub><em>m</em></sub>)<sub><em>j</em></sub> = ∑<sub><em>j</em></sub><em>m</em><sub><em>j</em></sub><em>ρ</em><sub><em>j</em></sub></span></li>
<li>在<span class="math inline"><em>t</em></span>时刻<span
class="math inline"><em>r</em></span>处所有物种粒子总质量平均速度为：
<span class="math display">$$\begin{equation}
\boldsymbol{v}_0(\boldsymbol{r}, t) \equiv
\frac{\sum_j\left\langle\boldsymbol{v}_j\right\rangle\left(\rho_m\right)_j}{\sum_j\left(\rho_m\right)_j}=\frac{\sum_j\left\langle\boldsymbol{v}_j\right\rangle
m_j \rho_j}{\sum_j m_j \rho_j}=\frac{1}{\rho_m}
\sum_j\left\langle\boldsymbol{v}_j\right\rangle\left(\rho_m\right)_j
\end{equation}$$</span></li>
<li>第<span
class="math inline"><em>j</em></span>种物种中的某一粒子相对于整个流体质心的相对速度为：
<span
class="math display"><em>V</em><sub><em>j</em></sub> ≡ <em>v</em><sub><em>j</em></sub> − <em>v</em><sub>0</sub></span></li>
<li>用相对速度的局域瞬时的系综平均称为第<span
class="math inline"><em>j</em></span>种物种的扩散速度： <span
class="math display">$$\begin{equation}
\left\langle\boldsymbol{V}_j\right\rangle \equiv
\frac{1}{\rho_j(\boldsymbol{r}, t)}
\int\left(\boldsymbol{v}_j-\boldsymbol{v}_0\right)
f_j^{(1)}\left(\boldsymbol{r}, \boldsymbol{v}_j, t\right) \mathrm{d}
\boldsymbol{v}_j
\end{equation}$$</span> 乘以其质量密度后，再对所有物种加和，则： <span
class="math display">∑<sub><em>j</em></sub>⟨<em>V</em><sub><em>j</em></sub>⟩(<em>ρ</em><em>m</em>)<sub><em>j</em></sub> = 0</span></li>
</ol>
<h3 id="流向量">流向量</h3>
<p>设<span class="math inline"><em>ψ</em></span>为第<span
class="math inline"><em>j</em></span>种物种分子的某一性质（物理量），<span
class="math inline"><em>ψ</em><sub><em>j</em></sub></span>的流是指单位时间内通过单位截面的<span
class="math inline"><em>ψ</em><sub><em>j</em></sub></span>的量，即<span
class="math inline"><em>ψ</em><sub><em>j</em></sub></span>的通量。</p>
<figure>
<img src="./11-1.png" alt="11-1" />
<figcaption aria-hidden="true">11-1</figcaption>
</figure>
<p>单位时间，以相对速度并且有密度分布，穿过单位面积的粒子数为: <span
class="math display">(d<strong>s</strong> ⋅ <strong>V</strong><sub><em>j</em></sub> d<em>t</em>)(<em>f</em><sub><em>j</em></sub> d<strong>v</strong><sub><em>j</em></sub>) = d<em>s</em>(<strong>n</strong> ⋅ <strong>V</strong><sub><em>j</em></sub><em>f</em><sub><em>j</em></sub> d<strong>v</strong><sub><em>j</em></sub>)d<em>t</em></span></p>
<p>因此流等于： <span
class="math display">∫<em>ψ</em><sub><em>j</em></sub>(<strong>n</strong> ⋅ <strong>V</strong><sub><em>j</em></sub><em>f</em><sub><em>j</em></sub> d<strong>v</strong><sub><em>j</em></sub>) = <strong>n</strong> ⋅ ∫<em>ψ</em><sub><em>j</em></sub><strong>V</strong><sub><em>j</em></sub><em>f</em><sub><em>j</em></sub> d<strong>v</strong><sub><em>j</em></sub></span></p>
<p>因此定义流向量： <span
class="math display"><strong>Ψ</strong><sub><strong>j</strong></sub> = ∫<em>ψ</em><sub><em>j</em></sub><strong>V</strong><sub><em>j</em></sub><em>f</em><sub><em>j</em></sub> d<strong>v</strong><sub><em>j</em></sub></span></p>
<p><font color='red'>质量流向量、动量流向量、能量流向量（热流向量）</font></p>
<h3 id="boltzmann方程-1">Boltzmann方程</h3>
<p>讨论单粒子分布函数需要遵守的方程，即Boltzmann方程，这是稀薄气体严格的动理论的基本方程。由该方程出发，从分子质量、分子间相互作用，可得到指定温度下的扩散系数、导热系数、黏滞系数等输运性质。</p>
<p>继续讨论多组分混合稀薄气体。仅考虑两体的弹性碰撞。将单粒子分布函数<span
class="math inline"><em>f</em><sub><em>j</em></sub><sup>(</sup>1)</span>省略上标，简记为<span
class="math inline"><em>f</em><sub><em>j</em></sub></span>。</p>
<ol type="1">
<li><p>二体弹性碰撞</p></li>
<li><p>碰撞分析 任意时刻、任意位置碰撞前后第<span
class="math inline"><em>j</em></span>种组分粒子数一定守恒，故有： <span
class="math display">$$\begin{equation}
\begin{aligned}
&amp; f_j\left(\boldsymbol{r}+\boldsymbol{v}_j \mathrm{~d} t,
\boldsymbol{v}_j+\frac{\boldsymbol{X}_j}{m_j} \mathrm{~d} t,
t+\mathrm{d} t\right) \mathrm{d} \boldsymbol{r} \mathrm{d}
\boldsymbol{v}_j \\
= &amp; f_j\left(\boldsymbol{r}, \boldsymbol{v}_j, t\right) \mathrm{d}
\boldsymbol{r} \mathrm{d} \boldsymbol{v}_j+\sum_i\left(\Gamma_{j
i}^{(+)}-\Gamma_{j i}^{(-)}\right) \mathrm{d} \boldsymbol{r} \mathrm{d}
\boldsymbol{v}_j \mathrm{~d} t \\
\end{aligned}
\end{equation}$$</span> 其中<span
class="math inline"><em>Γ</em><sub><em>j</em><em>i</em></sub><sup>(+)</sup>d<strong>r</strong>d<strong>v</strong><sub><em>j</em></sub> d<em>t</em>, <em>Γ</em><sub><em>j</em><em>i</em></sub><sup>(−)</sup>d<strong>r</strong>d<strong>v</strong><sub><em>j</em></sub> d<em>t</em></span>分别代表流入的与流出相空间的粒子数。
<span class="math display">$$\begin{equation}
\begin{aligned}
f_j\left(\boldsymbol{r}+\boldsymbol{v}_j \mathrm{~d} t,
\boldsymbol{v}_j+\frac{\boldsymbol{X}_j}{m_j} \mathrm{~d} t,
t+\mathrm{d} t\right)=&amp;f_j\left(\boldsymbol{r}, \boldsymbol{v}_j,
t\right)+\frac{\partial f_j}{\partial t} \mathrm{~d}
t+\nabla_{\boldsymbol{r}} f_j \cdot \boldsymbol{v}_j \mathrm{~d}
t+\nabla_{\boldsymbol{v}_j} f_j \cdot \frac{\boldsymbol{X}_j}{m_j}
\mathrm{~d} t \\
\frac{\partial f_j}{\partial t}+\boldsymbol{v}_j \cdot
\nabla_{\boldsymbol{r}} f_j+\frac{\boldsymbol{X}_j}{m_j} \cdot
\nabla_{\boldsymbol{v}_j} f_j=&amp;\sum_i\left(\Gamma_{j
i}^{(+)}-\Gamma_{j i}^{(-)}\right), \quad \forall j
\end{aligned}
\end{equation}$$</span> 上面便是Boltzmann方程，代表第<span
class="math inline"><em>j</em></span>种组分粒子<span
class="math inline"><em>μ</em></span>空间的数密度时间演化规律。左边是非碰撞流动对数密度的贡献，称为漂移项；右边代表碰撞对数密度变化的贡献，称为碰撞项。</p>
<p>当体系处于定态的时候<span class="math inline">$\frac{\partial
f_j}{\partial t}=0$</span>，故Boltzmann方程为： <span
class="math display">$$\begin{equation}
\boldsymbol{v}_j \cdot \nabla_{\boldsymbol{r}}
f_j+\frac{\boldsymbol{X}_j}{m_j} \cdot \nabla_{\boldsymbol{v}_j}
f_j=\sum\left(\Gamma_{j i}^{(+)}-\Gamma_{j i}^{(-)}\right), \quad
\forall j .
\end{equation}$$</span></p></li>
<li><p>碰撞项分析 具体分析<span
class="math inline"><em>Γ</em><sub><em>j</em><em>i</em></sub><sup>(+)</sup>, <em>Γ</em><sub><em>j</em><em>i</em></sub><sup>(−)</sup></span>的计算方法，针对组分<span
class="math inline"><em>j</em></span>计算其碰撞<span
class="math inline"><em>i</em></span>组分之后脱离<span
class="math inline"><em>j</em></span>组分，记为<span
class="math inline"><em>Γ</em><sub><em>j</em><em>i</em></sub><sup>(−)</sup></span>:
<span
class="math display"><em>Γ</em><sub><em>j</em><em>i</em></sub><sup>(−)</sup> = 2<em>π</em>∬<em>f</em><sub><em>j</em></sub>(<strong>r</strong>, <strong>v</strong><sub><em>j</em></sub>, <em>t</em>)<em>f</em><sub><em>i</em></sub>(<strong>r</strong>, <strong>v</strong><sub><em>i</em></sub>, <em>t</em>)|<strong>v</strong><sub><em>i</em></sub> − <strong>v</strong><sub><em>j</em></sub>|<em>b</em> d<em>b</em> d<strong>v</strong><sub><em>i</em></sub></span>
这个表达式假定了粒子速度与位置无关，称为分子<strong>混沌假设</strong>。这是证明过程中薄弱的地方。
同理，考虑由<span class="math inline"><em>i</em></span>组分碰撞成<span
class="math inline"><em>j</em></span>组分的贡献。最后得到： <span
class="math display">$$\begin{equation}
\frac{\partial f_j}{\partial t}+\boldsymbol{v}_j \cdot
\nabla_{\boldsymbol{r}} f_j+\frac{\boldsymbol{X}_j}{m_j} \cdot
\nabla_{\boldsymbol{v}_j} f_j=2 \pi \sum_i \int_b
\int_{\boldsymbol{v}_i} b \mathrm{~d} b \mathrm{~d}
\boldsymbol{v}_i\left|\boldsymbol{v}_i-\boldsymbol{v}_j\right|\left\{f_j^{\prime}
f_i^{\prime}-f_j f_i\right\}, \quad \forall j
\end{equation}$$</span> 其中： <span
class="math display">$$\begin{equation}
\left\{\begin{aligned}
f_i^{\prime} &amp; \equiv f_i\left(\boldsymbol{r},
\boldsymbol{v}_i^{\prime}\left(\boldsymbol{v}_i,
\boldsymbol{v}_j\right), t\right) \\
f_j^{\prime} &amp; \equiv f_j\left(\boldsymbol{r},
\boldsymbol{v}_j^{\prime}\left(\boldsymbol{v}_i,
\boldsymbol{v}_j\right), t\right) \\
f_i &amp; \equiv f_i\left(\boldsymbol{r}, \boldsymbol{v}_i, t\right) \\
f_j &amp; \equiv f_j\left(\boldsymbol{r}, \boldsymbol{v}_j, t\right)
\end{aligned}\right.
\end{equation}$$</span></p></li>
</ol>
<h2 id="enskog方程">Enskog方程</h2>
<p>接下来从Boltamann输运方程出发，求混合稀薄气体中的各种输运过程中粒子<span
class="math inline"><em>ψ</em></span>的时间演化，这便是Enskog方程。</p>
<h3 id="性质的时间演化">性质的时间演化</h3>
<p>考虑<span class="math inline"><em>j</em></span>组分粒子的性质<span
class="math inline"><em>ψ</em></span>，将Boltzmann方程两边作用<span
class="math inline">∫d<strong>v</strong><sub><em>j</em></sub><em>ψ</em><sub><em>j</em></sub>(⋅)</span>得到：
<span class="math display">$$\begin{equation}
\begin{aligned}
&amp; \int \mathrm{d} \boldsymbol{v}_j \psi_j\left[\frac{\partial
f_j}{\partial t}+\boldsymbol{v}_j \cdot \nabla_r
f_j+\frac{\boldsymbol{X}_j}{m_j} \cdot \nabla_{\boldsymbol{v}_j}
f_j\right] \\
= &amp; 2 \pi \sum_i \iiint \psi_j\left(f_j^{\prime} f_i^{\prime}-f_j
f_i\right)\left|\boldsymbol{v}_i-\boldsymbol{v}_j\right| b \mathrm{~d} b
\mathrm{~d} \boldsymbol{v}_i \mathrm{~d} \boldsymbol{v}_j, \quad \forall
j
\end{aligned}
\end{equation}$$</span> 左边第一项写为： <span
class="math display">$$\begin{equation}
\frac{\partial}{\partial t} \int \mathrm{d} \boldsymbol{v}_j \psi_j
f_j-\int \mathrm{d} \boldsymbol{v}_j\left(\frac{\partial
\psi_j}{\partial t}\right) f_j=\frac{\partial}{\partial
t}\left(\rho_j\left\langle\psi_j\right\rangle\right)-\rho_j\left\langle\frac{\partial
\psi_j}{\partial t}\right\rangle
\end{equation}$$</span> 左边第二项写为： <span
class="math display">$$\begin{equation}
\begin{aligned}
\int \mathrm{d} \boldsymbol{v}_j \psi_j \boldsymbol{v}_j \cdot
\nabla_{\boldsymbol{r}} f_j &amp; =\nabla_{\boldsymbol{r}} \cdot \int
\mathrm{d} \boldsymbol{v}_j \psi_j \boldsymbol{v}_j f_j-\int \mathrm{d}
\boldsymbol{v}_j\left(\nabla_{\boldsymbol{r}} \psi_j\right) \cdot
\boldsymbol{v}_j f_j-\int \mathrm{d} \boldsymbol{v}_j
\psi_j\left(\nabla_{\boldsymbol{r}} \cdot \boldsymbol{v}_j\right) f_j \\
&amp; =\nabla_{\boldsymbol{r}} \cdot\left(\rho_j\left\langle\psi_j
\boldsymbol{v}_j\right\rangle\right)-\rho_j\left\langle\boldsymbol{v}_j
\cdot \nabla_{\boldsymbol{r}} \psi_j\right\rangle
\end{aligned}
\end{equation}$$</span> 左边第三项： <span
class="math display">$$\begin{equation}
\begin{aligned}
\int \mathrm{d} \boldsymbol{v}_j \psi_j \frac{\boldsymbol{X}_j}{m_j}
\cdot \nabla_{\boldsymbol{v}_j} f_j &amp; =\frac{1}{m_j}
\boldsymbol{X}_j \cdot \int \mathrm{d} \boldsymbol{v}_j \psi_j
\nabla_{\boldsymbol{v}_j} f_j \\
&amp; =\frac{1}{m_j} \boldsymbol{X}_j \cdot\left[\int \mathrm{d}
\boldsymbol{v}_j \nabla_{\boldsymbol{v}_j}\left(\psi_j f_j\right)-\int
\mathrm{d} \boldsymbol{v}_j\left(\nabla_{\boldsymbol{v}_j} \psi_j\right)
f_j\right]
\end{aligned}
\end{equation}$$</span> 结合Gauss定律： <span
class="math display">$$\begin{equation}
\iiint_V \mathrm{~d} \boldsymbol{v} \nabla\left(\begin{array}{c}
f \\
\cdot \boldsymbol{A} \\
\times \boldsymbol{A}
\end{array}\right)=\oint_S \mathrm{~d}
\boldsymbol{s}\left(\begin{array}{c}
f \\
\cdot \boldsymbol{A} \\
\times \boldsymbol{A}
\end{array}\right)
\end{equation}$$</span> 得到： <span
class="math display">∫d<strong>v</strong><sub><em>j</em></sub>∇<sub><strong>v</strong><sub><em>j</em></sub></sub>(<em>ψ</em><sub><em>j</em></sub><em>f</em><sub><em>j</em></sub>) = ∮<sub>edge</sub>d<strong>s</strong>(<em>ψ</em><sub><em>j</em></sub><em>f</em><sub><em>j</em></sub>) = 0.
</span></p>
<p>结合以上三项得到关于<span
class="math inline"><em>j</em></span>组分的Enskog方程： <span
class="math display">$$\begin{equation}
\begin{aligned}
&amp; \frac{\partial}{\partial
t}\left(\rho_j\left\langle\psi_j\right\rangle\right)+\nabla_{\boldsymbol{r}}
\cdot\left(\rho_j\left\langle\psi_j
\boldsymbol{v}_j\right\rangle\right)-\rho_j\left[\left\langle\frac{\partial
\psi_j}{\partial t}\right\rangle+\left\langle\boldsymbol{v}_j \cdot
\nabla_{\boldsymbol{r}} \psi_j\right\rangle+\frac{\boldsymbol{X}_j}{m_j}
\cdot\left\langle\nabla_{\boldsymbol{v}_j} \psi_j\right\rangle\right] \\
= &amp; 2 \pi \sum_i \iiint \psi_j\left(f_j^{\prime} f_i^{\prime}-f_j
f_i\right)\left|\boldsymbol{v}_{\boldsymbol{i}}-\boldsymbol{v}_j\right|
b \mathrm{~d} b \mathrm{~d} \boldsymbol{v}_i \mathrm{~d}
\boldsymbol{v}_j, \quad \forall j
\end{aligned}
\end{equation}$$</span></p>
<h3 id="不变量性质psi的enskog方程">不变量性质<span
class="math inline"><em>ψ</em></span>的Enskog方程</h3>
<p>部分性质在碰撞前后不变，即满足： <span
class="math display"><em>ψ</em><sub><em>i</em></sub>(<strong>v</strong><sub><em>i</em></sub>) + <em>ψ</em><sub><em>j</em></sub>(<strong>v</strong><sub><em>j</em></sub>) = <em>ψ</em><sub><em>i</em></sub><sup>′</sup>(<strong>v</strong><sub><em>i</em></sub><sup>′</sup>) + <em>ψ</em><sub><em>j</em></sub><sup>′</sup>(<strong>v</strong><sub><em>j</em></sub><sup>′</sup>),  ∀<em>i</em>, <em>j</em></span>
可以得到不变量的定理： <span
class="math display">∑<sub><em>i</em>, <em>i</em></sub>∭<em>ψ</em><sub><em>j</em></sub>(<em>f</em><sub><em>i</em></sub><sup>′</sup><em>f</em><sub><em>j</em></sub><sup>′</sup> − <em>f</em><sub><em>i</em></sub><em>f</em><sub><em>j</em></sub>)|<strong>v</strong><sub><em>i</em></sub> − <strong>v</strong><sub><em>j</em></sub>|<em>b</em> d<em>b</em> d<strong>v</strong><sub><em>i</em></sub> d<strong>v</strong><sub><em>j</em></sub> = 0</span>
可以得到不变量<span class="math inline"><em>ψ</em></span>的方程： <span
class="math display">$$\begin{equation}
\frac{\partial}{\partial t} \sum_j
\rho_j\left\langle\psi_j\right\rangle+\nabla_{\boldsymbol{r}} \cdot
\sum_j \rho_j\left\langle\psi_j \boldsymbol{v}_j\right\rangle-\sum_j
\rho_j\left[\left\langle\frac{\partial \psi_j}{\partial
t}\right\rangle+\left\langle\boldsymbol{v}_j \cdot
\nabla_{\boldsymbol{r}} \psi_j\right\rangle+\frac{\boldsymbol{X}_j}{m_j}
\cdot\left\langle\nabla_{\boldsymbol{v}_j} \psi_j\right\rangle\right]=0
\end{equation}$$</span></p>
<h2 id="boltzmann的h定理">Boltzmann的H定理</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">以下是基于LLM的回答：</span><br><span class="line">1. 什么是H定理？</span><br><span class="line"></span><br><span class="line">H定理是描述一个系统随时间演变向着熵增的方向发展的统计物理原理，由玻尔兹曼在1872年提出。</span><br><span class="line"></span><br><span class="line">H定理的核心内容在于它提供了一个概率性的框架来理解热力学第二定律，即在一个孤立系统中，总熵不会减少。玻尔兹曼通过引入一个特殊的物理量“H”，证明了在理想气体系统中，这个量随时间单调递减，表明系统趋向于最大熵状态，也就是热力学平衡状态。</span><br><span class="line"></span><br><span class="line">具体来说，H定理指出，系统的状态函数H(t)，定义为粒子分布函数f的对数乘以f在整个相空间的积分，会随时间单调递减，即dH/dt ≤ 0。等号成立的情况对应于系统达到了平衡态分布（例如麦克斯韦-玻尔兹曼分布）。</span><br><span class="line"></span><br><span class="line">此外，H定理对于理解热力学和统计力学之间的桥梁具有重要意义。它不仅揭示了单个分子动力学行为与宏观热力学变量之间的关系，也为后续关于系统远离平衡态的研究奠定了基础。</span><br><span class="line"></span><br><span class="line">2. H定理对于理解热力学和统计力学之间的桥梁有何重要意义？</span><br><span class="line">H定理在连接热力学和统计力学之间扮演了桥梁的角色，其重要意义体现在以下几个方面：</span><br><span class="line"></span><br><span class="line">从微观到宏观：H定理通过玻尔兹曼方程展示了如何从微观的粒子动力学行为出发，推导出宏观的热力学第二定律。这一过程体现了统计力学的一个核心思想，即宏观的不可逆现象（如熵增）可以由微观的可逆机制得出。</span><br><span class="line">定义热力学熵：H定理提供了一个可能的途径来定义热力学熵。它探讨了在何种条件下物理量H可以等同于热力学熵，并尝试将两者关联起来。</span><br><span class="line">探讨非平衡态：H定理与非平衡态统计物理有着密切的联系。在现代物理学中，对于非平衡系统的理解和描述仍然是一个重要的研究领域，H定理及其相关讨论为这一领域的研究提供了理论基础。</span><br><span class="line">引发深入讨论：H定理衍伸出许多关于其真实含意的讨论，这些讨论有助于深化我们对热力学和统计力学概念的理解，例如熵的本质以及它在物理学中的多种表述。</span><br><span class="line">总的来说，H定理不仅是理解热力学和统计力学关系的关键，也是推动物理学深入发展的一个动力源泉。</span><br></pre></td></tr></table></figure>
<p>考虑单组分体系。Boltxmann定义H函数为： <span
class="math display"><em>H</em>(<em>t</em>) ≡ ∬d<strong>r</strong>d<strong>v</strong><em>f</em>(<strong>r</strong>, <strong>v</strong>, <em>t</em>)ln <em>f</em>(<strong>r</strong>, <strong>v</strong>, <em>t</em>)</span>
H函数本质上就是“负熵”，以下从本章第一节的Boltamann积分微分方程出发，分析H定理的物理含义。
H函数对时间求导： <span class="math display">$$\begin{equation}
\begin{aligned}
\frac{\mathrm{d} H}{\mathrm{~d} t}=&amp;\iint \mathrm{d} r \mathrm{~d}
\boldsymbol{v}\left(\frac{\partial f}{\partial t}\right)(1+\ln f) \\
=&amp; -\iint \mathrm{d} \boldsymbol{r} \mathrm{d} \boldsymbol{v}(1+\ln
f) \boldsymbol{v} \cdot \nabla_{\boldsymbol{r}} f-\iint \mathrm{d}
\boldsymbol{r} \mathrm{d} \boldsymbol{v}(1+\ln f)
\frac{\boldsymbol{X}}{m} \cdot \nabla_{\boldsymbol{v}} f \\
&amp; +2 \pi \iiint \int \mathrm{d} b \mathrm{~d} \boldsymbol{v}_1
\mathrm{~d} \boldsymbol{r} \mathrm{d} \boldsymbol{v} b(1+\ln
f)\left|\boldsymbol{v}_1-\boldsymbol{v}\right|\left(f^{\prime}
f_1^{\prime}-f f_1\right) \\
=&amp; 2 \pi \iiint \int \mathrm{d} b \mathrm{~d} \boldsymbol{v}_1
\mathrm{~d} \boldsymbol{r} \mathrm{d} \boldsymbol{v} b(1+\ln
f)\left|\boldsymbol{v}_1-\boldsymbol{v}\right|\left(f^{\prime}
f_1^{\prime}-f f_1\right) \\
=&amp; 2 \pi \iiint \int \mathrm{d} b \mathrm{~d} \boldsymbol{v}_1
\mathrm{~d} \boldsymbol{r} \mathrm{d} \boldsymbol{v} b(1+\ln
f)\left|\boldsymbol{v}-\boldsymbol{v}_1\right|\left(f_1^{\prime}
f^{\prime}-f_1 f\right) \\
=&amp;\frac{1}{2}[(\cdot)+(\cdot \cdot)] \\
=&amp;2 \pi \iiint b \mathrm{~d} b \mathrm{~d} \boldsymbol{v}_1
\mathrm{~d} \boldsymbol{r} \mathrm{d}
\boldsymbol{v}\left|\boldsymbol{v}_1-\boldsymbol{v}\right|
\frac{1}{2}\left(\left(f^{\prime} f_1^{\prime}-f f_1\right)(1+\ln
f)+\left(f_1^{\prime} f^{\prime}-f_1 f\right)\left(1+\ln
f_1\right)\right) \\
=&amp;\pi \iiint b \mathrm{d} b \mathrm{~d} \boldsymbol{v}_1 \mathrm{~d}
\boldsymbol{r} \mathrm{d}
\boldsymbol{v}\left|\boldsymbol{v}_1-\boldsymbol{v}\right|\left(f^{\prime}
f_1^{\prime}-f f_1\right)\left(2+\ln \left(f f_1\right)\right) \\
=&amp;\pi \iiint \int b \mathrm{~d} b \mathrm{~d} v_1 \mathrm{~d} r
\mathrm{~d}
v\left|\boldsymbol{v}_1-\boldsymbol{v}\right|\left(f\left(\boldsymbol{v}^{\prime}\right)
f\left(\boldsymbol{v}_1^{\prime}\right)-f(\boldsymbol{v})
f\left(\boldsymbol{v}_1\right)\right)\left[2+\ln \left(f(\boldsymbol{v})
f\left(\boldsymbol{v}_1\right)\right)\right] \\
=&amp;\pi \iiint \int b^{\prime} \mathrm{d} b^{\prime} \mathrm{d}
\boldsymbol{v}_1^{\prime} \mathrm{d} \boldsymbol{r} \mathrm{d}
\boldsymbol{v}^{\prime}\left|\boldsymbol{v}_1^{\prime}-\boldsymbol{v}\right|\left(f(\boldsymbol{v})
f\left(\boldsymbol{v}_1\right)-f\left(\boldsymbol{v}^{\prime}\right)
f\left(\boldsymbol{v}_1^{\prime}\right)\right)\left[2+\ln
\left(f\left(\boldsymbol{v}^{\prime}\right)
f\left(\boldsymbol{v}_1^{\prime}\right)\right)\right] \\
=&amp; \pi \iiint \int b \mathrm{~d} b \mathrm{~d} \boldsymbol{v}_1
\mathrm{~d} \boldsymbol{r} \mathrm{d}
\boldsymbol{v}\left|\boldsymbol{v}_1-\boldsymbol{v}\right|\left(f
f_1-f^{\prime} f_1^{\prime}\right)\left(2+\ln \left(f^{\prime}
f_1^{\prime}\right)\right) \\
=&amp; \pi \iiint \int b \mathrm{~d} b \mathrm{~d} \boldsymbol{v}_1
\mathrm{~d} r \mathrm{~d} v\left|\boldsymbol{v}_1-\boldsymbol{v}\right|
\frac{1}{2}\left(f^{\prime} f_1^{\prime}-f f_1\right)\left[\ln \left(f
f_1\right)-\ln \left(f^{\prime} f_1^{\prime}\right)\right] \\
=&amp; \frac{\pi}{2} \iiint \int b \mathrm{~d} b \mathrm{~d}
\boldsymbol{v}_1 \mathrm{~d} \boldsymbol{r} \mathrm{d}
\boldsymbol{v}\left|\boldsymbol{v}_1-\boldsymbol{v}\right|(x-y) \ln
\left(\frac{y}{x}\right)
\end{aligned}
\end{equation}$$</span></p>
<p>通过分析<span
class="math inline"><em>x</em>, <em>y</em></span>的大小，可知<span
class="math inline">$\frac{\mathrm{~d} H}{\mathrm{~d} t}\leq
0$</span>，称之为H定理。</p>
<p>H定理受到“可逆性佯谬”与“重现性佯谬”的诘难，为了解释这个问题，需要先讨论“微观上可逆性”与“宏观上不可逆性”的概念。</p>
<h2
id="微观变化的可逆性与宏观变化的不可逆性">微观变化的可逆性与宏观变化的不可逆性</h2>
<h3 id="lagrange方程的时间反演可逆性">Lagrange方程的时间反演可逆性</h3>
<h3
id="schrodinger方程的时间反演可逆性">Schrodinger方程的时间反演可逆性</h3>
<h3 id="loschmidt佯谬">Loschmidt佯谬</h3>
<p>”Loschmidt佯谬“即“可逆性佯谬”，认为如果在某一时刻，令体系所有分子速度方向反转，<span
class="math inline"><em>v</em> → −<em>v</em></span>，按照微观运动的可逆性，每个分子将沿原来的轨迹回溯，整体表现出的宏观过程也就逆转了。这与H定理所表现出的宏观不可逆性是矛盾的。</p>
<p>这里对H定理进行修改，指出并不是严格的下降，在平衡态度也存在涨落，只是涨落很小，涨落很大的概率很小。</p>
<p>以下使用分子动力学模拟方法，对100个小球进行研究。其中分别在碰撞50次与100次的时候，将速度进行翻转，较大的点表示为速度没有翻转情况，较小的点表示对速度进行翻转。可以看出，在速度未翻转的情况下，H是逐渐下降的，符合H定理的预言。abc三幅图分别表示在不同随机性下的模型，可以发现在随机性较小的情况下，确实H增大到初始状态，但是随着随机涨落变大，可逆性性基本消失。</p>
<figure>
<img src="./11-2.png" alt="Loschmidt佯谬" />
<figcaption aria-hidden="true">Loschmidt佯谬</figcaption>
</figure>
<h3 id="zermelo佯谬">Zermelo佯谬</h3>
<p>根据动力学理论，孤立有限大保守的动力学体系，在其演化的过程中，经过有限长的时间，总是能回复到任意接近它原来起始的状态。“Zermelo佯谬”也称“重现性佯谬”认为，既然能够接近原来的状态，那么如何认为H定理总是下降？</p>
<p>调和方案依旧是利用涨落概念，重现初始状态是利用涨落，重现时间相当长。
<font color='red'>这本书并没有更细致的讲解。</font></p>
<h1 id="概率论方法">概率论方法</h1>
<h2 id="随机过程">随机过程</h2>
<p>Markov过程的时间演化方程称为Fokker-Planck方程。</p>
<h2
id="联合概率条件概率联合条件概率">联合概率、条件概率、联合条件概率</h2>
<h2
id="markov过程chapman-kolmogorov方程">Markov过程、Chapman-Kolmogorov方程</h2>
<p><a
href="https://renyixiong-ai.github.io/2021/09/16/Markov_Chain/">Markov过程详细介绍。</a></p>
<p>Chapman-Kolmogorov方程（简称C-K方程），或称为 Smoluchowski
方程，描述马尔可夫过程中转移概率之间的联系： <span
class="math display"><em>P</em><sub>1 ∣ 1</sub>(<em>y</em><sub>3</sub>, <em>t</em><sub>3</sub> ∣ <em>y</em><sub>1</sub>, <em>t</em><sub>1</sub>) = ∫d<em>y</em><sub>2</sub><em>P</em><sub>1 ∣ 1</sub>(<em>y</em><sub>3</sub>, <em>t</em><sub>3</sub> ∣ <em>y</em><sub>2</sub>, <em>t</em><sub>2</sub>)<em>P</em><sub>1 ∣ 1</sub>(<em>y</em><sub>2</sub>, <em>t</em><sub>2</sub> ∣ <em>y</em><sub>1</sub>, <em>t</em><sub>1</sub>)</span></p>
<h2 id="主方程">主方程</h2>
<p>单位时间内体系<span
class="math inline"><em>y</em><sub>1</sub></span>转移到<span
class="math inline"><em>y</em><sub>2</sub></span>的条件概率密度，概率密度跃迁速度<span
class="math inline"><em>W</em><sub><em>t</em></sub>(<em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>)</span>：
<span class="math display">$$\begin{equation}
W_t\left(y_1, y_2\right) \equiv \lim _{\tau \rightarrow
0}\left\{\frac{P_{1 \mid 1}\left(y_2, t_1+\tau \mid y_1,
t_1\right)}{\tau}\right\}_{y_1 \neq y_2}
\end{equation}$$</span></p>
<p>Pauli主方程（Master equation）： <span
class="math display">$$\begin{equation}
\frac{\partial P_1\left(y_2, t\right)}{\partial t}=\int_{\text {all }
y_1} \mathrm{~d} y_1\left\{W_t\left(y_1, y_2\right) P_1\left(y_1,
t\right)-W_t\left(y_2, y_1\right) P_1\left(y_2, t\right)\right\}
\end{equation}$$</span> 右边第一项表示从其它态到<span
class="math inline"><em>y</em><sub>2</sub></span>，第二项表示从<span
class="math inline"><em>y</em><sub>2</sub></span>变化到其它态的项。主方程就是Markov过程概率分布的时间演化。</p>
<h2 id="fokker-planck方程">Fokker-Planck方程</h2>
<p>略去跃迁速度的下标，认为跃迁速度取决于初态及其跃迁幅度<span
class="math inline"><em>ξ</em> ≡ <em>y</em> − <em>y</em><sup>′</sup></span>。
<span
class="math display"><em>W</em>(<em>y</em><sup>′</sup>, <em>y</em>) ≡ <em>W</em>(<em>y</em><sup>′</sup>; <em>y</em> − <em>y</em><sup>′</sup>) = <em>W</em>(<em>y</em><sup>′</sup>; <em>ξ</em>)</span></p>
<p>利用变化与Taylor展开，可将主方程改写为： <span
class="math display">$$\begin{equation}
\begin{aligned}
\frac{\partial P_1(y, t)}{\partial t}= &amp; \int \mathrm{d} \xi P_1(y,
t) W(y ; \xi)+\sum_{n=1}^{\infty} \frac{(-1)^n}{n !}
\frac{\partial^n}{\partial y^n}\left[P_1(y, t) \int \mathrm{d} \xi \xi^n
W(y ; \xi)\right] \\
&amp; -P_1(y, t) \int \mathrm{d} \xi W(y ;-\xi)
\end{aligned}
\end{equation}$$</span></p>
<p>可以假定，当<span
class="math inline">|<em>ξ</em>|</span>变大时，跃迁速度<span
class="math inline"><em>W</em></span>会很快变小。 <img src="./12-1.png"
alt="跃迁速度与跃迁幅度" /></p>
<p><span
class="math display"><em>P</em><sub>1</sub>(<em>y</em> + <em>Δ</em><em>y</em>, <em>t</em>) ≃ <em>P</em><sub>1</sub>(<em>y</em>, <em>t</em>),  ∀|<em>Δ</em><em>y</em>| &lt; <em>δ</em>,</span>
得到<span
class="math inline"><em>W</em>(<em>y</em>; <em>ξ</em>) = <em>W</em>(<em>y</em>; −<em>ξ</em>)</span>，同时令
<span
class="math display"><em>α</em><sub><em>n</em></sub>(<em>y</em>) ≡ ∫d<em>ξ</em><em>ξ</em><sup><em>n</em></sup><em>W</em>(<em>y</em>; <em>ξ</em>)</span></p>
<p>得到： <span class="math display">$$\begin{equation}
\frac{\partial P_1(y, t)}{\partial t}=\sum_{n=1}^{\infty}
\frac{(-1)^n}{n !} \frac{\partial^n}{\partial y^n}\left[\alpha_n(y)
P_1(y, t)\right]
\end{equation}$$</span></p>
<p>如果只取到二次统计矩，忽略高阶统计矩，得到但变的Fokker-Planck方程：
<span class="math display">$$\begin{equation}
\frac{\partial P_1(y, t)}{\partial t} \simeq-\frac{\partial}{\partial
y}\left[\alpha_1(y) P_1(y, t)\right]+\frac{1}{2}
\frac{\partial^2}{\partial y^2}\left[\alpha_2(y) P_1(y, t)\right]
\end{equation}$$</span></p>
<p>当有两个变量时，对应Chapman-Kolmogorov方程： <span
class="math display"><em>P</em><sub>1 ∣ 1</sub>(<em>x</em><sub>3</sub>, <em>y</em><sub>3</sub>, <em>t</em><sub>3</sub> ∣ <em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>, <em>t</em><sub>1</sub>) = ∫d<em>x</em><sub>2</sub> d<em>y</em><sub>2</sub><em>P</em><sub>1 ∣ 1</sub>(<em>x</em><sub>3</sub>, <em>y</em><sub>3</sub>, <em>t</em><sub>3</sub> ∣ <em>x</em><sub>2</sub>, <em>y</em><sub>2</sub>, <em>t</em><sub>2</sub>)<em>P</em><sub>1 ∣ 1</sub>(<em>x</em><sub>2</sub>, <em>y</em><sub>2</sub>, <em>t</em><sub>2</sub> ∣ <em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>, <em>t</em><sub>1</sub>)</span></p>
<p>同理, 可以得到对应的 Fokker-Planck 方程 <span class="math display">$$
\begin{aligned}
\frac{\partial P_1(x, y, t)}{\partial t}= &amp;
-\frac{\partial}{\partial x}\left[\alpha_1 P_1(x, y,
t)\right]-\frac{\partial}{\partial y}\left[\beta_1 P_1(x, y, t)\right]
\\
&amp; +\frac{1}{2} \frac{\partial^2}{\partial x^2}\left[\alpha_2 P_1(x,
y, t)\right]+\frac{1}{2} \frac{\partial^2}{\partial y^2}\left[\beta_2
P_1(x, y, t)\right]+\frac{\partial^2}{\partial x \partial y}\left[\gamma
P_1(x, y, t)\right]
\end{aligned}
$$</span></p>
<p>其中 <span class="math display">$$
\begin{aligned}
\alpha_n(x, y) &amp; \equiv \iint \mathrm{d} \xi \mathrm{d} \zeta \xi^n
W(x, y ; \xi, \zeta) \\
\beta_n(x, y) &amp; \equiv \iint \mathrm{d} \xi \mathrm{d} \zeta \zeta^n
W(x, y ; \xi, \zeta) \\
\gamma(x, y) &amp; \equiv \iint \mathrm{d} \xi \mathrm{d} \zeta \xi
\zeta W(x, y ; \xi, \zeta)
\end{aligned}
$$</span></p>
<h2
id="从fokker-planck方程到fick定律">从Fokker-Planck方程到Fick定律</h2>
<h1
id="brown运动langevin方程及fokker-planck方程">Brown运动、Langevin方程及Fokker-Planck方程</h1>
<h2 id="brown-运动和langevin方程">Brown 运动和Langevin方程</h2>
<p>Brown运动理论的基本方程称为Langevin方程，含有随机力与摩擦力。涨落耗散定理把这些力联系起来。这样一类引入随机过程用来处理复杂体系，或对同一体系相宏观方向提高描述的层次，从而达到粗粒化目的的方法，就是推广意义上的Brown运动。</p>
<h3 id="无外场langevin方程">无外场Langevin方程</h3>
<p><span class="math display">$$\begin{equation}
\begin{aligned}
&amp;m \dot{\boldsymbol{v}}=-m \zeta \boldsymbol{v}+\boldsymbol{f}(t)\\
&amp;\dot{\boldsymbol{v}}=-\zeta
\boldsymbol{v}+\frac{\boldsymbol{f}(t)}{m}
\end{aligned}
\end{equation}$$</span> 其中<span
class="math inline"><em>f</em>(<em>t</em>)</span>是随机力，<span
class="math inline">−<em>m</em><em>ζ</em><em>v</em></span>是粘滞力（摩擦力），该方程称为无外场Langevin方程。</p>
<p>利用Laplace变换： <span class="math display">$$
\begin{gathered}
s \tilde{\boldsymbol{v}}(s)-\boldsymbol{v}(0)=-\zeta
\tilde{\boldsymbol{v}}(s)+\mathcal{L}\left(\frac{\boldsymbol{f}(t)}{m}\right)
\\
\tilde{\boldsymbol{v}}(s)=(s+\zeta)^{-1}
\tilde{\boldsymbol{v}}(0)+(s+\zeta)^{-1}
\mathcal{L}\left(\frac{\boldsymbol{f}(t)}{m}\right)
\end{gathered}
$$</span></p>
<p>再利用反变换得到： <span class="math display">$$
\boldsymbol{v}(t)=\mathrm{e}^{-\zeta t}
\boldsymbol{v}(0)+\mathrm{e}^{-\zeta t} *
\mathcal{L}\left(\frac{\boldsymbol{f}(t)}{m}\right)
$$</span> <span class="math display">$$
\boldsymbol{v}(t)=\mathrm{e}^{-\zeta t} \boldsymbol{v}(0)+\int_0^t
\mathrm{~d} \tau \mathrm{e}^{-\zeta(t-\tau)}
\frac{\boldsymbol{f}(\tau)}{m}
$$</span>
右边第一项给出了初始速度的衰减，第二项给出了随机力造成的速度。</p>
<p>假设随机力的平均值和它的自时间相关函数服从以下关系： <span
class="math display">⟨<strong>f</strong>(<em>t</em>)⟩ = 0 and
⟨<strong>f</strong>(<em>t</em>) ⋅ <strong>f</strong>(<em>t</em><sup>′</sup>)⟩ = <em>ϕ</em>(<em>t</em> − <em>t</em><sup>′</sup>),</span></p>
<h3 id="速度的自时间相关函数">速度的自时间相关函数</h3>
<p>接下来考虑Brown运动体系处于稳态时长时间的速度的自时间相关函数。</p>
<p>当<span
class="math inline"><em>t</em> → ∞</span>时，第二项进行变量替换<span
class="math inline"><em>u</em> ≡ <em>t</em> − <em>τ</em></span>： <span
class="math display">$$\begin{equation}
\begin{aligned}
\lim _{t \rightarrow \infty} \boldsymbol{v}(t) &amp;\simeq \lim _{t
\rightarrow \infty} \int_t^0(-\mathrm{d} u) \mathrm{e}^{-\zeta u}
\frac{\boldsymbol{f}(t-u)}{m}\\
&amp;=\frac{1}{m} \int_0^{\infty} \mathrm{d} u \mathrm{e}^{-\zeta u}
f(t-u)
\end{aligned}
\end{equation}$$</span> 速度的自时间相关函数，用时间平均可表示为： <span
class="math display">$$\begin{equation}
\begin{aligned}
\left\langle\boldsymbol{v}(t) \cdot
\boldsymbol{v}\left(t^{\prime}\right)\right\rangle &amp;= \lim _{\tau
\rightarrow \infty} \frac{1}{\tau} \int_0^\tau \mathrm{d} s
\boldsymbol{v}(t+s) \cdot \boldsymbol{v}\left(t^{\prime}+s\right) \\
&amp;=\lim _{\tau \rightarrow \infty} \int_0^{\infty} \mathrm{d} u_1
\int_0^{\infty} \mathrm{d} u_2 \mathrm{e}^{-\zeta\left(u_1+u_2\right)}
\frac{1}{\tau} \int_0^\tau \mathrm{d} s \frac{1}{m^2}
\boldsymbol{f}\left(t+s-u_1\right) \cdot
\boldsymbol{f}\left(t^{\prime}+s-u_2\right)
\end{aligned}
\end{equation}$$</span> 考虑随机力的性质： <span
class="math display">⟨<strong>f</strong>(<em>t</em>) ⋅ <strong>f</strong>(<em>t</em><sup>′</sup>)⟩ = <em>ϕ</em>(<em>t</em> − <em>t</em><sup>′</sup>) ≃ <em>λ</em><em>δ</em>(<em>t</em> − <em>t</em><sup>′</sup>)</span></p>
<p>进一步得到 <span class="math display">$$
\left\langle\boldsymbol{v}(t) \cdot
\boldsymbol{v}\left(t^{\prime}\right)\right\rangle=\int_0^{\infty}
\mathrm{d} u_1 \int_0^{\infty} \mathrm{d} u_2
\mathrm{e}^{-\zeta\left(u_1+u_2\right)} \lim _{\tau \rightarrow \infty}
\frac{1}{\tau} \int_0^\tau \mathrm{d} s \frac{1}{m^2} \lambda
\delta\left[\left(t-u_1\right)-\left(t^{\prime}-u_2\right)\right],
$$</span></p>
<p>其中对 <span class="math inline"><em>s</em></span> 的积分为 <span
class="math display">$$
\begin{aligned}
\lim _{\tau \rightarrow \infty} \frac{1}{\tau} \int_0^\tau \mathrm{d} s
\frac{1}{m^2} \lambda
\delta\left[\left(t-u_1\right)-\left(t^{\prime}-u_2\right)\right] &amp;
=\frac{\lambda}{m^2}
\delta\left[\left(t-t^{\prime}\right)-\left(u_1-u_2\right)\right] \lim
_{\tau \rightarrow \infty} \frac{1}{\tau} \int_0^\tau \mathrm{d} s \\
&amp; =\frac{\lambda}{m^2}
\delta\left[u_2-\left(u_1-t+t^{\prime}\right)\right],
\end{aligned}
$$</span></p>
<p>进而 <span class="math display">$$
\begin{aligned}
\left\langle\boldsymbol{v}(t) \cdot
\boldsymbol{v}\left(t^{\prime}\right)\right\rangle &amp;
=\int_0^{\infty} \mathrm{d} u_1 \int_0^{\infty} \mathrm{d} u_2
\mathrm{e}^{-\zeta\left(u_1+u_2\right)} \frac{\lambda}{m^2}
\delta\left[u_2-\left(u_1-t+t^{\prime}\right)\right] \\
&amp; =\frac{\lambda}{m^2} \mathrm{e}^{-\zeta\left(t^{\prime}-t\right)}
\int_0^{\infty} \mathrm{d} u_1 \mathrm{e}^{-2 \zeta u_1} .
\end{aligned}
$$</span></p>
<p>最右边的积分等于 <span class="math inline">1/(2<em>ζ</em>)</span>,
于是得到 <span class="math display">$$
\left\langle\boldsymbol{v}(t) \cdot
\boldsymbol{v}\left(t^{\prime}\right)\right\rangle=\frac{\lambda}{2
\zeta m^2} \mathrm{e}^{-\zeta\left(t^{\prime}-t\right)} .
$$</span></p>
<p>又因为必有 <span
class="math inline">⟨<strong>v</strong>(<em>t</em>) ⋅ <strong>v</strong>(<em>t</em><sup>′</sup>)⟩ = ⟨<strong>v</strong>(<em>t</em><sup>′</sup>) ⋅ <strong>v</strong>(<em>t</em>)⟩</span>,
故得到 <span class="math display">$$
\left\langle\boldsymbol{v}(t) \cdot
\boldsymbol{v}\left(t^{\prime}\right)\right\rangle=\frac{\lambda}{2
\zeta m^2} \mathrm{e}^{-\zeta\left|t^{\prime}-t\right|},
$$</span></p>
<p>即时间相关函数取决于时间间隔的绝对值 <span
class="math inline">|<em>t</em> − <em>t</em><sup>′</sup>|</span>. 现在,
再根据 Brown 运动中的涨落耗散定理, 可以得到稳态时 Brown
粒子速度的自时间相关函数为 <span class="math display">$$
\left\langle\boldsymbol{v}(t) \cdot
\boldsymbol{v}\left(t^{\prime}\right)\right\rangle=\frac{3
k_{\mathrm{B}} T}{m} \mathrm{e}^{-\zeta\left|t^{\prime}-t\right|} .
$$</span></p>
<p>根据Doob定理，若Gauss过程的自相关函数是指数形的。则该过程一定是一个Markov过程。</p>
<h3 id="brown粒子的均方位移">Brown粒子的均方位移</h3>
<p>得到了稳态时 Brown 粒子速度的自时间相关函数之后, 就可以求解 Brown
运动中粒子位移平方的系综平均值, 简称均方位移(MSD).
因为速度是位置的一阶导数, 即 <span class="math display">$$
\left\langle\dot{\boldsymbol{r}}(t) \cdot
\dot{\boldsymbol{r}}\left(t^{\prime}\right)\right\rangle=\left\langle\boldsymbol{v}(t)
\cdot \boldsymbol{v}\left(t^{\prime}\right)\right\rangle \quad \text {
和 } \quad \boldsymbol{r}(t)=\int_0^t \mathrm{~d} \tau
\boldsymbol{v}(\tau),
$$</span></p>
<p>于是求解稳态 (即 <span
class="math inline"><em>t</em> ≫ <em>ζ</em><sup>−1</sup></span> ) 时
Brown 粒子的均方位移 <span
class="math inline">⟨<em>r</em>(<em>t</em>)<sup>2</sup>⟩</span>,
只需对速度自相关积分两次, 即 <span class="math display">$$
\begin{aligned}
\left\langle r(t)^2\right\rangle=\lim _{t \rightarrow
t^{\prime}}\left\langle\boldsymbol{r}(t) \cdot
\boldsymbol{r}\left(t^{\prime}\right)\right\rangle &amp; =\lim _{t
\rightarrow t^{\prime}} \int_0^{t^{\prime}} \mathrm{d} \tau^{\prime}
\int_0^t \mathrm{~d} \tau\left\langle\boldsymbol{v}(\tau) \cdot
\boldsymbol{v}\left(\tau^{\prime}\right)\right\rangle \\
&amp; =\lim _{t \rightarrow t^{\prime}} \int_0^{t^{\prime}} \mathrm{d}
\tau^{\prime} \int_0^t \mathrm{~d} \tau \frac{\lambda}{2 \zeta m^2}
\mathrm{e}^{-\zeta\left|\tau-\tau^{\prime}\right|},
\end{aligned}
$$</span></p>
<p>即 <span class="math display">$$
\left\langle r(t)^2\right\rangle=\int_0^t \mathrm{~d} \tau^{\prime}
\int_0^t \mathrm{~d} \tau \frac{\lambda}{2 \zeta m^2} \mathrm{e}^{-\zeta
\mid \tau-\tau^{\prime}} .
$$</span></p>
<p>式 (13.1.3-1) 中的积分属于 <span
class="math inline"><em>I</em> = ∫<sub>0</sub><sup><em>t</em></sup> d<em>τ</em>∫<sub>0</sub><sup><em>t</em></sup> d<em>τ</em><sup>′</sup><em>f</em>(<em>τ</em> − <em>τ</em><sup>′</sup>)</span>
类型, 可以证明, 此类积分有如下性质: <span
class="math display">∫<sub>0</sub><sup><em>t</em></sup> d<em>τ</em>∫<sub>0</sub><sup><em>t</em></sup> d<em>τ</em><sup>′</sup><em>f</em>(<em>τ</em> − <em>τ</em><sup>′</sup>) = ∫<sub>0</sub><sup><em>t</em></sup> d<em>u</em>(<em>t</em> − <em>u</em>)[<em>f</em>(<em>u</em>) + <em>f</em>(−<em>u</em>)]</span></p>
<p>得到： <span class="math display">$$
\begin{aligned}
\left\langle r(t)^2\right\rangle &amp; =\int_0^t \mathrm{~d}
\tau^{\prime} \int_0^t \mathrm{~d} \tau \frac{\lambda}{2 \zeta m^2}
\mathrm{e}^{-\zeta\left|\tau-\tau^{\prime}\right|}=\int_0^t \mathrm{~d}
u(t-u)\left[\frac{\lambda}{2 \zeta m^2}
\mathrm{e}^{-\zeta|u|}+\frac{\lambda}{2 \zeta m^2}
\mathrm{e}^{-\zeta|-u|}\right] \\
&amp; =\frac{\lambda}{\zeta m^2} \int_0^t \mathrm{~d} u(t-u)
\mathrm{e}^{-\zeta|u|}=\frac{\lambda}{\zeta m^2} \int_0^t \mathrm{~d}
u(t-u) \mathrm{e}^{-\zeta u} .
\end{aligned}
$$</span></p>
<p>可以直接积分得到其中的 <span class="math display">$$
\int_0^t \mathrm{~d} u(t-u) \mathrm{e}^{-\zeta
u}=\frac{t}{\zeta}+\frac{1}{\zeta^2}\left(\mathrm{e}^{-\zeta t}-1\right)
.
$$</span></p>
<p>于是 <span class="math display">$$
\left\langle r(t)^2\right\rangle=\frac{\lambda}{\zeta
m^2}\left[\frac{t}{\zeta}+\frac{1}{\zeta^2}\left(\mathrm{e}^{-\zeta
t}-1\right)\right] .
$$</span></p>
<p>在时间较长的情况下有 <span class="math display">$$
\lim _{t \rightarrow \infty}\left\langle r^2(t)\right\rangle \simeq
\frac{\lambda}{\zeta^2 m^2} t .
$$</span></p>
<p>至此, 理论把 Langevin 方程中的摩擦系数 <span
class="math inline"><em>ζ</em><em>m</em></span>, 随机力强度 <span
class="math inline"><em>λ</em></span> 与均方位移联系起来了,
还没有联系到扩散系数 <span class="math inline"><em>D</em></span>.</p>
<h3 id="唯象规律中的扩散系数">唯象规律中的扩散系数</h3>
<p>扩散系数<span
class="math inline"><em>D</em></span>的概念是从实验中引入。 <span
class="math display"><em>ρ</em><strong>v</strong> = −<em>D</em>∇<em>ρ</em></span>
其中<span
class="math inline"><em>ρ</em>(<em>r</em>, <em>t</em>)</span>表示<span
class="math inline"><em>t</em></span>时刻<span
class="math inline"><em>r</em></span>处单位溶液体积内溶质的粒子数，即数密度；<span
class="math inline"><em>v</em></span>为溶质粒子的速度；<span
class="math inline"><em>D</em></span>为扩散系数，准确点为平动扩散系数。上式称为Fick第一定律。</p>
<p>另一方面，从粒子数守恒方程出发： <span
class="math display">$$\begin{align}
0=&amp;\frac{\partial \rho}{\partial t}+ \nabla\cdot(\rho
\boldsymbol{v}) \\
=&amp;  \frac{\partial \rho}{\partial t}+ \nabla\cdot(-D\nabla\rho) \\
=&amp; \frac{\partial \rho}{\partial t}-D \nabla\cdot\nabla\rho \\
\frac{\partial \rho}{\partial t}=&amp;D \nabla^2\rho
\end{align}$$</span>
称为Fick第二定律，表示溶液中任意一处溶质数密度随时间的增速正比于<span
class="math inline">∇<sup>2</sup><em>ρ</em></span>，比例系数为平动扩散系数。</p>
<p>以下假设初始情况（<span
class="math inline"><em>t</em> = 0</span>）<span
class="math inline"><em>N</em></span>个粒子再<span
class="math inline"><em>r</em><sub>0</sub></span>处： <span
class="math display"><em>ρ</em>(<em>r</em>, 0) = <em>N</em><em>δ</em>(<em>r</em> − <em>r</em><sub>0</sub>)</span>
再设<span
class="math inline"><em>G</em>(<em>r</em>, <em>t</em>)<em>d</em><em>r</em></span>为<span
class="math inline"><em>t</em></span>时刻在微体积<span
class="math inline"><em>d</em><em>r</em></span>中出现溶质粒子的概率，于是：
<span class="math display">$$\begin{equation}
G(r, t) = \frac{\rho(r, t)}{N}
\end{equation}$$</span> 因此： <span
class="math display">$$\begin{align}
\frac{\partial G(r, t)}{\partial t}=&amp;D \nabla^2G(r, t) \\
G(r, 0) =&amp; \delta(r-r_0)
\end{align}$$</span> 利用傅里叶变换解得： <span
class="math display">$$\begin{equation}
G(r, t)=\left(\frac{1}{2 \pi}\right)^3\left(\frac{\pi}{D t}\right)^{3 /
2} \mathrm{e}^{-\frac{\left|r-r_0\right|^2}{4 D t}}
\end{equation}$$</span></p>
<p>任意物理量<span
class="math inline"><em>A</em>(<em>r</em>)</span>的系综平均为： <span
class="math display">$$\begin{align}
\langle A\rangle = \int_\Omega dr A(r)G(r, t)
\end{align}$$</span></p>
<h2
id="从langevin方程到fokker-planck方程">从Langevin方程到Fokker-Planck方程</h2>
<h3
id="无外场langevin方程的fokker-planck方程">无外场Langevin方程的Fokker-Planck方程</h3>
<p>设Brown粒子的速度平均为<span
class="math inline"><em>ξ</em></span>，在<span
class="math inline"><em>t</em></span>时刻粒子速度处于<span
class="math inline">|<em>ξ</em>, <em>ξ</em> + <em>d</em><em>ξ</em>|</span>的概率为<span
class="math inline"><em>P</em>(<em>ξ</em>, <em>t</em>)</span>，概率密度的定义为：
<span class="math display">$$\begin{align}
P(\xi, t)\equiv \langle\delta(\xi-v(t))\rangle
\end{align}$$</span> 密度与概率密度之间的对应关系为： <span
class="math display">$$\begin{equation}
\begin{array}{ccccc}
\hline \rho(\boldsymbol{r})=\sum_{i=1}^N
\delta\left(\boldsymbol{r}-\boldsymbol{r}_{\boldsymbol{i}}\right) &amp;
\rho(\boldsymbol{r}) &amp; \boldsymbol{r} &amp; \boldsymbol{r}_i &amp;
\sum_{i=1}^N(\cdot) \\
\hline P(\boldsymbol{\xi}, t)
\equiv\langle\delta(\boldsymbol{\xi}-\boldsymbol{v}(t))\rangle &amp;
P(\boldsymbol{\xi}, t) &amp; \boldsymbol{\xi} &amp; \boldsymbol{v}(t)
&amp; \langle\cdot\rangle \\
\hline
\end{array}
\end{equation}$$</span> 求某物理量的运动方程就是求<span
class="math inline"><em>P</em></span>的时间偏导数。</p>
<p><span class="math display">$$\begin{equation}
\begin{aligned}
\frac{\partial}{\partial t} P(\boldsymbol{\xi},
t)&amp;=-\frac{\partial}{\partial
\boldsymbol{\xi}}\langle\delta(\boldsymbol{\xi}-\boldsymbol{v}(t))
\dot{\boldsymbol{v}}(t)\rangle \\
&amp;
=\frac{\partial\langle\delta(\boldsymbol{\xi}-\boldsymbol{v}(t))\rangle}{\partial(\boldsymbol{\xi}-\boldsymbol{v}(t))}
\frac{\partial(\boldsymbol{\xi}-\boldsymbol{v}(t))}{\partial
t}=\frac{\partial\langle\delta(\boldsymbol{\xi}-\boldsymbol{v}(t))\rangle}{\partial
\boldsymbol{\xi}}[-\dot{\boldsymbol{v}}(t)] \\
&amp; =-\frac{\partial}{\partial
\boldsymbol{\xi}}\langle\delta(\boldsymbol{\xi}-\boldsymbol{v}(t))
\dot{\boldsymbol{v}}(t)\rangle \\
&amp; =-\frac{\partial}{\partial
\boldsymbol{\xi}}\left\langle\delta(\boldsymbol{\xi}-\boldsymbol{v}(t))\left(-\zeta
\boldsymbol{\xi}+\frac{\boldsymbol{f}(t)}{m}\right)\right\rangle \\
&amp; =\frac{\partial}{\partial \boldsymbol{\xi}}\{\zeta
\boldsymbol{\xi}\langle\delta(\boldsymbol{\xi}-\boldsymbol{v}(t))\rangle\}-\frac{1}{m}
\frac{\partial}{\partial
\boldsymbol{\xi}}\langle\delta(\boldsymbol{\xi}-\boldsymbol{v}(t))
\boldsymbol{f}(t)\rangle \\
&amp; =\frac{\partial}{\partial \boldsymbol{\xi}}[\zeta
P(\boldsymbol{\xi}, t) \boldsymbol{\xi}]-\frac{1}{m}
\frac{\partial}{\partial
\boldsymbol{\xi}}\langle\delta(\boldsymbol{\xi}-\boldsymbol{v}(t))
\boldsymbol{f}(t)\rangle
\end{aligned}
\end{equation}$$</span></p>
<p>假设随机力的概率密度为Gauss分布： <span
class="math display">$$\begin{equation}
P[\boldsymbol{f}(t)]=\exp \left\{-\int_{t_0}^{t_f} \mathrm{~d} t
\frac{f^2(t)}{4 \zeta m k_{\mathrm{B}} T}\right\}
\end{equation}$$</span> 其中 <span
class="math inline"><em>t</em><sub>0</sub>, <em>t</em><sub><em>f</em></sub></span>
分别为 <span class="math inline"><strong>f</strong>(<em>t</em>)</span>
的初始时间和结束时间. 为了理解泛函型的概率密度, 要对 Gauss
分布多作些解释: 因为这里整个范围内的随机力函数 <span
class="math inline"><strong>f</strong>(<em>t</em>)</span> 都与一个标量
<span class="math inline"><em>P</em></span> 对应,
所以这样的概率密度是一个泛函 <span
class="math inline"><em>P</em>[<em>f</em>(<em>t</em>)]</span>,
而不是复合函数 <span
class="math inline"><em>P</em>(<em>f</em>(<em>t</em>))</span> 。 相应地,
概率密度的归一化是泛函积分。</p>
<ol type="1">
<li><p>对于一个随机变量归一化的 Gauss 分布 <span
class="math inline"><em>N</em>(<em>μ</em>, <em>σ</em><sup>2</sup>)</span>,
其概率密度 (函数) 为 <span class="math display">$$
f(x)=\frac{1}{\sqrt{2 \pi \sigma^2}} \mathrm{e}^{-\frac{(z-\mu)^2}{2
\sigma^2}}
$$</span> 它满足 <span
class="math inline">∫<sub>−∞</sub><sup>∞</sup>d<em>x</em><em>f</em>(<em>x</em>) = 1</span>。</p></li>
<li><p>推广到 <span class="math inline"><em>n</em></span> 个随机变量
<span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ⋯, <em>x</em><sub><em>n</em></sub></span>
(令列阵 <span
class="math inline"><strong>x</strong> ≡ [<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ⋯, <em>x</em><sub><em>n</em></sub>]<sup>T</sup></span>
) 归一化的 Gauss 分布 <span
class="math inline"><em>N</em>(<strong>μ</strong>, <strong>S</strong>)</span>,
其概率密度 (函数) 为 <span class="math display">$$
f(\boldsymbol{x})=\frac{1}{(2 \pi|\boldsymbol{S}|)^{n / 2}}
\mathrm{e}^{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}}
S^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}
$$</span></p>
<p>其中 <span
class="math inline"><strong>μ</strong> ≡ [<em>μ</em><sub>1</sub>, <em>μ</em><sub>2</sub>, ⋯, <em>μ</em><sub><em>n</em></sub>]<sup>T</sup></span>
为所有平均值排成的列阵, 协方差阵 <span
class="math inline">$\boldsymbol{S} \equiv
\frac{1}{n-1}(\boldsymbol{x}-$</span> <span
class="math inline"><em>μ</em>)(<strong>x</strong> − <strong>μ</strong>)<sup>T</sup></span>。</p>
<p>在现在的情况 (见式 (13.2.1-4)) 下, 随机变量不是离散型的,
而是一个连续型的函数 <span
class="math inline"><em>f</em>(<em>t</em>)</span>,
相当于无穷多个互为独立的随机自变量. 与此对应, 对于 <span
class="math inline"><em>n</em></span> 个独立的随机自变量, 并且均值 <span
class="math inline"><strong>μ</strong> = <strong>0</strong></span> 和
<span
class="math inline"><strong>S</strong> = 2<em>Γ</em><em>k</em><sub>B</sub><em>T</em>1</span>
的情况, 式 (13.2.1-6) 应该推广、改写为 <span class="math display">$$
f(\boldsymbol{x})=\frac{1}{\left(4 \pi \Gamma k_{\mathrm{B}} T\right)^{n
/ 2}} \mathrm{e}^{-\frac{1}{4 \Gamma k_{\mathrm{B}} T} \sum_{i=1}^n
x_i^2} .
$$</span></p></li>
<li><p>再从离散型推广到连续型随机变量 <span
class="math inline"><em>x</em>(<em>t</em>)</span>
就得到泛函型的概率密度为 <span class="math display">$$
f[x(t)]=C \exp \left\{-\int_{t_0}^{t_f} \mathrm{~d} t \frac{x^2(t)}{4
\Gamma k_{\mathrm{B}} T}\right\},
$$</span></p>
<p>其中 <span class="math inline"><em>C</em></span> 为归一化常数, <span
class="math inline"><em>t</em><sub>0</sub>, <em>t</em><sub><em>f</em></sub></span>
分别为 <span class="math inline"><em>x</em>(<em>t</em>)</span>
的初始时间和结束时间。</p></li>
</ol>
<p><font color='red'>此处省略很多化简步骤</font></p>
<p>最终得到： <span class="math display">$$\begin{equation}
\frac{\partial}{\partial t} P(\boldsymbol{v}, t)=\zeta
\frac{\partial}{\partial \boldsymbol{v}}[P(\boldsymbol{v}, t)
\boldsymbol{v}]+\zeta \frac{k_{\mathrm{B}} T}{m}
\frac{\partial^2}{\partial v^2} P(\boldsymbol{v}, t)
\end{equation}$$</span> 称为Fokker-Planck方程。</p>
<h3
id="过阻尼langevin方程的smoluchowski方程">过阻尼Langevin方程的Smoluchowski方程</h3>
<h3
id="有外场langevin方程的fokker-plank方程">有外场Langevin方程的Fokker-Plank方程</h3>
<h2
id="自由brown运动的fokker-planck方程的严格解">自由Brown运动的Fokker-Planck方程的严格解</h2>
<h1 id="线性响应理论">线性响应理论</h1>
<p><strong>静态响应</strong>：外场不随时间变化，则体系会在该恒定外场之下达到新的平衡态，求这样响应的平衡统计力学问题。</p>
<p><strong>动态响应</strong>：求体系对外场的瞬时响应，或者周期性外场。</p>
<p>以下先讨论体系对静态外场的响应，然后讨论对含时外场响应的Green-Kubo线性响应理论。</p>
<h2 id="静态线性响应">静态线性响应</h2>
<p>讨论如何求平衡的线性响应。</p>
<p>设体系在未受扰动时的 Hamilton 量为 <span
class="math inline"><em>H</em><sub>0</sub>(<em>x</em>)</span>,其中 <span
class="math inline"><em>x</em> ≡ {<em>q</em>, <em>p</em>}, <em>q</em> ≡ {<em>q</em><sub>1</sub>, <em>q</em><sub>2</sub>, ⋯, <em>q</em><sub>3<em>N</em></sub>}, <em>p</em> ≡ {<em>p</em><sub>2</sub>, <em>p</em><sub>2</sub>, ⋯, <em>p</em><sub>3<em>N</em></sub>}</span>,
记恒定外场为 <span class="math inline"><em>E</em></span>.
体系受扰动后的能量增量, 即体系与外场耦合的能量, 设为 <span
class="math inline"><em>H</em><sup>′</sup> = −<em>A</em>(<em>x</em>)<em>E</em></span>,
其中 <span
class="math inline"><em>A</em>(<em>x</em>)</span>为体系状态的某个与
<span class="math inline"><em>E</em></span> 共轭的已知函数. “共轭”
是指若 <span class="math inline"><em>E</em></span> 是外电场的电场强度
<span class="math inline"><strong>E</strong></span>, 则 <span
class="math inline"><em>A</em></span> 就是体系总的电偶极矩 <span
class="math inline"><strong>μ</strong></span>. 与外磁场强度 <span
class="math inline"><strong>H</strong></span> 共轭的是体系的总磁矩 <span
class="math inline"><strong>M</strong></span>.对应的扰动能量为 <span
class="math inline"><em>H</em><sup>′</sup> = −<strong>μ</strong> ⋅ <strong>E</strong></span>
和 <span
class="math inline"><em>H</em><sup>′</sup> = −<strong>M</strong> ⋅ <strong>H</strong></span>.
实际上, 在以下讨论中, <span class="math inline"><em>E</em></span> 和
<span class="math inline"><em>A</em></span>的具体形式并不重要.
扰动时体系的 Hamilton 量为 <span
class="math display"><em>H</em>(<em>x</em>, <em>E</em>) = <em>H</em><sub>0</sub> + <em>H</em><sup>′</sup> = <em>H</em><sub>0</sub>(<em>x</em>) − <em>A</em>(<em>x</em>)<em>E</em></span></p>
<h3 id="经典力学中的静态线性响应">经典力学中的静态线性响应</h3>
<p>设未扰动的分布函数<span
class="math inline"><em>f</em>(<em>x</em>)</span>为： <span
class="math display">$$\begin{align}
f(x) = \frac{1}{Q}\mathbb{e}^{-\beta H_0(x)}
\end{align}$$</span> 其中<span
class="math inline"><em>Q</em> = ∫<em>d</em><em>x</em><em>e</em><sup>−<em>β</em><em>H</em><sub>0</sub>(<em>x</em>)</sup></span>为未扰动时体系的配分函数，<span
class="math inline"><em>x</em> ≡ (<em>q</em>, <em>p</em>)</span>表示<span
class="math inline"><em>Γ</em></span>相空间变量。未扰动时，若体系在该外场之下达到平衡，则扰动时的分布函数为：
<span class="math display">$$\begin{align}
f(x, E)=&amp;\frac{1}{Q(E)} \mathrm{e}^{-\beta H(x, E)}=\frac{1}{Q(E)}
\mathrm{e}^{-\beta\left[H_0(x)-A(x) E\right]} \\
Q(E)=&amp;\int \mathrm{d} x \mathrm{e}^{-\beta\left[H_0(x)-A(x)
E\right]}
\end{align}$$</span></p>
<p>通常做法，把扰动体系的物理量在未扰动体系处展开。将未受扰动的平衡态的系综<strong>均值</strong><span
class="math inline">⟨⋅⟩<sub><em>e</em><em>q</em></sub></span>。若外场是弱场，则可作近似展开：
<span class="math display">$$\begin{align}
\mathrm{e}^{-\beta\left[H_0(x)-A(x) E\right]}=[1+\beta
AE+O(E^2)]\mathbb{e}^{-\beta H_0}
\end{align}$$</span> 于是体系配分函数展开为： <span
class="math display">$$\begin{align}
Q(E)=&amp;\int \mathrm{d} x \mathrm{e}^{-\beta\left[H_0(x)-A(x)
E\right]} \\
=&amp; \int \mathrm{d} x [1+\beta AE+O(E^2)]\mathbb{e}^{-\beta H_0} \\
=&amp; Q[1+\beta\langle A\rangle_{eq}E+O(E^2)]
\end{align}$$</span></p>
<p>可以得到扰动时分布函数的展开为： <span
class="math display">$$\begin{equation}
f(x, E)=\frac{\left[1+\beta A E+O\left(E^2\right)\right]
\mathrm{e}^{-\beta H_0}}{\left[1+\beta\langle A\rangle_{\mathrm{eq}}
E+O\left(E^2\right)\right] Q}=\frac{1+\beta A
E+O\left(E^2\right)}{1+\beta\langle A\rangle_{\mathrm{eq}}
E+O\left(E^2\right)} f(x)
\end{equation}$$</span> 根据<span
class="math inline">|<em>x</em>| &lt; 1</span>时<span
class="math inline">$\frac{1}{1+x}$</span>的级数展开： <span
class="math display">$$\begin{align}
\frac{1+\beta A E+O\left(E^2\right)}{1+\beta \langle
A\rangle_{\mathrm{eq}} E+O\left(E^2\right)}= 1+\beta[A-\langle
A\rangle_{eq}]E+O(E^2)
\end{align}$$</span> 得到： <span class="math display">$$\begin{align}
f(x, E)=\left \{ 1+\beta[A-\langle A\rangle_{eq}]E\right\}f(x)+O(E^2)
\end{align}$$</span> 其中<span
class="math inline">⟨<em>A</em>⟩<sub><em>e</em><em>q</em></sub></span>为未扰动时平衡态<span
class="math inline"><em>A</em></span>的均值，不失普遍性的可令<span
class="math inline">⟨<em>A</em>⟩<sub><em>e</em><em>q</em></sub> = 0</span>，因为<span
class="math inline"><em>A</em></span>是扰动项的共轭物理量，在平衡没有扰动的情况下，可以认为该物理量是不存在的。</p>
<p>设<span
class="math inline"><em>B</em>(<em>x</em>)</span>为体系的任意力学量，扰动时其均值为：
<span class="math display">$$\begin{align}
\langle B(E) \rangle =&amp; \int dx f(x, E)B(x) \\
=&amp;\int dx f(x)\left [ 1+\beta A(x) E\right]B(x)+O(E^2) \\
=&amp; \langle B\rangle_{eq} +\beta\langle AB\rangle_{eq}E+O(E^2) \\
\langle \Delta B(E) \rangle \equiv&amp; \langle B(E) \rangle -\langle B
\rangle_{eq} \\
=&amp; \chi_{BA}E+O(E^2) \\
\chi_{BA}=&amp; \beta\langle AB\rangle_{eq}
\end{align}$$</span> 其中<span
class="math inline"><em>χ</em><sub><em>B</em><em>A</em></sub></span>称为广义极化率，第二个下标<span
class="math inline"><em>A</em></span>为外场<span
class="math inline"><em>E</em></span>共轭的那个体系的力学量，即外因<span
class="math inline"><em>E</em></span>是通过内因<span
class="math inline"><em>A</em></span>对体系的任意力学量<span
class="math inline"><em>B</em></span>起作用的，那个作用就是响应，也就是第一个下标<span
class="math inline"><em>B</em></span>。若取力学量<span
class="math inline"><em>B</em></span>为体系总的电偶极矩<span
class="math inline"><em>M</em></span>，则<span
class="math inline"><em>χ</em><sub><em>B</em><em>A</em></sub></span>就是电极化率。当高次项可以忽略的时候，可以得到线性响应为：
<span
class="math display">⟨<em>Δ</em><em>B</em>(<em>E</em>)⟩ = <em>χ</em><sub><em>B</em><em>A</em></sub><em>E</em></span>
上式表示在施加弱外场<span
class="math inline"><em>E</em></span>时，外场<span
class="math inline"><em>E</em></span>通过与体系种共轭力学量<span
class="math inline"><em>A</em></span>的相互作用，造成任意力学量<span
class="math inline"><em>B</em></span>产生响应的增量。广义极化率<span
class="math inline"><em>χ</em><sub><em>B</em><em>A</em></sub></span>表示单位外场<span
class="math inline"><em>E</em></span>通过<span
class="math inline"><em>A</em></span>造成力学量<span
class="math inline"><em>B</em></span>的响应增量。</p>
<h3 id="量子力学中的静态线性响应">量子力学中的静态线性响应</h3>
<h2 id="动态线性响应">动态线性响应</h2>
<h3 id="经典力学中的动态线性响应">经典力学中的动态线性响应</h3>
<h3 id="kubo变换">Kubo变换</h3>
<h3 id="复数方法响应的频率关系">复数方法、响应的频率关系</h3>
<h3 id="响应函数的客观属性">响应函数的客观属性</h3>
<h3 id="kramers-kronig关系式">Kramers-Kronig关系式</h3>
<h2 id="线性响应理论的应用">线性响应理论的应用</h2>
<h1 id="zwanzig-mori投影算符理论">Zwanzig-Mori投影算符理论</h1>
<p>近平衡体系时有两种方法： 1. 概率论方法
在适当假设的基础上求出概率密度服从的方程，如Fokker-Planck方程、主方程 2.
基于Langevin方程或广义Langevin方程
直接求目标随机变量的均值，如求Brown运动粒子其速度服从的方程。</p>
<p>以上两种方法在一定程度上需要借助唯象规律，即根据经验规律将随机变量假定成某种特定的形式。</p>
<p>许多人设法给Fokker-Planck方程、Langevin方程一个微观基础，重要进展是R.Zwanzig的投影算符方法。他个呢局第一性原理从Liouville方程出发，严格推导出广义Kokker-Planck方程。几年后，Hazime
Mori用投影算符方法得到广义Langevin方程。</p>
<p>Zwanzig-Mori理论统一了各种非平衡态的问题经验方程，提供了一普适性更强的方法，给线性响应理论提供严格的理论基础，给出计算相关函数的新方法。</p>
<h1 id="密度泛函理论">密度泛函理论</h1>
<p>多电子体系的基态能量是电子密度的唯一泛函，在真实的基态电子密度时，体系能量泛函必定达到极小；在介观层次液体理论中，经典的Helmholtz自由能、巨势也是体系数密度的唯一泛函。</p>
<h2 id="多电子体系的密度泛函理论">多电子体系的密度泛函理论</h2>
<p>Hohenberg-Kohn第一第二定理为电子密度泛函的基石，将基于波函数基础上的多电子体系量子理论经过统计粗粒化，改造成以电子数密度为基础的理论。</p>
<h3 id="hohenberg-kohn第一定理">Hohenberg-Kohn第一定理</h3>
<p><strong>Hohenberg-Kohn第一定理</strong>：多电子体系的基态是无论是非简并的或是简并的，则该基态体系中核与电子的相互作用势能<span
class="math inline"><em>v</em>(<em>r</em>)</span>唯一地取决于体系的电子密度<span
class="math inline"><em>ρ</em>(<em>r</em>)</span>（只差一个无关紧要的常数）。</p>
<h3 id="hohenberg-kohn第二定理">Hohenberg-Kohn第二定理</h3>
<p>Hohenberg-Kohn第二定理给出了电子密度泛函理论的能量变分原理。</p>
<p>用Rayleigh-Ritz变分原理导出量子力学的定态Schrodinger方程<span
class="math inline"><em>H</em>|<em>Ψ</em>⟩ = <em>E</em>|<em>Ψ</em>⟩</span>，体系能量的均值为<span
class="math inline">⟨<em>Ψ</em>|<em>H</em>|<em>Ψ</em>⟩</span>，同时体系波函数必须归一化<span
class="math inline">⟨<em>Ψ</em>|<em>Ψ</em>⟩ = 1</span>，把这个当成约束同时利用改变波函数求能量均值的最小值。利用Lagrange乘子法，变为一个无约束的变分问题：
<span
class="math display"><em>Ω</em> ≡ ⟨<em>Ψ</em>|<em>H</em>|<em>Ψ</em>⟩ − <em>ε</em>(⟨<em>Ψ</em> ∣ <em>Ψ</em>⟩ − 1)</span></p>
<p>其中 <span class="math inline"><em>ε</em></span> 为待定常数, 即所谓
Lagrange 待定乘子. 于是达到极值时必有 <span class="math display">$$
0=\frac{\partial
\Omega}{\partial|\Psi\rangle}=\frac{\partial}{\partial|\Psi\rangle}\{\langle\Psi|H|
\Psi\rangle-\varepsilon(\langle\Psi \mid
\Psi\rangle-1)\}=H|\Psi\rangle+\left[\left\langle H^{\dagger}
\Psi\right|\right]^{\dagger}-\varepsilon|\Psi\rangle-\varepsilon[\langle\Psi|]^{\dagger},
$$</span></p>
<p>于是得到 <span
class="math display"><em>H</em>|<em>Ψ</em>⟩ = <em>ε</em>|<em>Ψ</em>⟩</span></p>
<p>接下来考虑电子密度泛函理论的数密度<span
class="math inline"><em>ρ</em>(<em>r</em>)</span>的变分原理。将能量泛函的外场项记为<span
class="math inline"><em>E</em><sub><em>v</em></sub>[<em>ρ</em>]</span>。体系电子总能量应为三相之和：电子总动能<span
class="math inline"><em>T</em></span>，核与电子的相互作用<span
class="math inline"><em>V</em><sub><em>n</em><em>e</em></sub></span>，电子间相互作用<span
class="math inline"><em>V</em><sub><em>e</em><em>e</em></sub></span>：
<span
class="math display"><em>E</em><sub><em>v</em></sub>[<em>ρ</em>] = <em>T</em>[<em>ρ</em>] + <em>V</em><sub>ne</sub>[<em>ρ</em>] + <em>V</em><sub>ee</sub>[<em>ρ</em>] = <em>F</em><sub>HK</sub>[<em>ρ</em>] + ∫<em>v</em>(<strong>r</strong>)<em>ρ</em>(<strong>r</strong>)d<strong>r</strong></span></p>
<p>其中定义 <span class="math display">$$
\begin{gathered}
F_{\mathrm{HK}}[\rho] \equiv
T[\rho]+V_{\mathrm{ee}}[\rho]=\left\langle\Psi\left|T+V_{\mathrm{ee}}\right|
\Psi\right\rangle, \\
V_{\mathrm{ee}}[\rho] \equiv J[\rho]+(\text { not classical }) \simeq
J[\rho]+E_{\mathrm{xc}}[\rho],
\end{gathered}
$$</span></p>
<p><span
class="math inline"><em>J</em>[<em>ρ</em>]</span>为根据经典电动力学计算该体系的电子间作用势能（静电势能or
库伦势能）： <span class="math display">$$\begin{align}
J[\rho]=\frac{1}{2}\iint\frac{\rho(r_1)\rho(r_2)}{|r_1-r_2|}dr_1dr_2
\end{align}$$</span> 在电子间作用势能<span
class="math inline"><em>V</em><sub><em>e</em><em>e</em></sub>[<em>ρ</em>]</span>种扣除<span
class="math inline"><em>J</em>[<em>ρ</em>]</span>之后得到的非经典项的主要部分<span
class="math inline"><em>E</em><sub><em>x</em><em>c</em></sub>[<em>ρ</em>]</span>，称为<strong>交换相关能</strong>。</p>
<p>另一方面电子密度<span
class="math inline"><em>ρ</em>(<em>r</em>)</span>在全空间积分为<span
class="math inline"><em>N</em></span>： <span
class="math display">∫<em>d</em><em>r</em><em>ρ</em>(<em>r</em>) = <em>N</em></span></p>
<p><strong>Hohenberg-Kohn第二定理</strong>：设<span
class="math inline"><em>ρ̃</em>(<em>r</em>)</span>为“试探密度“，同时满足条件<span
class="math inline">0 ≤ <em>ρ̃</em>(<em>r</em>)</span>和<span
class="math inline">∫<em>d</em><em>r</em><em>ρ̃</em>(<em>r</em>) = <em>N</em></span>。将<span
class="math inline"><em>ρ̃</em>(<em>r</em>)</span>代入体系的能量表达式，有：
<span class="math display">$$\begin{align}
E_0\leq E_v[\tilde \rho]
\end{align}$$</span> 其中<span
class="math inline"><em>E</em><sub>0</sub></span>为基态能量。</p>
<p><span class="math inline"><em>v</em></span> 可表示性 ( <span
class="math inline"><em>v</em></span>-representable): 设多电子体系的
Hamilton 量为 <span class="math display">$$
H=\sum_{i=1}^N\left(-\frac{1}{2} \nabla_i^2\right)+\sum_{i=1}^N
v\left(\boldsymbol{r}_i\right)+\sum_{i&lt;j}^N \frac{1}{r_{i j}} .
$$</span></p>
<p>若电子密度 <span
class="math inline"><em>ρ</em>(<strong>r</strong>)</span> 具有与 <span
class="math inline"><em>H</em></span> 对应的反对称性基态波函数,
则该密度称为可表达外势 <span
class="math inline"><em>v</em>(<strong>r</strong>)</span> 的, 即所谓具有
<span class="math inline"><em>v</em></span> 可表示性.</p>
<p>由此, 可以把 Hohenberg-Kohn 第一定理重新表述如下:
多电子体系的基态波函数与 <span class="math inline"><em>v</em></span>
可表示的密度 <span
class="math inline"><em>ρ</em>(<strong>r</strong>)</span> 是一一对应的,
所以从这样的密度 <span
class="math inline"><em>ρ</em>(<strong>r</strong>)</span>
出发就可以得到该基态的一切性质.</p>
<p>Hohenberg-Kohn 第二定理可重新表述如下: 对于所有的 <span
class="math inline"><em>v</em></span> 可表示的密度 <span
class="math inline"><em>ρ</em>(<strong>r</strong>)</span>,存在 <span
class="math display"><em>E</em><sub><em>v</em></sub>[<em>ρ</em>] = <em>F</em><sub>HK</sub>[<em>ρ</em>] + ∫d<strong>r</strong><em>ρ</em>(<strong>r</strong>)<em>v</em>(<strong>r</strong>) ≥ <em>E</em><sub><em>v</em></sub>[<em>ρ</em><sub>0</sub>],</span></p>
<p>其中 <span
class="math inline"><em>E</em><sub><em>v</em></sub>[<em>ρ</em><sub>0</sub>]</span>
为含外场 <span class="math inline"><em>v</em>(<strong>r</strong>)</span>
的 Hamilton 量体系的基态能量, <span
class="math inline"><em>ρ</em><sub>0</sub></span> 为基态密度.</p>
<h3 id="levy约束搜索法">Levy约束搜索法</h3>
<p>根据Hohenberg-Kohn第一定理，基态的电子密度<span
class="math inline"><em>ρ</em><sub>0</sub>(<em>r</em>)</span>与基态波函数<span
class="math inline"><em>Ψ</em><sub>0</sub></span>之间是一一对应的，因此接下来需要知道如何从给定的<span
class="math inline"><em>ρ</em><sub>0</sub></span>中求出<span
class="math inline"><em>Ψ</em><sub>0</sub></span>。</p>
<h3
id="基于第一性原理的电负性绝对硬度fukui函数">基于第一性原理的电负性、绝对硬度、Fukui函数</h3>
<h3 id="最大硬度原理">最大硬度原理</h3>
<h2 id="介观体系的密度泛函理论">介观体系的密度泛函理论</h2>
]]></content>
      <categories>
        <category>Statistical Physics</category>
      </categories>
      <tags>
        <tag>Nonequilibrium Thermodynamics</tag>
        <tag>基本概念</tag>
        <tag>系综</tag>
        <tag>Clasical dynamics</tag>
      </tags>
  </entry>
</search>
