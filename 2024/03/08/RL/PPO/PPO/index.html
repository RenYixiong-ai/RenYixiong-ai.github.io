<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"renyixiong-ai.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":false,"back2top":{"enable":false,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":{}},"algolia":{"hits":5,"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":false,"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="æ–‡ä¸­æœ‰ä¸€äº›é—®é¢˜ä»æœªå¤„ç†ï¼Œç¼ºå¤±å…·ä½“ä»£ç çš„è§£è¯»ï¼Œå¯¹äºTRPOç®—æ³•çš„è®¤çŸ¥ä»ç„¶å­˜åœ¨ä¸æ¸…æ¥šçš„åœ°æ–¹ï¼Œé«˜é˜¶æ¢¯åº¦æ€ä¹ˆç®—çš„ Abstract è¿™æ˜¯ä¸€ç¯‡å…³äºç­–ç•¥æ¢¯åº¦ç®—æ³•çš„æ€»ç»“ã€‚é¦–å…ˆç»™å‡ºæ¢¯åº¦ç­–ç•¥ï¼Œä»‹ç»å…¶åŸºæœ¬å«ä¹‰ï¼Œä½†æ˜¯åˆå§‹æ–¹æ¡ˆå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œå¯ä»¥çŸ¥é“æ¢¯åº¦å˜åŒ–çš„æ–¹å‘ï¼Œä¸çŸ¥é“æ¢¯åº¦çš„æ­¥é•¿ã€‚ç„¶åï¼Œæå‡ºè‡ªç„¶æ¢¯åº¦ç®—æ³•ï¼Œé€šè¿‡åŠ å…¥çº¦æŸçš„æ–¹æ¡ˆè®¡ç®—å‡ºæ¢¯åº¦çš„æ­¥é•¿ã€‚æ¥ä¸‹æ¥ï¼ŒTrust Region Policy Optimizationï¼ˆTRPO">
<meta property="og:type" content="article">
<meta property="og:title" content="Policy Gradients In Reinforcement">
<meta property="og:url" content="https://renyixiong-ai.github.io/2024/03/08/RL/PPO/PPO/index.html">
<meta property="og:site_name" content="renyixiong blog">
<meta property="og:description" content="æ–‡ä¸­æœ‰ä¸€äº›é—®é¢˜ä»æœªå¤„ç†ï¼Œç¼ºå¤±å…·ä½“ä»£ç çš„è§£è¯»ï¼Œå¯¹äºTRPOç®—æ³•çš„è®¤çŸ¥ä»ç„¶å­˜åœ¨ä¸æ¸…æ¥šçš„åœ°æ–¹ï¼Œé«˜é˜¶æ¢¯åº¦æ€ä¹ˆç®—çš„ Abstract è¿™æ˜¯ä¸€ç¯‡å…³äºç­–ç•¥æ¢¯åº¦ç®—æ³•çš„æ€»ç»“ã€‚é¦–å…ˆç»™å‡ºæ¢¯åº¦ç­–ç•¥ï¼Œä»‹ç»å…¶åŸºæœ¬å«ä¹‰ï¼Œä½†æ˜¯åˆå§‹æ–¹æ¡ˆå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œå¯ä»¥çŸ¥é“æ¢¯åº¦å˜åŒ–çš„æ–¹å‘ï¼Œä¸çŸ¥é“æ¢¯åº¦çš„æ­¥é•¿ã€‚ç„¶åï¼Œæå‡ºè‡ªç„¶æ¢¯åº¦ç®—æ³•ï¼Œé€šè¿‡åŠ å…¥çº¦æŸçš„æ–¹æ¡ˆè®¡ç®—å‡ºæ¢¯åº¦çš„æ­¥é•¿ã€‚æ¥ä¸‹æ¥ï¼ŒTrust Region Policy Optimizationï¼ˆTRPO">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/03/08/RL/PPO/PPO/REINFORCE.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/03/08/RL/PPO/PPO/figure1.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/03/08/RL/PPO/PPO/figure2.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/03/08/RL/PPO/PPO/natural_gradients.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/03/08/RL/PPO/PPO/TRPO.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/03/08/RL/PPO/PPO/TRPO2.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/03/08/RL/PPO/PPO/CLIP.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/03/08/RL/PPO/PPO/PPO_Clip.png">
<meta property="article:published_time" content="2024-03-07T16:00:00.000Z">
<meta property="article:modified_time" content="2025-05-08T10:20:12.698Z">
<meta property="article:author" content="renyixiong">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="Natural Gradients">
<meta property="article:tag" content="PPO">
<meta property="article:tag" content="TRPO">
<meta property="article:tag" content="Policy Gradients">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://renyixiong-ai.github.io/2024/03/08/RL/PPO/PPO/REINFORCE.png">

<link rel="canonical" href="https://renyixiong-ai.github.io/2024/03/08/RL/PPO/PPO/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Policy Gradients In Reinforcement | renyixiong blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">renyixiong blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://renyixiong-ai.github.io/2024/03/08/RL/PPO/PPO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="renyixiong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="renyixiong blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Policy Gradients In Reinforcement
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2024-03-08 00:00:00" itemprop="dateCreated datePublished" datetime="2024-03-08T00:00:00+08:00">2024-03-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-05-08 18:20:12" itemprop="dateModified" datetime="2025-05-08T18:20:12+08:00">2025-05-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
              <span>17k</span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
              <span>15 åˆ†é’Ÿ</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><font color='red'>æ–‡ä¸­æœ‰ä¸€äº›é—®é¢˜ä»æœªå¤„ç†ï¼Œç¼ºå¤±å…·ä½“ä»£ç çš„è§£è¯»ï¼Œå¯¹äºTRPOç®—æ³•çš„è®¤çŸ¥ä»ç„¶å­˜åœ¨ä¸æ¸…æ¥šçš„åœ°æ–¹ï¼Œé«˜é˜¶æ¢¯åº¦æ€ä¹ˆç®—çš„</font></p>
<h1 id="abstract">Abstract</h1>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºç­–ç•¥æ¢¯åº¦ç®—æ³•çš„æ€»ç»“ã€‚é¦–å…ˆç»™å‡ºæ¢¯åº¦ç­–ç•¥ï¼Œä»‹ç»å…¶åŸºæœ¬å«ä¹‰ï¼Œä½†æ˜¯åˆå§‹æ–¹æ¡ˆå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œå¯ä»¥çŸ¥é“æ¢¯åº¦å˜åŒ–çš„æ–¹å‘ï¼Œä¸çŸ¥é“æ¢¯åº¦çš„æ­¥é•¿ã€‚ç„¶åï¼Œæå‡ºè‡ªç„¶æ¢¯åº¦ç®—æ³•ï¼Œé€šè¿‡åŠ å…¥çº¦æŸçš„æ–¹æ¡ˆè®¡ç®—å‡ºæ¢¯åº¦çš„æ­¥é•¿ã€‚æ¥ä¸‹æ¥ï¼ŒTrust
Region Policy
Optimizationï¼ˆTRPOï¼‰ç®—æ³•åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æå‡ºçº¦æŸï¼Œä½¿å¾—æ»¡è¶³è¯¥çº¦æŸæ¡ä»¶çš„æ ·æœ¬å¯ä»¥ç¨³å®šæå‡ç­–ç•¥æ€§èƒ½ã€‚æœ€åï¼Œè™½ç„¶TRPOååˆ†ä¼˜ç§€ï¼Œä½†æ˜¯å¤§é‡çš„è®¡ç®—ä½¿å…¶æ•ˆç‡ä¸é«˜ï¼Œå› æ­¤è¿›è¡Œç®€åŒ–æå‡ºProximal
Policy Optimizationï¼ˆPPOï¼‰ç®—æ³•ã€‚</p>
<p><strong>åŸºäºå€¼å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ </strong>ï¼šé€šè¿‡é€’å½’ï¼Œæ±‚è§£bellman
æ–¹ç¨‹ç»´æŠ¤Qå€¼ï¼ˆç¦»æ•£åˆ—è¡¨æˆ–è€…ç¥ç»ç½‘ç»œï¼‰ï¼Œæ¯æ¬¡é€‰æ‹©åŠ¨ä½œä¼šé€‰æ‹©è¯¥çŠ¶æ€ä¸‹å¯¹åº”Qå€¼æœ€å¤§çš„åŠ¨ä½œã€‚ä½¿å¾—æœŸæœ›å¥–åŠ±å€¼æœ€å¤§ã€‚</p>
<p><strong>åŸºäºç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ </strong>ï¼šä¸å†é€šè¿‡ä»·å€¼å‡½æ•°ç¡®å®šåŠ¨ä½œï¼Œè€Œæ˜¯ç›´æ¥å­¦ä¹ ç­–ç•¥æœ¬èº«ï¼Œé€šè¿‡ä¸€ç»„å‚æ•°<span
class="math inline"><em>Î¸</em></span>å¯¹ç­–ç•¥è¿›è¡Œå‚æ•°åŒ–ï¼Œå¹¶é€šè¿‡ç¥ç»ç½‘ç»œä¼˜åŒ–<span
class="math inline"><em>Î¸</em></span>ã€‚</p>
<span id="more"></span>
<p>Reference: * ååˆ†æ¨èçš„åšå®¢ï¼Œå…¶å»¶ä¼¸é˜…è¯»æœ‰å¾ˆå¤šå…³äºç­–ç•¥æ¢¯åº¦çš„èµ„æ–™<a
href="https://towardsdatascience.com/policy-gradients-in-reinforcement-learning-explained-ecec7df94245">Policy
Gradients In Reinforcement Learning Explained</a> *
åŸºäºç­–ç•¥å¼ºåŒ–å­¦ä¹ çš„å¼€ç¯‡é¼»ç¥–<a
href="https://link.springer.com/article/10.1007/BF00992696">Simple
Statistical Gradient-Following Algorithms for Connectionist
Reinforcement Learning</a> * è‡ªç„¶æ¢¯åº¦ç®—æ³•<a
href="https://ieeexplore.ieee.org/abstract/document/6790500">Natural
Gradient Works Efficiently in Learning</a> * å¯¹è‡ªç„¶æ¢¯åº¦ç®—æ³•å¾ˆå¥½çš„æ€»ç»“<a
href="https://arxiv.org/pdf/2209.01820.pdf">Natural Policy Gradients In
Reinforcement Learning Explained</a> * CMUæ·±åº¦å¼ºåŒ–å­¦ä¹ <a
href="https://www.andrew.cmu.edu/course/10-403/#readings">è¯¾ç¨‹ä¸»é¡µ</a><a
href="https://cmudeeprl.github.io/403_website/lectures/">GitHubåœ°å€</a>
* <a href="https://arxiv.org/abs/1502.05477">Trust Region Policy
Optimization</a> * <a href="https://arxiv.org/abs/1707.06347">Proximal
Policy Optimization Algorithms</a> * <a
href="https://sham.seas.harvard.edu/publications/approximately-optimal-approximate-reinforcement-learning">Approximately
Optimal Approximate Reinforcement Learning</a> * Berkeleyæ·±åº¦å¼ºåŒ–å­¦ä¹ <a
href="https://rail.eecs.berkeley.edu/deeprlcourse-fa17/">è¯¾ç¨‹ä¸»é¡µ</a> *
<a
href="https://www.telesens.co/2018/06/09/efficiently-computing-the-fisher-vector-product-in-trpo/">Efficiently
Computing the Fisher Vector Product in TRPO</a> * ä»£ç åº“<a
href="https://spinningup.openai.com/en/latest/index.html">Spinning Up in
Deep RL</a></p>
<h1
id="policy-approximation-methods-moving-to-stochastic-policies">Policy
approximation methods : Moving to stochastic policies</h1>
<p>åœ¨ç­–ç•¥è¿‘ä¼¼çš„æ–¹æ³•ä¸­ï¼Œå¿½ç•¥ä¼ ç»Ÿçš„ä»·å€¼å‡½æ•°ï¼Œç›´æ¥è°ƒæ•´ç­–ç•¥æœ¬èº«ã€‚é€šè¿‡<span
class="math inline"><em>Î¸</em></span>ï¼ˆå¯èƒ½æ˜¯ç¥ç»ç½‘ç»œå‚æ•°ï¼‰å‚æ•°åŒ–ç­–ç•¥<span
class="math inline"><em>Ï€</em><sub><em>Î¸</em></sub></span>ã€‚</p>
<p>éœ€è¦è§£å†³çš„é—®é¢˜ï¼š 1. å¦‚ä½•è¯„ä¼°ç­–ç•¥çš„è´¨é‡ 2. å¦‚ä½•æ›´æ–°<span
class="math inline"><em>Î¸</em></span></p>
<p>ç­–ç•¥æ¢¯åº¦ç®—æ³•æœ‰å¾ˆå¤šç§ï¼Œè¿™ç¯‡æ–‡ç« èšç„¦äºlikelihood ratio policy
gradientsã€‚è¿™ç§ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ç­–ç•¥è½¬åŒ–ä¸ºä¸€ç§æ¦‚ç‡åˆ†å¸ƒ<span
class="math inline"><em>Ï€</em><sub><em>Î¸</em></sub>(<em>a</em>|<em>s</em>)â€„=â€„<em>P</em>(<em>a</em>|<em>s</em>;â€†<em>Î¸</em>)</span>ï¼Œä»è€Œè¿”å›çš„ä¸æ˜¯ä¸€ä¸ªå•ä¸€çš„ç»“æœï¼Œè€Œæ˜¯åŠ¨ä½œçš„åˆ†å¸ƒæ¦‚ç‡ï¼Œç„¶åè¿›è¡Œé‡‡æ ·ã€‚</p>
<h2 id="establishing-the-objective-function">Establishing the objective
function</h2>
<p>åœ¨è¿›è¡Œä¸€ç³»åˆ—å†³ç­–ä¹‹åï¼Œå¾—åˆ°çŠ¶æ€-åŠ¨ä½œè½¨è¿¹<span
class="math inline"><em>Ï„</em>â€„=â€„(<em>s</em><sub>1</sub>,â€†<em>a</em><sub>1</sub>â‹¯<em>s</em><sub><em>T</em></sub>,â€†<em>a</em><sub><em>T</em></sub>)</span>ï¼Œæ¯ä¸€æ¡è½¨è¿¹æœ‰ç›¸åº”çš„æ¦‚ç‡<span
class="math inline"><em>P</em>(<em>Ï„</em>)</span>å’Œç§¯ç´¯å›æŠ¥<span
class="math inline"><em>R</em>(<em>Ï„</em>)â€„=â€„âˆ‘<em>Î³</em><sup><em>t</em></sup><em>R</em><sub><em>t</em></sub></span>ï¼ˆ<span
class="math inline"><em>Î³</em></span>æ˜¯æŠ˜æ‰£ç‡ï¼Œ<span
class="math inline"><em>R</em><sub><em>t</em></sub></span>æ˜¯<span
class="math inline"><em>t</em></span>æ—¶åˆ»å›æŠ¥ï¼‰ï¼ŒåŒæ—¶å®šä¹‰ç›®æ ‡å‡½æ•°ï¼š</p>
<p><span class="math display">$$\begin{align}
J(\theta)&amp;=\mathbb{E}_{\tau \sim\pi_{\theta}}R(\tau)=\sum_\tau
P(\tau;\theta)R(\tau) \\
\max_{\theta}J(\theta)&amp;=\max_{\theta}E_{\tau
\sim\pi_{\theta}}R(\tau)=\max_{\theta}\sum_\tau P(\tau;\theta)R(\tau)
\end{align}$$</span></p>
<h2 id="defining-trajectory-probabilities">Defining trajectory
probabilities</h2>
<p>æ¥ä¸‹æ¥çš„ä¸»è¦ä»»åŠ¡æ˜¯å¦‚ä½•è®¡ç®—<span
class="math inline"><em>P</em>(<em>Ï„</em>;â€†<em>Î¸</em>)</span>ã€‚</p>
<p>éœ€è¦å¤„ç†ä¸¤ç±»æ¦‚ç‡ï¼š * <strong>ç­–ç•¥æ¦‚ç‡åˆ†å¸ƒ</strong>ï¼š<span
class="math inline"><em>Ï€</em><sub><em>Î¸</em></sub>(<em>a</em>|<em>s</em>)â€„=â€„<em>P</em>(<em>a</em>|<em>s</em>;â€†<em>Î¸</em>)</span>ï¼Œæè¿°åœ¨ç»™å®šçŠ¶æ€ä¸å‚æ•°ä¸‹ï¼ŒåŠ¨ä½œçš„æ¦‚ç‡ã€‚
* <strong>æ¦‚ç‡è½¬ç§»åˆ†å¸ƒ</strong>ï¼š<span
class="math inline"><em>P</em>(<em>s</em><sub><em>t</em>â€…+â€…1</sub>|<em>s</em><sub><em>t</em></sub>,â€†<em>a</em><sub><em>t</em></sub>)</span>ã€‚åœ¨ç›¸åŒçš„çŠ¶æ€ä¸‹ï¼Œä½œå‡ºç›¸åŒçš„åŠ¨ä½œï¼Œç¯å¢ƒä¹Ÿä¼šä»¥æ¦‚ç‡è¿”å›ä¸åŒçš„çŠ¶æ€ã€‚è¯¥å‚æ•°æè¿°åœ¨ç›¸åŒç¯å¢ƒä¸­ï¼ŒåŒä¸€åŠ¨ä½œï¼Œä¸‹ä¸€çŠ¶æ€åˆ†å¸ƒçš„å‡ ç‡ã€‚</p>
<p>è½¨è¿¹<span class="math inline"><em>Ï„</em></span>åœ¨ç­–ç•¥<span
class="math inline"><em>Ï€</em><sub><em>Î¸</em></sub>(<em>a</em>|<em>s</em>)</span>ä¸‹å‘ç”Ÿçš„æ¦‚ç‡å®šä¹‰ä¸ºï¼š
<span class="math display">$$\begin{align}
P(\tau;\theta)=\left[\prod_{t=0}^T P(s_{t+1}|s_t,a_t)\cdot
\pi_{\theta}(a_t|s_t) \right]
\end{align}$$</span></p>
<h2 id="deriving-the-policy-gradient">Deriving the policy gradient</h2>
<p>ä¸ºäº†å¾—åˆ°<span
class="math inline">max<sub><em>Î¸</em></sub><em>J</em>(<em>Î¸</em>)</span>ï¼Œå¯ä»¥åˆ©ç”¨æ±‚æå€¼çš„æ–¹æ³•ï¼ˆä¸€é˜¶å¯¼æ•°ä¸ºé›¶ï¼‰ï¼Œæ–¹æ³•é‡‡ç”¨ç‰›é¡¿æ¢¯åº¦è¿­ä»£æ³•ã€‚</p>
<p>ä¸ºäº†ä¼˜åŒ–<span
class="math inline"><em>Î¸</em></span>ï¼Œè®¡ç®—ç›®æ ‡å‚æ•°<span
class="math inline"><em>J</em>(<em>Î¸</em>)</span>å¯¹<span
class="math inline"><em>Î¸</em></span>çš„å¯¼æ•°ã€‚</p>
<p><span class="math display">$$\begin{align}
\nabla_{\theta}J(\theta) &amp;= \nabla_{\theta} \mathbb{E}_{\tau
\sim\pi_{\theta}}R(\tau)\\
&amp;= \sum_\tau \nabla_{\theta} P(\tau;\theta)R(\tau) \\
&amp;= \sum_\tau P(\tau;\theta)\frac{\nabla_{\theta}
P(\tau;\theta)}{P(\tau;\theta)}R(\tau) \\
&amp;= \sum_\tau P(\tau;\theta) \nabla_{\theta} \ln P(\tau;\theta)
R(\tau) \\
&amp;= \mathbb{E}_{\tau \sim\pi_{\theta}} R(\tau)  \nabla_{\theta} \ln
P(\tau;\theta) \\
&amp;= \mathbb{E}_{\tau \sim\pi_{\theta}} R(\tau)
\nabla_{\theta}\ln\left[\prod_{t=0}^T P(s_{t+1}|s_t,a_t)\cdot
\pi_{\theta}(a_t|s_t) \right] \\
&amp;= \mathbb{E}_{\tau \sim\pi_{\theta}} R(\tau)
\left[\nabla_{\theta}\sum_{t=0}^T \ln P(s_{t+1}|s_t,a_t)+
\nabla_{\theta}\sum_{t=0}^T\ln\pi_{\theta}(a_t|s_t) \right] \\
&amp;= \mathbb{E}_{\tau \sim\pi_{\theta}} R(\tau)
\nabla_{\theta}\sum_{t=0}^T\ln\pi_{\theta}(a_t|s_t) \\
\end{align}$$</span></p>
<p>ç›´æ¥è®¡ç®—å­˜åœ¨å›°éš¾ï¼Œéœ€è¦è¿‘ä¼¼å¤„ç†ï¼š <span
class="math display">$$\begin{align}
\nabla_{\theta}J(\theta) &amp;= \mathbb{E}_{\tau \sim\pi_{\theta}}
R(\tau)  \nabla_{\theta} \ln P(\tau;\theta) \\
&amp;\approx \frac{1}{m}\sum_{i=0}^m R(\tau^i)  \nabla_{\theta} \ln
P(\tau^i;\theta) \\
&amp;= \frac{1}{m}\sum_{i=0}^m R(\tau^i) \sum_{t^i=0}^{T^i}
\nabla_{\theta} \ln \pi_{\theta}(a_{t^i}|s_{t^i})\\
&amp;\approx \frac{1}{n} \sum_{i=1}^n R(t^i) \nabla_{\theta} \ln
\pi_{\theta}(a_{t^i}|s_{t^i})
\end{align}$$</span></p>
<p>ç°åœ¨æ¢¯åº¦å®Œå…¨å¯ä»¥è®¡ç®—ï¼Œåªéœ€è¦ç»™å‡ºç­–ç•¥<span
class="math inline"><em>Ï€</em><sub><em>Î¸</em></sub></span>çš„å®šä¹‰ï¼Œå°±å¯ä»¥è®¡ç®—å‡º<span
class="math inline">âˆ‡<sub><em>Î¸</em></sub><em>J</em>(<em>Î¸</em>)</span>ï¼Œä»è€Œç”¨ç­–ç•¥æ¢¯åº¦æ›´æ–°è§„åˆ™ï¼š
<span class="math display">$$\begin{align}
\theta \leftarrow \theta + \alpha \nabla_{\theta}J(\theta)
\end{align}$$</span></p>
<h2 id="examples-softmax-and-gaussian-policies">Examples: Softmax and
Gaussian policies</h2>
<p>ä¸ºäº†è¯´æ˜ä»¥ä¸Šç­–ç•¥çš„å¯è¡Œæ€§ï¼Œä¸‹é¢ç»™å‡ºç¦»æ•£ç©ºé—´ä¸è¿ç»­ç©ºé—´çš„ä¸¤ä¸ªä¾‹å­ã€‚å…¶ä¸­<span
class="math inline"><em>Ï•</em>(<em>s</em>,â€†<em>a</em>)</span>ä¸€ä¸ªåŒ…å«åŸºæœ¬ä¿¡æ¯çš„å‘é‡ï¼ŒåŒ…å«å½“å‰çŠ¶æ€çš„ä¿¡æ¯ä¸åŠ¨ä½œä¿¡æ¯ï¼Œ<span
class="math inline"><em>Î¸</em></span>ä¸ºæƒé‡å› å­ã€‚å‡è®¾ä¸€ä¸ªæœ€ç®€å•çš„ç½‘ç»œï¼š<span
class="math inline"><em>Ï•</em>(<em>s</em>,â€†<em>a</em>)<sup><em>T</em></sup>â€…â‹…â€…<em>Î¸</em></span>ï¼Œä¹˜ç§¯ç»“æœå°±æ˜¯å¯¹å½“å‰çŠ¶æ€ä¸åŠ¨ä½œçš„è¯„ä¼°ã€‚</p>
<p>åŸºäºä»¥ä¸Šå‡è®¾ï¼Œä¸‹é¢ä¸¤ç§å¸¸è§ç­–ç•¥ï¼š * Softmaxç­–ç•¥
å¯¹äºç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Œå¤šä½¿ç”¨Softmaxç­–ç•¥ã€‚å®šä¹‰å¦‚ä¸‹ï¼š <span
class="math display">$$\begin{align}
  \pi_{\theta}(a|s) &amp;= \frac{e^{\phi(s,a)^T \cdot
\theta}}{\sum_{a'\in A}e^{\phi(s,a)^T \cdot \theta}}
  \end{align}$$</span></p>
<p>å¯¹åº”ç­–ç•¥çš„æ¢¯åº¦ä¸ºï¼š <span class="math display">$$\begin{align}
  \nabla_\theta \ln \pi_{\theta}(a|s) = \phi(s,a)- \sum_{a'\in
A}\pi_\theta(a|s)\phi(s,a')
  \end{align}$$</span></p>
<ul>
<li>é«˜æ–¯ç­–ç•¥ å¯¹äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œç»å¸¸ä½¿ç”¨é«˜æ–¯ç­–ç•¥ã€‚å®šä¹‰å¦‚ä¸‹ï¼š <span
class="math display">$$\begin{align}
\pi_{\theta}(a|s) &amp;=
\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(a-\mu_\theta)^2}{2\sigma^2}}
\end{align}$$</span> å…¶ä¸­<span
class="math inline"><em>Î¼</em><sub><em>Î¸</em></sub></span>æ˜¯æ­£æ€åˆ†å¸ƒçš„å‡å€¼ï¼Œ<span
class="math inline"><em>Ïƒ</em><sub><em>Î¸</em></sub></span>æ˜¯æ ‡å‡†å·®ï¼ˆè¿™é‡Œå‡è®¾ä¸ºä¸€ä¸ªä¸ä¾èµ–äº<span
class="math inline"><em>Î¸</em></span>çš„è¶…å‚ï¼‰ï¼Œ<font color='red'>å®è·µä¸­å‡å€¼å’Œæ–¹å·®ï¼ˆä¸€èˆ¬ç”Ÿæˆçš„æ˜¯å¯¹æ•°æ–¹å·®ï¼‰å‡æ˜¯ç”±ç¥ç»ç½‘ç»œç”Ÿæˆï¼Œå…·ä½“è®ºè¿°å‚è§VAEç›¸å…³å†…å®¹</font>ã€‚å¯¹åº”çš„ç­–ç•¥æ¢¯åº¦ä¸ºï¼š
<span class="math display">$$\begin{align}
\nabla_\theta \ln\pi_{\theta}(a|s) =
\frac{(a-\mu_\theta)\phi(s)}{\sigma^2}
\end{align}$$</span></li>
</ul>
<h2 id="loss-functions-and-algorithmic-implementation-reinforce">Loss
functions and Algorithmic implementation (REINFORCE)</h2>
<p>åœ¨å®é™…çš„è®¡ç®—ä¸­ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œåªéœ€è¦è®¾ç½®æŸå¤±å‡½æ•°ï¼Œè®¡ç®—æœºè‡ªåŠ¨æ±‚å¯¼å°±è¡Œï¼ˆ<span
class="math inline"><em>r</em></span>æ˜¯rewardï¼‰ï¼š <span
class="math display">$$\begin{align}
\cal L(a,s,r) = -\ln(\pi_\theta(a|s))r
\end{align}$$</span></p>
<figure>
<img src="./REINFORCE.png" alt="REINFORCE" />
<figcaption aria-hidden="true">REINFORCE</figcaption>
</figure>
<h1 id="natural-gradients">Natural Gradients</h1>
<p>å°½ç®¡è‡ªç„¶æ¢¯åº¦å·²è¢«TRPOå’ŒPPOç­‰ç®—æ³•è¶…è¶Šï¼Œä½†æŒæ¡å®ƒçš„åŸºæœ¬åŸç†å¯¹äºç†è§£è¿™äº›å½“ä»£RLç®—æ³•è‡³å…³é‡è¦ã€‚</p>
<h2 id="the-problems-with-first-order-policy-gradients">The problems
with first-order policy gradients</h2>
<p>ä¼ ç»Ÿç­–ç•¥æ¢¯åº¦ç®—æ³•åªæ˜¯æä¾›äº†å‚æ•°çš„æ›´æ–°æ–¹å‘ï¼Œæ²¡æœ‰ç›´æ¥è¯´æ˜æ›´æ–°çš„æ­¥é•¿ã€‚ä¸‹é¢æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„ç­–ç•¥æ¢¯åº¦æ›´æ–°æ–¹ç¨‹ï¼š
<span class="math display">$$\begin{align}
\theta \leftarrow \theta + \alpha \nabla_{\theta}J(\theta)
\end{align}$$</span> ä¼ ç»Ÿæ–¹æ³•åŸºäºç›®æ ‡å‡½æ•°æ¢¯åº¦<span
class="math inline">âˆ‡<sub><em>Î¸</em></sub><em>J</em>(<em>Î¸</em>)</span>ä¸æ­¥é•¿å› å­<span
class="math inline"><em>Î±</em></span>ã€‚ä¼šå¯¼è‡´ä»¥ä¸‹ä¸¤ä¸ªå¸¸è§çš„é—®é¢˜ï¼š</p>
<ul>
<li>Overshooting:æ›´æ–°ç›´æ¥é”™è¿‡ç›®æ ‡ã€‚è™½ç„¶åœ¨æœ‰ç›‘ç£å­¦ä¹ ä¸­ä¸æˆé—®é¢˜ï¼Œå¯ä»¥é€šè¿‡é€æ­¥ä¿®æ”¹æ›´æ–°ç‡æ¥è¿‘ç›®æ ‡å€¼ã€‚ä½†æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå¯èƒ½ä¼šå› ä¸ºæ–°çš„å€¼å¯¼è‡´3æ¢¯åº¦æ¶ˆå¤±ã€‚
<img src="./figure1.png" alt="overshoot" /></li>
<li>Undershooting:æ­¥é•¿å› å­<span
class="math inline"><em>Î±</em></span>è¿‡å°ï¼Œæ— æ³•æ”¶æ•›åˆ°ç›®æ ‡ä½ç½®ã€‚</li>
</ul>
<p>ä½†æ˜¯ï¼Œå¹¶ä¸èƒ½ç®€å•çš„é™åˆ¶æ›´æ–°æ­¥é•¿<span
class="math inline">||<em>Î”</em><em>Î¸</em>||</span>ï¼ˆEuclidian
distanceï¼‰ï¼Œä¾‹å¦‚ï¼š <span class="math display">$$\begin{align}
\Delta \theta^* = \arg\max_{||\Delta\theta||&lt;\epsilon}J(\theta+\Delta
\theta)
\end{align}$$</span> å› ä¸ºåœ¨ä¸åŒçš„å‚æ•°ä¸­ï¼Œ<span
class="math inline"><em>Î¸</em></span>å¯¹äºæ­¥é•¿çš„æ•æ„Ÿæ€§ä¸åŒã€‚</p>
<figure>
<img src="./figure2.png" alt="fig2" />
<figcaption aria-hidden="true">fig2</figcaption>
</figure>
<h2 id="capping-the-difference-between-policies">Capping the difference
between policies</h2>
<p>å› ä¸ºå‚æ•°å¯¹äºæ­¥é•¿çš„æ•æ„Ÿæ€§ä¸åŒï¼Œå› æ­¤é‡‡ç”¨KLæ•£åº¦è¡¡é‡å‚æ•°å˜åŒ–å‰åå¯¹åˆ†å¸ƒçš„å½±å“ï¼Œå°†å‚æ•°<span
class="math inline"><em>Î¸</em></span>çš„å˜åŒ–é™åˆ¶åœ¨ä¸€å®šçš„èŒƒå›´å†…ã€‚ <span
class="math display">$$\begin{align}
D_{KL}(\pi_\theta||\pi_{\theta+\Delta \theta}) = \sum_{x\in \mathcal
x}\pi_\theta(x)\ln
\left(\frac{\pi_\theta(x)}{\pi_{\theta+\Delta\theta}(x)}\right)
\end{align}$$</span></p>
<p>è°ƒæ•´åçš„æ›´æ–°ç­–ç•¥é™åˆ¶ä¸ºï¼š <span class="math display">$$\begin{align}
\Delta \theta^* = \arg\max_{D_{KL}(\pi_\theta||\pi_{\theta+\Delta
\theta})&lt;\epsilon}J(\theta+\Delta \theta)
\end{align}$$</span></p>
<p>è¿™æ ·å¤„ç†ä¹‹åï¼Œåœ¨å‚æ•°ç©ºé—´è¿›è¡Œæ›´æ–°ï¼ŒåŒæ—¶ä¹Ÿèƒ½ä¿è¯ç­–ç•¥æœ¬èº«å˜åŒ–ä¸ä¼šååˆ†å‰§çƒˆã€‚ä½†æ˜¯é‡åˆ°ä¸€ä¸ªé—®é¢˜ï¼Œåœ¨è®¡ç®—KLæ•£åº¦çš„æ—¶å€™éœ€è¦å¯¹æ‰€æœ‰åŠ¨ä½œç©ºé—´è¿›è¡Œè¿ç®—ï¼Œè¿™ä¼šå¯¹è®¡ç®—å¸¦æ¥é—®é¢˜ï¼Œä¸‹é¢æ˜¯ç®€åŒ–æ–¹æ³•ã€‚</p>
<p>ä½¿ç”¨Lagrangianæ–¹æ³•ï¼Œå°†çº¦æŸé¡¹å˜æˆæƒ©ç½šé¡¹ï¼š <span
class="math display">$$\begin{align}
\Delta \theta^* = \arg\max_{\Delta\theta}J(\theta+\Delta
\theta)-\lambda(D_{KL}(\pi_\theta||\pi_{\theta+\Delta \theta})-\epsilon)
\end{align}$$</span></p>
<p>è¿›è¡ŒTaylorå±•å¼€ï¼Œä¸ºäº†è®°å·ç»Ÿä¸€ï¼Œä»¥ä¸‹å°†<span
class="math inline"><em>Î¸</em>â€„=â€„<em>Î¸</em><sub><em>o</em><em>l</em><em>d</em></sub>â€…+â€…<em>Î”</em><em>Î¸</em></span>ï¼š
<span class="math display">$$\begin{align}
\Delta \theta^* &amp;\approx \arg\max_{\Delta\theta}\left[
J(\theta_{old})+ \nabla_\theta J(\theta)|_{\theta=\theta_{old}}\cdot
\Delta\theta-\frac{1}{2}\lambda(\Delta\theta^T \nabla_{\theta}^2
D_{KL}(\pi_{\theta_{old}}||\pi_{\theta}|_{\theta=\theta_{old}
})\Delta\theta+\lambda \epsilon\right]\\
\end{align}$$</span></p>
<p>è¿™é‡Œåªè®¡ç®—<span
class="math inline"><em>D</em><sub><em>K</em><em>L</em></sub></span>çš„äºŒé˜¶é¡¹ï¼Œå› ä¸ºå…¶é›¶é˜¶ä¸ä¸€é˜¶é¡¹å‡ä¸ºé›¶ï¼Œè¯æ˜å¦‚ä¸‹ï¼š
<span class="math display">$$\begin{align}
D_{KL}(p_{\theta_{old}}||p_{\theta}ï¼‰&amp;\approx
D_{KL}(p_{\theta_{old}}||p_{\theta_{old}})+\Delta\theta^T\nabla_{\theta}D_{KL}(p_{\theta_{old}}|p_{\theta})+\frac{1}{2}\Delta\theta^T
\nabla_{\theta}^2
D_{KL}(\pi_{\theta_{old}}||\pi_{\theta}|_{\theta=\theta_{old}
})\Delta\theta \\
\nabla_{\theta}D_{KL}(p_{\theta_{old}}|p_{\theta})|_{\theta=\theta_{old}}
&amp; = -\nabla_{\theta}\mathbb{E}_{x\sim p_{\theta_{old}}}\ln
p_{\theta}(x)|_{\theta=\theta_{old}}+\nabla_{\theta}\mathbb{E}_{x\sim
p_{\theta_{old}}}\ln p_{\theta_{old}}(x)|_{\theta=\theta_{old}} \\
&amp;= -\mathbb{E}_{x\sim p_{\theta_{old}}}\nabla_{\theta}\ln
p_{\theta}(x)|_{\theta=\theta_{old}} \\
&amp;= -\mathbb{E}_{x\sim
p_{\theta_{old}}}\frac{1}{p_{\theta_{old}}}\nabla_{\theta}
p_{\theta}(x)|_{\theta=\theta_{old}} \\
&amp;= -\int_x p_{\theta_{old}}\frac{1}{p_{\theta_{old}}}\nabla_{\theta}
p_{\theta}(x)|_{\theta=\theta_{old}} \\
&amp;= -\int_x \nabla_{\theta} p_{\theta}(x)|_{\theta=\theta_{old}} \\
&amp;= -\nabla_{\theta} \int_x p_{\theta}(x)|_{\theta=\theta_{old}} \\
&amp;= 0 \\
\nabla_{\theta}^2
D_{KL}(\pi_{\theta_{old}}||\pi_{\theta}|_{\theta=\theta_{old}
})|_{\theta=\theta_{old}} &amp;= -\mathbb{E}_{x\sim
p_{\theta_{old}}}\nabla_{\theta}^2\ln
p_{\theta}(x)|_{\theta=\theta_{old}}  \\
&amp;= -\mathbb{E}_{x\sim
p_{\theta_{old}}}\nabla_{\theta}\left(\frac{\nabla_{\theta}
p_{\theta}(x)}{p_{\theta}(x)}\right)|_{\theta=\theta_{old}}  \\
&amp;= -\mathbb{E}_{x\sim p_{\theta_{old}}}\nabla_{\theta}\ln
p_{\theta}\nabla_{\theta}\ln p_{\theta}^T|_{\theta=\theta_{old}}  \\
\end{align}$$</span></p>
<p><font color='red'>äºŒé˜¶å¯¼æ•°å¯ä»¥è¡¨è¿°ä¸ºHessian matrixï¼Œç­‰ä»·äºFisher
information matrixï¼ˆè¿™ä¸¤ä¸ªçŸ©é˜µæœ‰ä»€ä¹ˆæ ·çš„å«ä¹‰ï¼Ÿï¼‰ã€‚</font></p>
<p><span class="math display">$$\begin{align}
F(\theta) &amp;= \mathbb{E}_{\theta}\nabla_{\theta}\ln
p_{\theta}\nabla_{\theta}\ln p_{\theta}^T\\
F(\theta_{old}) &amp;=\nabla^2_\theta
D_{KL}(p_{\theta_{old}}||p_{\theta})|_{\theta=\theta_{old}} \\
D_{KL}(p_{\theta_{old}}||p_{\theta}ï¼‰&amp;\approx
\frac{1}{2}\Delta\theta^T F(\theta_{old})\Delta\theta \\
&amp;= \frac{1}{2}(\theta-\theta_{old})^T
F(\theta_{old})(\theta-\theta_{old}) \\
\end{align}$$</span></p>
<p>æ ¹æ®ä»¥ä¸Šè¯æ˜å¯çŸ¥ï¼š <span class="math display">$$\begin{align}
\Delta \theta^* &amp;\approx \arg\max_{\Delta\theta}\left[
J(\theta_{old})+ \nabla_\theta J(\theta)|_{\theta=\theta_{old}}\cdot
\Delta\theta-\frac{1}{2}\lambda(\Delta\theta^T \nabla_{\theta}^2
D_{KL}(\pi_{\theta_{old}}||\pi_{\theta}|_{\theta=\theta_{old}
})\Delta\theta+\lambda \epsilon \right ]\\
&amp;= \arg\max_{\Delta\theta} \left[ \nabla_\theta
J(\theta)|_{\theta=\theta_{old}}\cdot
\Delta\theta-\frac{1}{2}\lambda\Delta\theta^T
F(\theta_{old})\Delta\theta\right]\\
\end{align}$$</span></p>
<p>è®¡ç®—æ¢¯åº¦ä¸ºé›¶çš„ç‚¹ï¼š <span class="math display">$$\begin{align}
0 &amp;= \frac{\partial}{\partial \Delta \theta} \left[ \nabla_\theta
J(\theta)|_{\theta=\theta_{old}}\cdot
\Delta\theta-\frac{1}{2}\lambda\Delta\theta^T
F(\theta_{old})\Delta\theta\right] \\
&amp;= \nabla_\theta J(\theta)|_{\theta=\theta_{old}}-\frac{1}{2}\lambda
F(\theta_{old})\Delta\theta \\
\Delta\theta &amp;= \frac{2}{\lambda}F^{-1}(\theta_{old})\nabla_\theta
J(\theta)|_{\theta=\theta_{old}}
\end{align}$$</span></p>
<p>å…¶ä¸­<span
class="math inline">$\frac{1}{\lambda}$</span>æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œå¯ä»¥æ”¶ç¼©è¿›å­¦ä¹ ç‡<span
class="math inline"><em>Î±</em></span>ã€‚å¹¶ä¸”æ ¹æ®å¯¹æ›´æ–°æ­¥é•¿çš„é™åˆ¶å…³ç³»ï¼Œå¯ä»¥å¾—åˆ°å­¦ä¹ ç‡è¡¨è¾¾å¼ï¼š
<span class="math display">$$\begin{align}
D_{KL}(p_{\theta_{old}}||p_{\theta}ï¼‰&amp;\approx
\frac{1}{2}(\Delta\theta)^T F(\theta_{old})(\Delta \theta) \\
&amp;= \frac{1}{2}(\alpha g_N)^T F(\theta_{old})(\alpha g_N)
&lt;\epsilon \\
\alpha &amp;=\sqrt{\frac{2\epsilon}{(g_N^TFg_N)}} \\
\end{align}$$</span> å…¶ä¸­<span
class="math inline"><em>g</em><sub><em>N</em></sub>â€„=â€„<em>F</em><sup>âˆ’1</sup>(<em>Î¸</em>)âˆ‡<sub><em>Î¸</em></sub><em>J</em>(<em>Î¸</em>)</span>ã€‚è‡ªç„¶æ¢¯åº¦ä¸æ›´æ–°æƒé‡å¯å†™ä¸ºï¼š
<span class="math display">$$\begin{align}
\tilde\nabla J(\theta) &amp;= F^{-1}(\theta)\nabla_\theta J(\theta) \\
\Delta \theta &amp;= \alpha \tilde\nabla J(\theta) \\
\theta &amp;= \theta_{old}+\alpha \tilde\nabla J(\theta)
\end{align}$$</span></p>
<p>è¯¥æ–¹æ¡ˆçš„æ ¸å¿ƒæ€æƒ³åœ¨äºé€šè¿‡å¼•å…¥KLæ•£åº¦ï¼Œå¯¹ä¸åŒå‚æ•°çš„æ­¥é•¿è¿›è¡Œé™åˆ¶ï¼Œç¼“è§£äº†Overshootä¸Undershooté—®é¢˜ã€‚</p>
<h2 id="algorithm">Algorithm</h2>
<figure>
<img src="./natural_gradients.png" alt="Algorithm" />
<figcaption aria-hidden="true">Algorithm</figcaption>
</figure>
<p>è‡ªç„¶æ¢¯åº¦æ–¹æ³•åœ¨ä¸¤ä¸ªæ–¹é¢ä¸åŒäºä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼š *
è€ƒè™‘åˆ°ç­–ç•¥å¯¹å±€éƒ¨å˜åŒ–çš„æ•æ„Ÿæ€§ï¼Œç­–ç•¥æ¢¯åº¦ç”±é€†FisherçŸ©é˜µæ ¡æ­£ï¼Œè€Œä¼ ç»Ÿçš„æ¢¯åº¦æ–¹æ³•å‡å®šæ›´æ–°ä¸ºæ¬§å‡ é‡Œå¾—è·ç¦»ã€‚
* æ›´æ–°æ­¥é•¿ <span class="math inline"><em>Î±</em></span>
å…·æœ‰é€‚åº”æ¢¯åº¦å’Œå±€éƒ¨æ•æ„Ÿæ€§çš„åŠ¨æ€è¡¨è¾¾å¼ï¼Œç¡®ä¿æ— è®ºå‚æ•°åŒ–å¦‚ä½•ï¼Œç­–ç•¥å˜åŒ–å¹…åº¦ä¸º
<span
class="math inline"><em>Ïµ</em></span>ã€‚åœ¨ä¼ ç»Ÿæ–¹æ³•ä¸­ï¼Œé€šå¸¸è®¾ç½®ä¸ºä¸€äº›æ ‡å‡†å€¼ï¼Œå¦‚<span
class="math inline">0.1</span>æˆ–<span
class="math inline">0.01</span>ã€‚</p>
<p>ä½†æ˜¯è¿™ä¸ªç®—æ³•ä¹Ÿå­˜åœ¨ç¼ºé™·ï¼š *
Tayloræä¾›äº†ä¸€ä¸ªå±€åŸŸäºŒé˜¶è¿‘ä¼¼ï¼Œ<font color='red'>è¿™ä¼šå¯¼è‡´Hessianå¯èƒ½éæ­£å®šï¼ˆä¸ºä»€ä¹ˆï¼Ÿï¼‰ã€‚</font>
* Fisher information matrix
è®¡ç®—é‡è¿‡å¤§ï¼Œå°¤å…¶æ˜¯ç¥ç»ç½‘ç»œè¿™ç§å¤§é‡å‚æ•°çš„æƒ…å†µã€‚</p>
<h1 id="trust-region-policy-optimization">Trust Region Policy
Optimization</h1>
<p>Trust Region Policy
Optimizationï¼ˆTRPOï¼‰ç®—æ³•ä¿è¯äº†ç­–ç•¥æ¢¯åº¦ç®—æ³•æ¯æ¬¡æ›´æ–°å§‹ç»ˆä¼šæå‡ç­–ç•¥ã€‚</p>
<p>å®šä¹‰<span
class="math inline"><em>Î·</em></span>ä¸ºæœŸæœ›æŠ˜æ‰£å¥–åŠ±ï¼ˆæ­¤å¤„ç¬¦å·å‘ç”Ÿæ”¹å˜ï¼‰ï¼š
<span class="math display">$$\begin{align}
\eta(\pi)=\mathbb{E}_{s_0, a_0, \ldots}\left[\sum_{t=0}^{\infty}
\gamma^t r\left(s_t\right)\right]
\end{align}$$</span></p>
<p>å®šä¹‰çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°<span
class="math inline"><em>Q</em><sub><em>Ï€</em></sub>(<em>s</em><sub><em>t</em></sub>,â€†<em>a</em><sub><em>t</em></sub>)</span>å’Œä»·å€¼å‡½æ•°<span
class="math inline"><em>V</em><sub><em>Ï€</em></sub></span>ï¼Œä»¥åŠä¼˜åŠ¿å‡½æ•°<span
class="math inline"><em>A</em><sub><em>Ï€</em></sub></span>:</p>
<p><span class="math display">$$\begin{align}
Q_\pi\left(s_t, a_t\right)&amp;=\mathbb{E}_{s_{t+1}, a_{t+1},
\ldots}\left[\sum_{l=0}^{\infty} \gamma^l r\left(s_{t+l}\right)\right]
\\ V_\pi\left(s_t\right)&amp;=\mathbb{E}_{a_t, s_{t+1},
\ldots}\left[\sum_{l=0}^{\infty} \gamma^l r\left(s_{t+l}\right)\right]
\\
A_\pi(s, a)&amp;=Q_\pi(s, a)-V_\pi(s) \\
\quad a_t \sim \pi\left(a_t \mid s_t\right), &amp;s_{t+1} \sim
P\left(s_{t+1} \mid s_t, a_t\right) \text { for } t \geq 0
\end{align}$$</span></p>
<p>å…¶ä¸­ä¼˜åŠ¿å‡½æ•°ï¼Œæ˜¯åœ¨ç»™å®šçš„ç­–ç•¥å’ŒçŠ¶æ€ä¸‹ï¼Œè®¡ç®—ç‰¹å®šåŠ¨ä½œ<span
class="math inline"><em>a</em></span>çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±ä¸æ€»ä½“æœŸæœ›å€¼ï¼ˆè¯¥çŠ¶æ€çš„æœŸæœ›å¥–åŠ±ï¼‰çš„å·®å€¼ã€‚</p>
<p>ä¸‹é¢çš„å¼å­è¡¨è¾¾äº†ç­–ç•¥<span
class="math inline"><em>Ï€</em></span>ä¸ä¼˜åŠ¿ç­–ç•¥<span
class="math inline"><em>Ï€Ìƒ</em></span>ä¹‹é—´çš„å·®å¼‚ï¼ˆè¯¦ç»†è¯æ˜å‚è§åŸå§‹è®ºæ–‡é™„å½•Aï¼‰ï¼š
<span class="math display">$$\begin{equation}
\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{s_0, a_0, \cdots \sim
\tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^t A_\pi\left(s_t,
a_t\right)\right]
\end{equation}$$</span> å…¶ä¸­<span
class="math inline"><em>a</em><sub><em>t</em></sub></span>çš„é‡‡æ ·æ¦‚ç‡ä¸º<span
class="math inline"><em>Ï€Ìƒ</em>(â‹…|<em>s</em><sub><em>t</em></sub>)</span>ï¼Œ<span
class="math inline"><em>s</em><sub><em>t</em></sub></span>çš„é‡‡æ ·æ¦‚ç‡ä¾èµ–äº<span
class="math inline"><em>Ï</em><sub><em>Ï€</em></sub></span><strong>è¿™é‡Œæœ¬è´¨ä¸Šæ˜¯é‡è¦æ€§é‡‡æ ·</strong>:
<span
class="math display"><em>Ï</em><sub><em>Ï€</em></sub>(<em>s</em>)â€„=â€„<em>P</em>(<em>s</em><sub>0</sub>â€„=â€„<em>s</em>)â€…+â€…<em>Î³</em><em>P</em>(<em>s</em><sub>1</sub>â€„=â€„<em>s</em>)â€…+â€…<em>Î³</em><sup>2</sup><em>P</em>(<em>s</em><sub>2</sub>â€„=â€„<em>s</em>)â€…+â€…â€¦</span>
å› æ­¤å°†å¼æ”¹å†™ä¸ºï¼š <span class="math display">$$
\begin{align}
\eta(\tilde{\pi}) &amp; =\eta(\pi)+\sum_{t=0}^{\infty} \sum_s
P\left(s_t=s \mid \tilde{\pi}\right) \sum_a \tilde{\pi}(a \mid s)
\gamma^t A_\pi(s, a) \\
&amp; =\eta(\pi)+\sum_s \sum_{t=0}^{\infty} \gamma^t P\left(s_t=s \mid
\tilde{\pi}\right) \sum_a \tilde{\pi}(a \mid s) A_\pi(s, a) \\
&amp; =\eta(\pi)+\sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a \mid
s) A_\pi(s, a) \\
&amp;= \eta(\pi)+\sum_s \rho_{\tilde{\pi}}(s) \sum_a
\pi(a|s)\frac{\tilde{\pi}(a \mid s)}{\pi(a|s)} A_\pi(s, a) \\
&amp;=
\eta(\pi)+\mathbb{E}_{s\sim\rho_{\theta_{old}},a\sim\pi_{\theta_{old}}}\left[\frac{\tilde{\pi}(a
\mid s)}{\pi(a|s)} A_\pi(s, a)\right]
\end{align}
$$</span> å¦‚æœèƒ½å¤Ÿä¿è¯<span
class="math inline">âˆ‘<sub><em>a</em></sub><em>Ï€Ìƒ</em>(<em>a</em>â€…âˆ£â€…<em>s</em>)<em>A</em><sub><em>Ï€</em></sub>(<em>s</em>,â€†<em>a</em>)â€„â‰¥â€„0</span>ï¼Œç­–ç•¥å°†ä¼šå§‹ç»ˆå¾—ä»¥æå‡æˆ–è€…ç­‰ä»·ï¼Œç„¶è€Œå¹¶ä¸èƒ½ä¿è¯ä¸ºéè´Ÿï¼Œå› ä¸ºä¸€äº›åŠ¨ä½œå¯èƒ½å¯¼è‡´<span
class="math inline"><em>A</em></span>ä¸ºè´Ÿã€‚è€Œä¸”ç”±äº<span
class="math inline"><em>Ï</em><sub><em>Ï€Ìƒ</em></sub></span>çš„å­˜åœ¨ï¼Œä½¿å¾—å¾ˆéš¾ç›´æ¥å»ä¼˜åŒ–ï¼Œå› æ­¤é‡‡ç”¨è¿‘ä¼¼ï¼Œç”¨<span
class="math inline"><em>Ï</em><sub><em>Ï€</em></sub></span>æ›¿æ¢<span
class="math inline"><em>Ï</em><sub><em>Ï€Ìƒ</em></sub></span>ï¼š <span
class="math display">$$\begin{align}
L_\pi(\tilde{\pi})=\eta(\pi)+\sum \rho_\pi(s) \sum \tilde{\pi}(a \mid s)
A_\pi(s, a)
\end{align}$$</span>
å¯ä»¥è¯æ˜è¯¥è¿‘ä¼¼åœ¨ä¸€é˜¶å¯¼æ•°ä¸‹æ˜¯ç²¾ç¡®çš„ï¼Œå­˜åœ¨ä»¥ä¸‹çš„å…³ç³»<font color='red'>proof
it</font>ï¼š <span class="math display">$$
\begin{align}
L_{\pi_{\theta_0}}\left(\pi_{\theta_0}\right) &amp;
=\eta\left(\pi_{\theta_0}\right) \\
\left.\nabla_\theta
L_{\pi_{\theta_0}}\left(\pi_\theta\right)\right|_{\theta=\theta_0} &amp;
=\left.\nabla_\theta
\eta\left(\pi_\theta\right)\right|_{\theta=\theta_0}
\end{align}
$$</span>
ä»å®é™…å«ä¹‰ä¸Šå¯ä»¥ç†è§£ï¼Œå¦‚æœç­–ç•¥ä¸å˜ï¼Œé‚£ä¹ˆå‰åç­–ç•¥åº”å½“æ˜¯ç›¸åŒçš„ã€‚ç¬¬äºŒéƒ¨åˆ†ä¿è¯ï¼Œåªè¦èƒ½æå‡<span
class="math inline"><em>L</em><sub><em>Ï€</em><sub><em>Î¸</em><sub>0</sub></sub></sub></span>ï¼Œä¹Ÿä¼šæå‡<span
class="math inline"><em>Î·</em></span>ã€‚</p>
<p>æ–‡çŒ®<a
href="https://sham.seas.harvard.edu/publications/approximately-optimal-approximate-reinforcement-learning">Approximately
Optimal Approximate Reinforcement
Learning</a><font color='red'>æœ‰æ—¶é—´çœ‹çœ‹</font>ï¼Œç»™å‡ºäº†ä¸€ç§æ··åˆæ›´æ–°ç­–ç•¥ï¼Œå¹¶ä¸”è¯æ˜äº†æ›´æ–°åçš„ç­–ç•¥çš„ä¸‹ç•Œã€‚</p>
<p><span class="math display">$$
\begin{aligned}
\pi_{\text {new }}(a \mid s)&amp;=(1-\alpha) \pi_{\text {old }}(a \mid
s)+\alpha \pi^{\prime}(a \mid s) \\
\eta\left(\pi_{\text {new }}\right) &amp; \geq L_{\pi_{\text {old
}}}\left(\pi_{\text {new }}\right)-\frac{2 \epsilon
\gamma}{(1-\gamma)^2} \alpha^2 \\
\epsilon&amp;=\max _s\left|\mathbb{E}_{a \sim \pi^{\prime}(a \mid
s)}\left[A_\pi(s, a)\right]\right|
\end{aligned}
$$</span></p>
<h2
id="monotonic-improvement-guarantee-for-general-stochastic-policies">Monotonic
Improvement Guarantee for General Stochastic Policies</h2>
<p>æ–‡ç«  Approximately Optimal Approximate Reinforcement Learning
æå‡ºçš„æ··åˆç­–ç•¥è¿‡å¼ºä¸å¤Ÿæ™®é€‚ç”¨ï¼ŒåŒæ—¶ä¸ä¾¿äºå®è·µï¼Œä¸å…·å¤‡ä¸€èˆ¬æ€§ã€‚TRPOç®—æ³•åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå¼±åŒ–ï¼Œä½†æ˜¯åŒæ—¶è¦ä¿è¯ä¸‹ç•Œä¸å˜ã€‚</p>
<p>å¼•å…¥æ€»å˜å·®ï¼ˆTotal Variation Distanceï¼‰ï¼š<span
class="math inline">$D_{T V}(p \| q)=\frac{1}{2}
\sum_i\left|p_i-q_i\right|$</span>ï¼Œå°†<span
class="math inline"><em>Î±</em></span>å®šä¹‰å¦‚ä¸‹ï¼š</p>
<p>$$ $$</p>
<p>ä¸Šé¢è®¡ç®—ä¸‹ç•Œçš„è¯æ˜å‚è§åŸæ–‡é™„å½•ã€‚æ ¹æ®TVä¸KLæ•£åº¦çš„å…³ç³»<font color='red'>è¯æ˜å®ƒ</font>ï¼š
<span class="math display">$$\begin{align}
D_{T V}(p \| q)^2 \leq D_{\mathrm{KL}}(p \| q)
\end{align}$$</span> ä»¤<span
class="math inline"><em>D</em><sub><em>T</em><em>V</em></sub><sup>max</sup>(<em>Ï€</em>,â€†<em>Ï€Ìƒ</em>)<sup>2</sup>â€„=â€„max<sub><em>s</em></sub><em>D</em><sub>KL</sub>(<em>Ï€</em>(â‹…|<em>s</em>)âˆ¥<em>Ï€Ìƒ</em>(â‹…|<em>s</em>))</span>ï¼Œå¯ä»¥å¾—åˆ°å¦‚ä¸‹çš„ä¸‹ç•Œï¼š
<span class="math display">$$
\begin{align}
\eta(\tilde{\pi}) &amp;\geq L_\pi(\tilde{\pi})-C D_{\mathrm{KL}}^{\max
}(\pi, \tilde{\pi}) \\
C&amp;=\frac{4 \epsilon \gamma}{(1-\gamma)^2}
\end{align}
$$</span></p>
<p>ä»¤<span
class="math inline"><em>M</em><sub><em>i</em></sub>(<em>Ï€</em>)â€„=â€„<em>L</em><sub><em>Ï€</em><sub><em>i</em></sub></sub>(<em>Ï€</em>)â€…âˆ’â€…<em>C</em><em>D</em><sub><em>K</em><em>L</em></sub><sup>max</sup>(<em>Ï€</em><sub><em>i</em></sub>,â€†<em>Ï€</em>)</span>ï¼Œåˆ©ç”¨è¿™ä¸ªä¸‹ç•Œè¯æ˜å•è°ƒæ€§ã€‚</p>
<p>é¦–å…ˆ: <span
class="math display"><em>Î·</em>(<em>Ï€</em><sub><em>i</em>â€…+â€…1</sub>)â€„â‰¥â€„<em>M</em><sub><em>i</em></sub>(<em>Ï€</em><sub><em>i</em>â€…+â€…1</sub>)</span>
å¹¶ä¸”: <span
class="math display"><em>Î·</em>(<em>Ï€</em><sub><em>i</em></sub>)â€„=â€„<em>M</em><sub><em>i</em></sub>(<em>Ï€</em><sub><em>i</em></sub>)</span>
åˆ™ï¼š <span
class="math display"><em>Î·</em>(<em>Ï€</em><sub><em>i</em>â€…+â€…1</sub>)â€…âˆ’â€…<em>Î·</em>(<em>Ï€</em><sub><em>i</em></sub>)â€„â‰¥â€„<em>M</em><sub><em>i</em></sub>(<em>Ï€</em><sub><em>i</em>â€…+â€…1</sub>)â€…âˆ’â€…<em>M</em>(<em>Ï€</em><sub><em>i</em></sub>)</span>
å¦‚æœæ–°ç­–ç•¥<span
class="math inline"><em>Ï€</em><sub><em>i</em>â€…+â€…1</sub></span>èƒ½ä½¿å¾—<span
class="math inline"><em>M</em><sub><em>i</em></sub></span>æœ€å¤§ï¼Œå°±æœ‰<span
class="math inline"><em>M</em><sub><em>i</em></sub>(<em>Ï€</em><sub><em>i</em>â€…+â€…1</sub>)â€…âˆ’â€…<em>M</em>(<em>Ï€</em><sub><em>i</em></sub>)â€„â‰¥â€„0</span>ï¼Œä»è€Œå°±ä¿è¯äº†ç­–ç•¥å¿…ç„¶ç¨³æ­¥æå‡ã€‚</p>
<p>è¿™æ ·é€šè¿‡ä¼˜åŒ–ä¸‹ç•Œä¾¿å¯ä»¥ä½¿å¾—ç­–ç•¥å¾—åˆ°ç¨³å®šçš„æå‡ã€‚</p>
<p>ç®—æ³•æµç¨‹å¦‚ä¸‹ï¼š <img src="./TRPO.png" alt="TRPO" /></p>
<h2 id="optimization-of-parameterized-policies">Optimization of
Parameterized Policies</h2>
<p>ä¸‹é¢å°†è¦åŸºäºä»¥ä¸Šçš„ç†è®ºåŸºç¡€ï¼Œåœ¨æœ‰é™çš„ç©ºé—´ä»¥åŠä»»æ„å‚æ•°ä¸‹ï¼Œç»™å‡ºå…·ä½“çš„ç®—æ³•ã€‚é¦–å…ˆæ›´æ”¹ç¬¦å·æ³¨è®°ï¼Œç”¨<span
class="math inline"><em>Î¸</em></span>è¡¨ç¤ºé‡è¦çš„å‚æ•°ï¼Œè€Œéç­–ç•¥<span
class="math inline"><em>Ï€</em><sub><em>Î¸</em></sub></span>ã€‚<span
class="math inline"><em>Î·</em>(<em>Î¸</em>)â€„:=â€„<em>Î·</em>(<em>Ï€</em><sub><em>Î¸</em></sub>),â€†<em>L</em><sub><em>Î¸</em></sub>(<em>Î¸Ìƒ</em>)â€„:=â€„<em>L</em><sub><em>Ï€</em><sub><em>Î¸</em></sub></sub>(<em>Ï€</em><sub><em>Î¸Ìƒ</em></sub>),â€†<em>D</em><sub><em>K</em><em>L</em></sub>(<em>Î¸</em>âˆ¥<em>Î¸Ìƒ</em>)â€„:=â€„<em>D</em><sub><em>K</em><em>L</em></sub>(<em>Ï€</em><sub><em>Î¸</em></sub>âˆ¥<em>Ï€</em><sub><em>Î¸Ìƒ</em></sub>)</span></p>
<p>å¯ä»¥å°†ï¼š <span class="math display">$$\begin{equation}
\underset{\theta}{\operatorname{maximize}}\left[L_{\theta_{\text {old
}}}(\theta)-C D_{\mathrm{KL}}^{\max }\left(\theta_{\text {old }},
\theta\right)\right]
\end{equation}$$</span> æ”¹å†™ä¸ºï¼š <span
class="math display">$$\begin{aligned}
&amp; \underset{\theta}{\operatorname{maximize}} L_{\theta_{\text {old
}}}(\theta) \\
&amp; \text { subject to } D_{\mathrm{KL}}^{\max }\left(\theta_{\text
{old }}, \theta\right) \leq \delta
\end{aligned}$$</span> ç”±äº<span
class="math inline"><em>D</em><sub>KL</sub><sup>max</sup></span>çš„è®¡ç®—è¿‡äºéº»çƒ¦ï¼Œé‡‡ç”¨å¸¦æƒé‡çš„è¿‘ä¼¼æ›¿ä»£ï¼š
<span
class="math display"><em>DÌ„</em><sub>KL</sub><sup><em>Ï</em></sup>(<em>Î¸</em><sub>1</sub>,â€†<em>Î¸</em><sub>2</sub>)â€„:=â€„ğ”¼<sub><em>s</em>â€„âˆ¼â€„<em>Ï</em></sub>[<em>D</em><sub>KL</sub>(<em>Ï€</em><sub><em>Î¸</em><sub>1</sub></sub>(â‹…â€…âˆ£â€…<em>s</em>)âˆ¥<em>Ï€</em><sub><em>Î¸</em><sub>2</sub></sub>(â‹…â€…âˆ£â€…<em>s</em>))]</span></p>
<p>åŸºäºæ­¤æœ€ç»ˆè§£å†³çš„ä¼˜åŒ–é—®é¢˜å½¢å¼æ˜¯ï¼š <span
class="math display">$$\begin{equation}
\begin{aligned}
&amp; \underset{\theta}{\operatorname{maximize}} L_{\theta_{\text {old
}}}(\theta) \\
&amp; \text { subject to } \bar{D}_{\mathrm{KL}}^{\rho_{\theta_{\text
{old }}}}\left(\theta_{\text {old }}, \theta\right) \leq \delta .
\end{aligned}
\end{equation}$$</span></p>
<h2 id="connections-with-natural-gradients">Connections with Natural
Gradients</h2>
<p>ä»TRPOç®—æ³•æœ€ç»ˆè§£å†³é—®é¢˜çš„å½¢å¼å¯ä»¥çœ‹å‡ºï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ç‰¹å®šå½¢å¼ä¼˜åŒ–é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡è®¡ç®—æƒ©ç½šå› å­ï¼Œä»è€Œä¿è¯åœ¨æ¯ä¸€æ¬¡æ›´æ–°è¿­ä»£ä¹‹åå°±èƒ½ä¿è¯ç­–ç•¥å¾—åˆ°ç¨³å®šçš„æå‡ã€‚ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œå¼•å…¥äº†ä¸¤ä¸ªé‡è¦çš„æœºåˆ¶ï¼š
* Advantage Estimates *
æ£€æŸ¥æœºåˆ¶ï¼šéšæœºé‡‡æ ·å¹¶ä¸èƒ½ç¡®å®šç»“æœæ˜¯å¦å¾—åˆ°æå‡ï¼Œä½†æ˜¯å¯ä»¥æ£€æŸ¥é‡‡æ ·ç»“æœï¼Œé€‰å–ç¡®å®æå‡æ•ˆæœçš„æ ·æœ¬ï¼Œå¯¹äºå…¶ä»–æ ·æœ¬åˆ™ç›´æ¥æ”¾å¼ƒã€‚</p>
<p>å¯¹äºè‡ªç„¶æ¢¯åº¦ç®—æ³•ï¼Œä¹Ÿæ˜¯å…¶ä¸€ä¸ªç‰¹ä¾‹ã€‚ <span
class="math display">$$\begin{equation}
\begin{aligned}
&amp;
\underset{\theta}{\operatorname{maximize}}\left[\left.\nabla_\theta
L_{\theta_{\text {old }}}(\theta)\right|_{\theta=\theta_{\text {old }}}
\cdot\left(\theta-\theta_{\text {old }}\right)\right] \\
&amp; \text { subject to } \frac{1}{2}\left(\theta_{\text {old
}}-\theta\right)^T A\left(\theta_{\text {old
}}\right)\left(\theta_{\text {old }}-\theta\right) \leq \delta \\
&amp; \text { where } A\left(\theta_{\text {old }}\right)_{i j}= \left.
\frac{\partial}{\partial \theta_i} \frac{\partial}{\partial \theta_j}
\mathbb{E}_{s \sim \rho_\pi}\left[D_{\mathrm{KL}}\left(\pi\left(\cdot
\mid s, \theta_{\text {old }}\right) \| \pi(\cdot \mid s,
\theta)\right)\right]\right|_{\theta=\theta_{\text {old }}}
\end{aligned}
\end{equation}$$</span> å…¶ä¸­å‚æ•°æ›´æ–°ä¸º<span
class="math inline">$\theta_{\text {new }}=\theta_{\text {old
}}+\left.\frac{1}{\lambda} A\left(\theta_{\text {old }}\right)^{-1}
\nabla_\theta L(\theta)\right|_{\theta=\theta_{\text {old
}}}$</span>ï¼Œåˆ©ç”¨TRPOç®—æ³•ï¼Œå¯ä»¥é™åˆ¶æƒ©ç½šé¡¹<span
class="math inline">$\frac{1}{\lambda}$</span>ï¼Œè™½ç„¶è¿™åªæ˜¯ä¸€ä¸ªå¾ˆå°çš„ç®—æ³•å‚æ•°ï¼Œä½†æ˜¯åœ¨å¤§é—®é¢˜ä¸Šæ˜¾è‘—æå‡äº†ç®—æ³•è¡¨ç°èƒ½åŠ›ã€‚</p>
<p><font color='red'>ç¼ºå¤±å…±è½­æ¢¯åº¦æ³•ï¼Œä¸ç®—æ³•ç®€åŒ–çš„å®ç°</font></p>
<figure>
<img src="./TRPO2.png" alt="TRPO structure" />
<figcaption aria-hidden="true">TRPO structure</figcaption>
</figure>
<h1 id="proximal-policy-optimization-algorithms">Proximal Policy
Optimization Algorithms</h1>
<p>TRPOç®—æ³•ä¿è¯äº†ç¨³å®šæå‡ï¼Œä½†æ˜¯ç”±äºè®¡ç®—äºŒé˜¶æ¢¯åº¦ï¼Œè¿ç®—é‡ååˆ†å·¨å¤§ã€‚ä¸ºäº†è§£å†³è¿ç®—é‡å¤§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è¿‘ä¼¼æ–¹æ³•ï¼Œé€šè¿‡å¯¹æƒ©ç½šå› å­è¿›è¡Œé™åˆ¶ï¼Œå¤§å¹…å‡å°‘è¿ç®—é‡ã€‚</p>
<h2 id="clipped-surrogate-objective">Clipped Surrogate Objective</h2>
<p>åœ¨TRPOä¸­ä¼˜åŒ–çš„ç›®æ ‡ä¸ºï¼š <span class="math display">$$\begin{equation}
L^{C P I}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(a_t \mid
s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}
\hat{A}_t\right]=\hat{\mathbb{E}}_t\left[r_t(\theta) \hat{A}_t\right]
\end{equation}$$</span> å…¶ä¸­<span
class="math inline">$r_t(\theta)=\frac{\pi_\theta\left(a_t \mid
s_t\right)}{\pi_{\theta_{\text {old }}}}$</span>ï¼ŒCPIæŒ‡çš„æ˜¯conservative
policy iterationã€‚</p>
<p>è°ƒæ•´TRPOçš„ä¼˜åŒ–ç›®æ ‡ä¸ºï¼š <span class="math display">$$\begin{equation}
L^{C L I P}(\theta)=\hat{\mathbb{E}}_t\left[\min \left(r_t(\theta)
\hat{A}_t, \operatorname{clip}\left(r_t(\theta), 1-\epsilon,
1+\epsilon\right) \hat{A}_t\right)\right]
\end{equation}$$</span> å…¶ä¸­<span
class="math inline"><em>Ïµ</em></span>ä¸ºè¶…å‚ã€‚è¯¥å¼çš„ç¬¬ä¸€é¡¹è¡¨ç¤º<span
class="math inline"><em>L</em><sup><em>C</em><em>P</em><em>I</em></sup></span>ï¼Œç¬¬äºŒé¡¹åˆ©ç”¨å‰ªåˆ‡æƒé‡è°ƒæ•´äº†ä¼˜åŠ¿å‡½æ•°ï¼Œå°†<span
class="math inline"><em>r</em><sub><em>t</em></sub></span>é™åˆ¶åœ¨ä¸€ä¸ªèŒƒå›´å†…ã€‚æœ€åè¿”å›å‰ªåˆ‡ä¸éå‰ªåˆ‡ä¹‹åçš„è¾ƒå°å€¼ï¼Œè¿™æ ·å°†ä¼šè¿”å›ä¸€ä¸ªä¸‹ç•Œã€‚</p>
<figure>
<img src="./CLIP.png" alt="clip" />
<figcaption aria-hidden="true">clip</figcaption>
</figure>
<p>ç®—æ³•æ¡†æ¶å¦‚ä¸‹ï¼š</p>
<figure>
<img src="./PPO_Clip.png" alt="PPO Clip" />
<figcaption aria-hidden="true">PPO Clip</figcaption>
</figure>
<h2 id="adaptive-kl-penalty-coefficient">Adaptive KL Penalty
Coefficient</h2>
<p>å¦ä¸€ç§ä¼˜åŒ–æ–¹æ¡ˆæ˜¯é’ˆå¯¹æƒ©ç½šé¡¹ï¼š <span
class="math display">$$\begin{equation}
L^{K L P E N}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(a_t
\mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}
\hat{A}_t-\beta \operatorname{KL}\left[\pi_{\theta_{\text {old
}}}\left(\cdot \mid s_t\right), \pi_\theta\left(\cdot \mid
s_t\right)\right]\right]
\end{equation}$$</span> ç„¶åè®¡ç®—KLæ•£åº¦ï¼Œä»è€Œç¡®å®šæƒ©ç½šå› å­çš„å¤§å°ã€‚ <span
class="math display">$$
\begin{aligned}
&amp;d=\hat{\mathbb{E}}_t\left[\operatorname{KL}\left[\pi_{\theta_{\text
{old }}}\left(\cdot \mid s_t\right), \pi_\theta\left(\cdot \mid
s_t\right)\right]\right] \\
&amp; \quad-\text { If } d&lt;d_{\text {targ }} / 1.5, \beta \leftarrow
\beta / 2 \\
&amp; \quad-\text { If } d&gt;d_{\text {targ }} \times 1.5, \beta
\leftarrow \beta \times 2
\end{aligned}
$$</span> å…¶ä¸­<span
class="math inline">1.5,â€†2</span>æ˜¯å¯å‘å¼å¾—åˆ°ï¼Œåˆå§‹å€¼<span
class="math inline"><em>Î²</em></span>ä¹Ÿæ˜¯ä¸€ä¸ªè¶…å‚ï¼Œä½†æ˜¯å¯¹ç»“æœå½±å“ä¸å¤§ã€‚</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
              <a href="/tags/Natural-Gradients/" rel="tag"># Natural Gradients</a>
              <a href="/tags/PPO/" rel="tag"># PPO</a>
              <a href="/tags/TRPO/" rel="tag"># TRPO</a>
              <a href="/tags/Policy-Gradients/" rel="tag"># Policy Gradients</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/03/04/DL/Neural_Network_Diffusion/Neural_Network_Diffusion/" rel="prev" title="Neural Network Diffusion">
      <i class="fa fa-chevron-left"></i> Neural Network Diffusion
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/03/15/DL/Deconstructing_Denoising_Diffusion_Models_for_Self_Supervised_Learning/Deconstructing_Denoising_Diffusion_Models_for_Self_Supervised_Learning/" rel="next" title="Deconstructing Denoising Diffusion Models for Self-Supervised Learning">
      Deconstructing Denoising Diffusion Models for Self-Supervised Learning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#policy-approximation-methods-moving-to-stochastic-policies"><span class="nav-number">2.</span> <span class="nav-text">Policy
approximation methods : Moving to stochastic policies</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#establishing-the-objective-function"><span class="nav-number">2.1.</span> <span class="nav-text">Establishing the objective
function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#defining-trajectory-probabilities"><span class="nav-number">2.2.</span> <span class="nav-text">Defining trajectory
probabilities</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#deriving-the-policy-gradient"><span class="nav-number">2.3.</span> <span class="nav-text">Deriving the policy gradient</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#examples-softmax-and-gaussian-policies"><span class="nav-number">2.4.</span> <span class="nav-text">Examples: Softmax and
Gaussian policies</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#loss-functions-and-algorithmic-implementation-reinforce"><span class="nav-number">2.5.</span> <span class="nav-text">Loss
functions and Algorithmic implementation (REINFORCE)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#natural-gradients"><span class="nav-number">3.</span> <span class="nav-text">Natural Gradients</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#the-problems-with-first-order-policy-gradients"><span class="nav-number">3.1.</span> <span class="nav-text">The problems
with first-order policy gradients</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#capping-the-difference-between-policies"><span class="nav-number">3.2.</span> <span class="nav-text">Capping the difference
between policies</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#algorithm"><span class="nav-number">3.3.</span> <span class="nav-text">Algorithm</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#trust-region-policy-optimization"><span class="nav-number">4.</span> <span class="nav-text">Trust Region Policy
Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#monotonic-improvement-guarantee-for-general-stochastic-policies"><span class="nav-number">4.1.</span> <span class="nav-text">Monotonic
Improvement Guarantee for General Stochastic Policies</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#optimization-of-parameterized-policies"><span class="nav-number">4.2.</span> <span class="nav-text">Optimization of
Parameterized Policies</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#connections-with-natural-gradients"><span class="nav-number">4.3.</span> <span class="nav-text">Connections with Natural
Gradients</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#proximal-policy-optimization-algorithms"><span class="nav-number">5.</span> <span class="nav-text">Proximal Policy
Optimization Algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#clipped-surrogate-objective"><span class="nav-number">5.1.</span> <span class="nav-text">Clipped Surrogate Objective</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adaptive-kl-penalty-coefficient"><span class="nav-number">5.2.</span> <span class="nav-text">Adaptive KL Penalty
Coefficient</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">renyixiong</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">56</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">åˆ†ç±»</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">64</span>
        <span class="site-state-item-name">æ ‡ç­¾</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">renyixiong</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="ç«™ç‚¹æ€»å­—æ•°">315k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="ç«™ç‚¹é˜…è¯»æ—¶é•¿">4:46</span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> å¼ºåŠ›é©±åŠ¨
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
