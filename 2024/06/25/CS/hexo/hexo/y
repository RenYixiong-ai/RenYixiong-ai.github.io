import math
from dataclasses import dataclass
from typing import Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F


# -----------------------------
# Utilities: masks + positional encoding
# -----------------------------
def make_padding_mask(tokens: torch.Tensor, pad_id: int) -> torch.Tensor:
    """
    tokens: [B, T]
    return: key_padding_mask: [B, T] with True at PAD positions (PyTorch convention)
    """
    return tokens.eq(pad_id)


def make_causal_mask(tgt_len: int, device=None) -> torch.Tensor:
    """
    return: attn_mask [T, T] with True meaning "cannot attend"
    """
    # upper triangular without diagonal => block future positions
    mask = torch.triu(torch.ones(tgt_len, tgt_len, device=device, dtype=torch.bool), diagonal=1)
    return mask


class SinusoidalPositionalEncoding(nn.Module):
    """
    Classic sinusoidal positional encoding (non-trainable).
    Adds position info to token embeddings.
    """
    def __init__(self, d_model: int, max_len: int = 2048):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]
        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)  # even
        pe[:, 1::2] = torch.cos(position * div_term)  # odd
        self.register_buffer("pe", pe.unsqueeze(0))   # [1, max_len, d_model]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: [B, T, d_model]
        """
        T = x.size(1)
        return x + self.pe[:, :T, :]


# -----------------------------
# Core blocks: EncoderLayer / DecoderLayer
# -----------------------------
class TransformerEncoderLayer(nn.Module):
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        d_ff: int,
        dropout: float = 0.1,
        activation: str = "gelu",
        pre_norm: bool = True,
    ):
        super().__init__()
        self.pre_norm = pre_norm

        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU() if activation.lower() == "gelu" else nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
        )

        self.dropout = nn.Dropout(dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(
        self,
        x: torch.Tensor,                                # [B, S, d_model]
        src_key_padding_mask: Optional[torch.Tensor] = None,  # [B, S] True for PAD
    ) -> torch.Tensor:
        if self.pre_norm:
            # Self-attention
            h = self.norm1(x)
            attn_out, _ = self.self_attn(
                h, h, h,
                key_padding_mask=src_key_padding_mask,
                need_weights=False
            )
            x = x + self.dropout(attn_out)

            # FFN
            h = self.norm2(x)
            x = x + self.dropout(self.ffn(h))
            return x
        else:
            attn_out, _ = self.self_attn(
                x, x, x,
                key_padding_mask=src_key_padding_mask,
                need_weights=False
            )
            x = self.norm1(x + self.dropout(attn_out))
            x = self.norm2(x + self.dropout(self.ffn(x)))
            return x


class TransformerDecoderLayer(nn.Module):
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        d_ff: int,
        dropout: float = 0.1,
        activation: str = "gelu",
        pre_norm: bool = True,
    ):
        super().__init__()
        self.pre_norm = pre_norm

        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)
        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)

        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU() if activation.lower() == "gelu" else nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
        )

        self.dropout = nn.Dropout(dropout)
        self.norm1 = nn.LayerNorm(d_model)  # self-attn
        self.norm2 = nn.LayerNorm(d_model)  # cross-attn
        self.norm3 = nn.LayerNorm(d_model)  # ffn

    def forward(
        self,
        x: torch.Tensor,                                 # [B, T, d_model]
        memory: torch.Tensor,                            # [B, S, d_model]
        tgt_mask: Optional[torch.Tensor] = None,          # [T, T] True => block
        tgt_key_padding_mask: Optional[torch.Tensor] = None,  # [B, T] True for PAD
        memory_key_padding_mask: Optional[torch.Tensor] = None, # [B, S] True for PAD
    ) -> torch.Tensor:
        if self.pre_norm:
            # Masked self-attention (causal)
            h = self.norm1(x)
            self_out, _ = self.self_attn(
                h, h, h,
                attn_mask=tgt_mask,
                key_padding_mask=tgt_key_padding_mask,
                need_weights=False
            )
            x = x + self.dropout(self_out)

            # Cross-attention over encoder memory
            h = self.norm2(x)
            cross_out, _ = self.cross_attn(
                h, memory, memory,
                key_padding_mask=memory_key_padding_mask,
                need_weights=False
            )
            x = x + self.dropout(cross_out)

            # FFN
            h = self.norm3(x)
            x = x + self.dropout(self.ffn(h))
            return x
        else:
            self_out, _ = self.self_attn(
                x, x, x,
                attn_mask=tgt_mask,
                key_padding_mask=tgt_key_padding_mask,
                need_weights=False
            )
            x = self.norm1(x + self.dropout(self_out))

            cross_out, _ = self.cross_attn(
                x, memory, memory,
                key_padding_mask=memory_key_padding_mask,
                need_weights=False
            )
            x = self.norm2(x + self.dropout(cross_out))

            x = self.norm3(x + self.dropout(self.ffn(x)))
            return x


# -----------------------------
# Stacks: Encoder / Decoder
# -----------------------------
class TransformerEncoder(nn.Module):
    def __init__(
        self,
        num_layers: int,
        d_model: int,
        n_heads: int,
        d_ff: int,
        dropout: float = 0.1,
        activation: str = "gelu",
        pre_norm: bool = True,
    ):
        super().__init__()
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(d_model, n_heads, d_ff, dropout, activation, pre_norm)
            for _ in range(num_layers)
        ])
        self.final_norm = nn.LayerNorm(d_model) if pre_norm else nn.Identity()

    def forward(self, x: torch.Tensor, src_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        for layer in self.layers:
            x = layer(x, src_key_padding_mask=src_key_padding_mask)
        return self.final_norm(x)


class TransformerDecoder(nn.Module):
    def __init__(
        self,
        num_layers: int,
        d_model: int,
        n_heads: int,
        d_ff: int,
        dropout: float = 0.1,
        activation: str = "gelu",
        pre_norm: bool = True,
    ):
        super().__init__()
        self.layers = nn.ModuleList([
            TransformerDecoderLayer(d_model, n_heads, d_ff, dropout, activation, pre_norm)
            for _ in range(num_layers)
        ])
        self.final_norm = nn.LayerNorm(d_model) if pre_norm else nn.Identity()

    def forward(
        self,
        x: torch.Tensor,
        memory: torch.Tensor,
        tgt_mask: Optional[torch.Tensor] = None,
        tgt_key_padding_mask: Optional[torch.Tensor] = None,
        memory_key_padding_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        for layer in self.layers:
            x = layer(
                x, memory,
                tgt_mask=tgt_mask,
                tgt_key_padding_mask=tgt_key_padding_mask,
                memory_key_padding_mask=memory_key_padding_mask
            )
        return self.final_norm(x)


# -----------------------------
# Full model example: Seq2Seq Transformer
# -----------------------------
@dataclass
class TransformerConfig:
    src_vocab: int
    tgt_vocab: int
    pad_id: int = 0
    d_model: int = 512
    n_heads: int = 8
    d_ff: int = 2048
    num_encoder_layers: int = 6
    num_decoder_layers: int = 6
    dropout: float = 0.1
    max_len: int = 2048
    pre_norm: bool = True


class Seq2SeqTransformer(nn.Module):
    def __init__(self, cfg: TransformerConfig):
        super().__init__()
        self.cfg = cfg

        self.src_embed = nn.Embedding(cfg.src_vocab, cfg.d_model, padding_idx=cfg.pad_id)
        self.tgt_embed = nn.Embedding(cfg.tgt_vocab, cfg.d_model, padding_idx=cfg.pad_id)
        self.pos_enc = SinusoidalPositionalEncoding(cfg.d_model, max_len=cfg.max_len)
        self.dropout = nn.Dropout(cfg.dropout)

        self.encoder = TransformerEncoder(
            cfg.num_encoder_layers, cfg.d_model, cfg.n_heads, cfg.d_ff,
            dropout=cfg.dropout, pre_norm=cfg.pre_norm
        )
        self.decoder = TransformerDecoder(
            cfg.num_decoder_layers, cfg.d_model, cfg.n_heads, cfg.d_ff,
            dropout=cfg.dropout, pre_norm=cfg.pre_norm
        )

        self.out_proj = nn.Linear(cfg.d_model, cfg.tgt_vocab)

    def encode(self, src_tokens: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        src_tokens: [B, S]
        return: memory [B, S, d_model], src_key_padding_mask [B, S]
        """
        src_kpm = make_padding_mask(src_tokens, self.cfg.pad_id)
        x = self.src_embed(src_tokens) * math.sqrt(self.cfg.d_model)
        x = self.dropout(self.pos_enc(x))
        memory = self.encoder(x, src_key_padding_mask=src_kpm)
        return memory, src_kpm

    def decode(
        self,
        tgt_tokens: torch.Tensor,
        memory: torch.Tensor,
        src_key_padding_mask: torch.Tensor,
    ) -> torch.Tensor:
        """
        tgt_tokens: [B, T]
        memory: [B, S, d_model]
        """
        tgt_kpm = make_padding_mask(tgt_tokens, self.cfg.pad_id)
        tgt_mask = make_causal_mask(tgt_tokens.size(1), device=tgt_tokens.device)

        x = self.tgt_embed(tgt_tokens) * math.sqrt(self.cfg.d_model)
        x = self.dropout(self.pos_enc(x))
        y = self.decoder(
            x, memory,
            tgt_mask=tgt_mask,
            tgt_key_padding_mask=tgt_kpm,
            memory_key_padding_mask=src_key_padding_mask
        )
        logits = self.out_proj(y)  # [B, T, tgt_vocab]
        return logits

    def forward(self, src_tokens: torch.Tensor, tgt_tokens: torch.Tensor) -> torch.Tensor:
        """
        Teacher forcing:
          src_tokens: [B, S]
          tgt_tokens: [B, T] (usually shifted-right input)
        """
        memory, src_kpm = self.encode(src_tokens)
        logits = self.decode(tgt_tokens, memory, src_kpm)
        return logits


# -----------------------------
# Minimal usage demo
# -----------------------------
if __name__ == "__main__":
    torch.manual_seed(0)

    cfg = TransformerConfig(src_vocab=32000, tgt_vocab=32000, pad_id=0, d_model=256, n_heads=8, d_ff=1024,
                            num_encoder_layers=4, num_decoder_layers=4, dropout=0.1, max_len=512)

    model = Seq2SeqTransformer(cfg)

    B, S, T = 2, 10, 12
    src = torch.randint(1, cfg.src_vocab, (B, S))
    tgt_in = torch.randint(1, cfg.tgt_vocab, (B, T))

    # add some pads
    src[0, -2:] = cfg.pad_id
    tgt_in[1, -3:] = cfg.pad_id

    logits = model(src, tgt_in)
    print("logits:", logits.shape)  # [B, T, tgt_vocab]
