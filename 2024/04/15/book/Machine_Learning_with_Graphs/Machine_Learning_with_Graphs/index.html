<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="GgT5ZflbkwsNKpDm5Tou0_J9qmdMniAiIzZ84RQe5zM">
  <meta name="msvalidate.01" content="07C0174F9E4B4B2C960172E0CDFB3DC9">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"renyixiong-ai.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="图论（Graphs）是数学的一个分支，它研究的是图这种数学结构。图论中的图是由点（也称为顶点）和连接这些点的线（也称为边）组成的。点通常用来代表事物，而边则表示事物之间的关系。图可以是有向的，也可以是无向的，这取决于边是否有方向。 图论的研究内容包括图的各种性质，如度（一个点连接的边的数量）、连通性（图中的点是否都通过边相互可达）、路径（点与点之间的连线序列）等。图论不仅在数学领域内有广泛的应用，">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning with Graphs">
<meta property="og:url" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/index.html">
<meta property="og:site_name" content="Yixiong&#39;s Blog">
<meta property="og:description" content="图论（Graphs）是数学的一个分支，它研究的是图这种数学结构。图论中的图是由点（也称为顶点）和连接这些点的线（也称为边）组成的。点通常用来代表事物，而边则表示事物之间的关系。图可以是有向的，也可以是无向的，这取决于边是否有方向。 图论的研究内容包括图的各种性质，如度（一个点连接的边的数量）、连通性（图中的点是否都通过边相互可达）、路径（点与点之间的连线序列）等。图论不仅在数学领域内有广泛的应用，">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/2-1.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/2-2.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/2-3.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/3-1.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/3-3.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/3-4.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/3-5.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/4-2-1.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/4-2-2.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/4-2-3.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/5-1.png">
<meta property="og:image" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/6-2.png">
<meta property="article:published_time" content="2024-04-15T12:39:00.000Z">
<meta property="article:modified_time" content="2025-05-08T09:51:36.413Z">
<meta property="article:author" content="Ren Yixiong">
<meta property="article:tag" content="Graphs">
<meta property="article:tag" content="Complex Network">
<meta property="article:tag" content="Laplacian Matrix">
<meta property="article:tag" content="Hypergraph Neural Networks">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/2-1.png">

<link rel="canonical" href="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Machine Learning with Graphs | Yixiong's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Yixiong's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://renyixiong-ai.github.io/2024/04/15/book/Machine_Learning_with_Graphs/Machine_Learning_with_Graphs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ren Yixiong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yixiong's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Machine Learning with Graphs
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-04-15 20:39:00" itemprop="dateCreated datePublished" datetime="2024-04-15T20:39:00+08:00">2024-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-05-08 17:51:36" itemprop="dateModified" datetime="2025-05-08T17:51:36+08:00">2025-05-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>27k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>49 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>图论（Graphs）是数学的一个分支，它研究的是图这种数学结构。图论中的图是由点（也称为顶点）和连接这些点的线（也称为边）组成的。点通常用来代表事物，而边则表示事物之间的关系。图可以是有向的，也可以是无向的，这取决于边是否有方向。</p>
<p>图论的研究内容包括图的各种性质，如度（一个点连接的边的数量）、连通性（图中的点是否都通过边相互可达）、路径（点与点之间的连线序列）等。图论不仅在数学领域内有广泛的应用，还在计算机科学、工程学、经济学等多个领域发挥着重要作用。</p>
<p>因多次发现与图论相关的内容，故在这里记录学习图论的笔记与想法。</p>
<p>参考文献： * <a
target="_blank" rel="noopener" href="https://datawhalechina.github.io/grape-book/#/">图深度学习（葡萄书）</a>
* <a
target="_blank" rel="noopener" href="https://jingboyang.github.io/stanford-cs224w-graph-ml/stanford_cs224w_graph_ml.pdf">The
Healthy Birds Trio, Jure Leskovec, Notes for Stanford CS224W Machine
Learning with Graphs, 2020</a> *
《图强化学习：原理与实践入门》谢文杰、周炜星，清华大学出版社
<font color='gray'>不推荐阅读，东拼西凑强行逻辑合理，很多内容点不到就止，同时不给深入阅读的材料。</font>
* <a target="_blank" rel="noopener" href="http://cs-www.cs.yale.edu/homes/spielman/sagt/">Spectral and
Algebraic Graph Theory Incomplete Draft</a></p>
<p>感谢<a
target="_blank" rel="noopener" href="https://github.com/datawhalechina/grape-book">Datawhale开源社区</a>提供学习平台与相关资源。</p>
<span id="more"></span>
<h1 id="why-graphs">Why Graphs</h1>
<p>最近频繁的看到图网络的相关内容，产生了好奇：图网络究竟有哪些优越性？</p>
<h2 id="数据结构">数据结构</h2>
<p>在拓扑结构上图明显具备更高的普适性质。语言可以理解为一维数据，图像可以理解为二维数据，这两种类型显然是图的一种特殊结构。对于复杂网络，如果使用NLP、CV等模型，则必须进行“近似”，而图网络就能囊括数据的信息。</p>
<p>这种“近似”因为要确定顺序，造成不同数据之间不等价，会破坏数据中的对称性。例如“茶也可以吃”，写成首尾相接的形式，无论从哪里开始读到结束，均保持语言上的合理性，并且意思一致。在这句话中重要的不同字之间的相对位置，而不是绝对位置。图在处理这类问题上具有优越性。</p>
<p>这种“近似”影响，在图像上更为明显。一张图像往往不会因其旋转、镜像变换而造成含义上的变化，而CNN等等网络不能理解这种对称。一般在进行训练的时候，都会使用数据增强的方法，尽可能让网络理解这种对称。</p>
<p>因此处理这种相对位置关键，绝对位置不关键的数据，图网络具备其先天的优势。</p>
<p>同时具备图这种拓扑结构，与许多问题等价。可以将其它问题转化为图上进行求解。例如对于物理中求解模型的配分函数：将配分函数转化为扭结展开，而扭结就是一种图；除此之外，图的形式与张量网络也极为相似，两者之间可能存在相关性。</p>
<p>许多优化问题，本身就是通过图结构描述，旅行商问题、最大割问题；其它优化问题虽然不是用图显式表达，但是也可以<a
href="https://renyixiong-ai.github.io/2024/04/15/Math/Ising_formulations_of_many_NP_problems/Ising_formulations_of_many_NP_problems/">转化为图结构</a>。</p>
<p>复杂网络和图的邻接矩阵可以任意的交换行和列，同样可以表示一个图或者网络，也就是图论中图同构的概念。邻接矩阵的行和列的交换知识重新对节点进行标号，并不会改变网络的结构和特征。图数据特征可以分为结构特征、属性特征、拓扑特征等，属性特征包含节点属性、连边属性和全局图属性。</p>
<h2 id="what-types-of-problems-have-graph-structured-data">What types of
problems have graph structured data?</h2>
<p>通常有三种类型的图任务： * graph-level In a graph-level task, our
goal is to predict the property of an entire graph. * node-level
Node-level tasks are concerned with predicting the identity or role of
each node within a graph. * edge-level Edge-level task get the
relationship between the nodes.</p>
<h1 id="graphs-with-complex-network">Graphs with Complex Network</h1>
<ul>
<li><a
target="_blank" rel="noopener" href="https://distill.pub/2021/gnn-intro/">优秀的可视化网站</a></li>
<li>python中关于图的包<a
target="_blank" rel="noopener" href="https://networkx.github.io/">Networkx</a></li>
</ul>
<h2 id="图的定义">图的定义</h2>
<h2 id="图的性质">图的性质</h2>
<h3 id="邻接节点neighbors">邻接节点（neighbors）</h3>
<h3 id="图的度degree">图的度（degree）</h3>
<h3 id="行走walk和路径path">行走（walk）和路径（path）</h3>
<h3
id="距离distance和直径diameter">距离（distance）和直径（diameter）</h3>
<h3
id="子图subgraph连通分量connected-component连通图conected-graph">子图（subgraph）、连通分量（connected
component）、连通图（conected graph）</h3>
<h3 id="聚类系数clustering-coefficient">聚类系数（Clustering
Coefficient）</h3>
<h3 id="接近中心度closeness-centrality">接近中心度（closeness
centrality）</h3>
<h2 id="图的连接表示">图的连接表示</h2>
<h3 id="邻接矩阵">邻接矩阵</h3>
<p>邻接矩阵（adjacency matrix）刻画点和点之间的关系。</p>
<p>给定一个图 <span
class="math inline"><em>G</em> = {<em>V</em>, <em>E</em>}</span> ,
其对应的邻接矩阵被记为 <span
class="math inline"><strong>A</strong> ∈ {0, 1}<sup><em>N</em> × <em>N</em></sup></span>
。<span
class="math inline"><strong>A</strong><sub><strong>i</strong>, j</sub> = 1</span>
表示存在从节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 到 <span
class="math inline"><em>v</em><sub><em>j</em></sub></span> 的边, <span
class="math inline"><strong>A</strong><sub><em>i</em>, <em>j</em></sub> = 0</span>
表示不存在从节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 到 <span
class="math inline"><em>v</em><sub><em>j</em></sub></span> 的边。</p>
<ul>
<li>在无向图中, 从节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 到 <span
class="math inline"><em>v</em><sub><em>j</em></sub></span> 的边存在,
意味着从节点 <span
class="math inline"><em>v</em><sub><em>j</em></sub></span> 到 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span>
的边也存在。因而无向图的邻接矩阵是对称的。</li>
<li>在无权图中, 各条边的权重被认为是等价的, 即认为各条边的权重为 1
。</li>
<li>对于有权图, 其对应的邻接矩阵通常被记为 <span
class="math inline"><strong>W</strong> ∈ R<sup><em>N</em> × <em>N</em></sup></span>,
其中 <span
class="math inline"><strong>W</strong><sub>i, j</sub> = <em>w</em><sub><em>i</em><em>j</em></sub></span>
表示从节 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 到 <span
class="math inline"><em>v</em><sub><em>j</em></sub></span>
的边的权重。若边不存在时, 边的权重为 0 。</li>
</ul>
<figure>
<img src="./2-1.png" alt="邻接矩阵ssss" />
<figcaption aria-hidden="true">邻接矩阵ssss</figcaption>
</figure>
<h3 id="关联矩阵">关联矩阵</h3>
<p>关联矩阵（incidence matrix）描述定点和边的关系。</p>
<p>给定一个图 <span
class="math inline"><em>G</em> = {<em>V</em>, <em>E</em>}</span>,
其对应的关联矩阵被记为<span
class="math inline"><strong>M</strong> ∈ {0, 1}<sup><em>N</em> × <em>M</em></sup></span>
。</p>
<ul>
<li><span
class="math inline"><strong>M</strong><sub><strong>i</strong>, j</sub> = 1</span>
表示节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 和边 <span
class="math inline"><em>e</em><sub><em>j</em></sub></span> 相连接, <span
class="math inline"><strong>M</strong><sub><strong>i</strong>, j</sub> = 0</span>
表示节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 和边 <span
class="math inline"><em>e</em><sub><em>j</em></sub></span>
不相连接。</li>
<li>与邻接矩阵不同, 关联矩阵描述的是定点和边之间的关系。</li>
</ul>
<figure>
<img src="./2-2.png" alt="关联矩阵" />
<figcaption aria-hidden="true">关联矩阵</figcaption>
</figure>
<h3 id="拉普拉斯矩阵">拉普拉斯矩阵</h3>
<p>拉普拉斯矩阵（Laplacian Matrix）：（也叫做 admittance matrix,
Kirchhoff matrix）给定一个图 <span
class="math inline"><em>G</em> = {<em>V</em>, <em>E</em>}</span>,
其邻接矩阵为 <span class="math inline"><em>A</em></span>, 其拉普拉斯矩阵
<span class="math inline"><em>L</em></span> 定义为 <span
class="math display"><strong>L</strong> = <strong>D</strong> − <strong>A</strong></span></p>
<p>其中 <span
class="math inline"><strong>D</strong> = diag (<strong>d</strong>(<strong>v</strong><sub>1</sub>), …, <strong>d</strong>(<strong>v</strong><sub><strong>N</strong></sub>))</span>
是度矩阵（表示每一个节点的度）。更具体地,
我们记拉普拉斯矩阵中每一个元素为 <span
class="math inline"><em>L</em><sub><em>i</em><em>j</em></sub></span>,
那么每一个元素可以被定义为 <span class="math display">$$
L_{i j}=\left\{\begin{array}{cl}
d_i, &amp; \text { if } i=j \\
-1, &amp; \text { if } i \neq j \text { and } v_i \text { adjacent with
} v_j \\
0, &amp; \text { otherwise }
\end{array}\right.
$$</span></p>
<figure>
<img src="./2-3.png" alt="拉普拉斯矩阵" />
<figcaption aria-hidden="true">拉普拉斯矩阵</figcaption>
</figure>
<p>拉普拉斯矩阵也有标准化的表示方法，即矩阵的主对角线元素为<span
class="math inline">1</span>： <span
class="math display">$$\begin{align}
L_{\text {norm }}=&amp; D^{-1 / 2} L D^{-1 / 2} \\
=&amp;\boldsymbol{I}_n-D^{-1 / 2} W D^{-1 / 2} \label{normal_laplace}\\
=&amp;\left\{\begin{array}{c}
1, i=j \\
-\frac{1}{\sqrt{d_i d_j}}, e_{i j} \in E \\
0, e_{i j} \notin E
\end{array}\right.
\end{align}$$</span></p>
<h2 id="图的类型">图的类型</h2>
<h3 id="图的拓扑结构">图的拓扑结构</h3>
<h3
id="同质图homogeneous-graph和异质图heterogeneous-graph">同质图（Homogeneous
Graph）和异质图（Heterogeneous Graph）</h3>
<h3 id="二分图-bipartite-graph">二分图 （bipartite graph）</h3>
<h2 id="节点指标">节点指标</h2>
<p>节点是构成图的关键因素，表示的是个体。 ### 节点的度
有向网络中节点<span class="math inline"><em>i</em></span>的度： * 出度
<span class="math inline">$k_i^{\text{out}}=\sum_{j=1}^N a_{ij}$</span>
* 入度 <span class="math inline">$k_i^{\text{in}}=\sum_{j=1}^N
a_{ji}$</span> * 节点的度 <span
class="math inline"><em>k</em><sub><em>i</em></sub> = <em>k</em><sub><em>i</em></sub><sup>in</sup> + <em>k</em><sub><em>i</em></sub><sup>out</sup></span></p>
<p>无向网络： * <span class="math inline">$k_i = \sum_{j=1}^N a_{ij} =
\sum_{j=1}^N a_{ji}$</span></p>
<h3 id="节点的强度">节点的强度</h3>
<p>加权网络中更准确刻画节点的中心性（重性质），必须要考虑边的权重。在有向网络中：
* 出度 <span class="math inline">$k_i^{\text{out}}=\sum_{j=1}^N
w_{ij}$</span> * 入度 <span
class="math inline">$k_i^{\text{in}}=\sum_{j=1}^N w_{ji}$</span> *
节点的度 <span
class="math inline"><em>k</em><sub><em>i</em></sub> = <em>k</em><sub><em>i</em></sub><sup>in</sup> + <em>k</em><sub><em>i</em></sub><sup>out</sup></span></p>
<h3 id="聚簇系数">聚簇系数</h3>
<p>聚簇系数（Clustering
Coefficient）刻画网络节点的邻居节点之间的连接紧密程度。节点<span
class="math inline"><em>i</em></span>的局部聚簇系数<span
class="math inline"><em>C</em><sub><em>c</em></sub><em>i</em></span>是它的相离节点之间的关系系数与它们所有可能存在的关系数量的比值：
<span class="math display">$$\begin{align}
C_c(i) = \frac{\sum_{i,j\in\mathcal{N}_i}a_{ij}}{k_i (k_i-1)}
\end{align}$$</span> 其中<span
class="math inline">𝒩<sub><em>i</em></sub></span>为节点<span
class="math inline"><em>i</em></span>的邻居节点集合，<span
class="math inline"><em>a</em><sub><em>i</em><em>j</em></sub></span>为邻接矩阵元素，<span
class="math inline"><em>k</em><sub><em>i</em></sub></span>表示节点<span
class="math inline"><em>i</em></span>相邻的边数。</p>
<p>整个网络的平均聚簇系数： <span class="math display">$$\begin{align}
C_c(i) = \frac{\sum_{i=1}^N C_c(i)}{N}
\end{align}$$</span></p>
<p>可以知道，星状网络的节点聚簇系数为<span
class="math inline">0</span>，全连接图的节点聚簇系数为<span
class="math inline">1</span>。</p>
<h2 id="网络连边指标">网络连边指标</h2>
<p>通过将原始图转化对偶图，原始图中的边成为对偶图中的节点，因此研究节点中心性和重要性的指标与方法同样适用于研究边。</p>
<h2 id="网络模体结构">网络模体结构</h2>
<p>网络模体（Network
Motif）是指在网络中重复出现的子网络的结构模式，被认为是在复杂网络中扮演重要功能角色的基本构建单元。</p>
<h1 id="graph-representation-learning">Graph Representation
Learning</h1>
<p>经典机器学习任务不能简单的迁移到图数据上，因此需要基于图类型数据开发新的算法（监督、无监督、强化学习）。图表示学习是其中的重要组件。</p>
<h2 id="图表示一般框架">图表示一般框架</h2>
<p>将图机器学习过程描述为学习一个函数映射的过程： <span
class="math display">$$\begin{align}
\boldsymbol{y} =&amp; f_\boldsymbol{w} (\boldsymbol{A}, \boldsymbol{X})
\\
\boldsymbol{y} =&amp; f_\boldsymbol{w} (\boldsymbol{A}, \boldsymbol{X},
\boldsymbol{X}_{\text{edge}})
\end{align}$$</span> 其中<span
class="math inline"><strong>y</strong></span>是标签，<span
class="math inline"><strong>w</strong></span>是模型参数，<span
class="math inline"><strong>A</strong><strong>X</strong></span>与<span
class="math inline"><strong>A</strong><strong>X</strong><strong>X</strong><sub>edge</sub></span>为图的相关信息。</p>
<p>可以构建基于均方差的损失函数： <span
class="math display">$$\begin{align}
\mathcal{L}(w)=&amp;\frac{1}{N}\sum_{k=1}^N \left(f_{w}(A_k, X_k)-y_k
\right) ^2 \\
\hat{\boldsymbol{w}}=&amp;\arg\max_{\boldsymbol{w}}\mathcal{L}\left(\boldsymbol{w}|(A_k,X_k,y_k)_{k=1,2,3\dots
N} \right)
\end{align}$$</span></p>
<p>自编码器架构，在图网络中广泛使用，通过encoder-decoder过程，将图信息从高维离散转化到低维稠密信息空间，在稠密信息空间计算相关特征值。其中encoder过是称为：图嵌入（Graph
Embedding）或者网络嵌入（Network Embedding）。</p>
<h3 id="编码器">编码器</h3>
<p>图嵌入模型的映射关系可用函数表示, 一般称之为编码器 <span
class="math inline"><em>f</em><sub>Encoder </sub></span>,
或者特征提取器: <span class="math display">$$\begin{align}
f_{\text {Encoder }}: V \rightarrow \mathbb{R}^D
\end{align}$$</span> 其中， <span class="math inline"><em>D</em></span>
表示低维嵌入空间的维度。编码器将网络节点映射为低维稠密向量。</p>
<p>下面给出一个浅层嵌入（shallow
Embedding）的例子，将所有节点向量表示成矩阵形式 <span
class="math inline"><em>Z</em></span>, 矩阵如下所示: <span
class="math display">$$
\begin{aligned}
&amp; \\
&amp; 1 \\
&amp; 2 \\
&amp; \vdots \\
&amp; D-1 \\
&amp; D
\end{aligned}\begin{array}{cccccc}
v_1 &amp; v_2 &amp; v_3 &amp; \cdots &amp; v_{N-1} &amp; v_N \\
z_{11} &amp; z_{12} &amp; z_{13} &amp; \cdots &amp; z_{1, N-1} &amp;
z_{1 N} \\
z_{21} &amp; z_{22} &amp; z_{23} &amp; \cdots &amp; z_{2, N-1} &amp;
z_{2 N} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots
\\
z_{D-1,1} &amp; z_{D-1,2} &amp; z_{D-1,3} &amp; \cdots &amp; z_{D-1,
N-1} &amp; z_{D-1, N} \\
z_{D 1} &amp; z_{D 2} &amp; z_{D 3} &amp; \cdots &amp; z_{D, N-1} &amp;
z_{D N}
\end{array}
$$</span></p>
<p>在节点属性向量矩阵中，第 <span class="math inline"><em>i</em></span>
行表示节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span>
的嵌入空间属性向量值,
属性向量表示节点的结构属性特征和语义属性特征的融合信息，可作为下游图机器学习任务的决策变量。因此，我们可定义编码器函数为
<span class="math display">$$\begin{align}
f_{\text {Encoder
}}\left(v_i\right)=\boldsymbol{Z}\left[\boldsymbol{e}_i\right]
\end{align}$$</span></p>
<p>其中 <span
class="math inline">[<em>e</em><sub><em>i</em></sub>]</span>
表示单位矩阵的第 <span class="math inline"><em>i</em></span>
列，即列向量长度为 <span class="math inline"><em>N</em></span>, 第 <span
class="math inline"><em>i</em></span> 个元素为 1 , 其他元素为 0 。矩阵
<span class="math inline"><em>Z</em></span> 和列向量 <span
class="math inline">[<strong>e</strong><sub><em>i</em></sub>]</span>
相乘，得到矩阵 <span class="math inline"><em>Z</em></span>的第 <span
class="math inline"><em>i</em></span> 列。 编码器映射函数 <span
class="math inline"><em>f</em><sub>Encoder </sub></span>
输出结果为节点对应的低维嵌入空间的坐标信息，或者称之为隐空间信息（潜变量空间）：
<span class="math display">$$\begin{align}
f_{\text {Encoder }}\left(v_i\right)=z_i=\left[z_{i 1}, z_{i 2}, \cdots,
z_{i D}\right]^{\top}
\end{align}$$</span></p>
<h3 id="解码器">解码器</h3>
<p>在图表示学习中，一般将解码器表示成函数形式： <span
class="math display">$$\begin{align}
f_{\text {Decoder }}: \mathbb{R}^D \times \mathbb{R}^D \rightarrow
\mathbb{R}^{+}
\end{align}$$</span></p>
<p>其中, <span
class="math inline">ℝ<sup>+</sup></span>表示解码器的输出。解码过程为信息还原的过程。一般而言,
现流行的一些图嵌入算法都选择重建原始网络结构信息。这样的好处在于为监督学习，有含标签的数据集，产生了高效的编码器，方便下游任务的使用。</p>
<p>为衡量与原始网络之间的差异大小，会选取一些特征值指标，如网络节点邻域信息或网络节点间相似性信息。最简单的情况则为基于编码信息预测节点之间的连边情况，或是节点之间的相似性测度
<span
class="math inline"><em>S</em>(<em>v</em><sub><em>i</em></sub>, <em>v</em><sub><em>j</em></sub>)</span>,
解码器可以表示为: <span class="math display">$$\begin{align}
f_{\text {Decoder }}\left(f_{\text {Encoder }}\left(v_i\right), f_{\text
{Encoder }}\left(v_j\right)\right)=f_{\text {Decoder
}}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) \approx S\left(v_i,
v_j\right)
\end{align}$$</span></p>
<p>公式表明, 越好的图表示向量或图嵌入属性 <span
class="math inline"><em>Z</em></span>,
能使得解码器更好地还原原始节点之间关联信息，如相似性或邻域结构。相似性测度
<span
class="math inline"><em>S</em>(<em>v</em><sub><em>i</em></sub>, <em>v</em><sub><em>j</em></sub>)</span>
可以作为先验知识，如邻接矩阵等。</p>
<h3 id="模型优化">模型优化</h3>
<p>编码器和解码器组合构成模型的整体框架和信息处理流程图,
而节点嵌入向量是需要最终学习的属性向量,
即为模型的可学习参数，也是可优化参数，因此设定损失函数为 <span
class="math display">$$\begin{align}
\mathcal{L}=\sum_{(i, j) \in E} f_{\text {Loss }}\left(f_{\text {Decoder
}}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right), S\left(v_i,
v_j\right)\right)
\end{align}$$</span></p>
<p>上式中，损失函数 <span class="math inline"><em>f</em><sub>Loss
</sub></span> 计算解码器输出值 <span
class="math inline"><em>f</em><sub>Decoder
</sub>(<em>z</em><sub><em>i</em></sub>, <em>z</em><sub><em>j</em></sub>)</span>
和相似性测度值 <span
class="math inline"><em>S</em>(<em>v</em><sub><em>i</em></sub>, <em>v</em><sub><em>j</em></sub>)</span>
之间的差异。差异测度函数有多种选择，如绝对值、均方差等。因此，最优化的图嵌入空间向量矩阵为:
<span class="math display">$$\begin{align}
\hat{\boldsymbol{Z}}=\arg \min _{\boldsymbol{Z}}
\mathcal{L}(\boldsymbol{Z} \mid G(V, E))
\end{align}$$</span></p>
<p>上式中，网络 <span
class="math inline"><em>G</em>(<em>V</em>, <em>E</em>)</span>
包含图嵌入算法需要的信息，如邻接矩阵和节点属性等。</p>
<h2 id="基于矩阵分解的图嵌入">基于矩阵分解的图嵌入</h2>
<p>图嵌入或网络嵌入是将复杂网络映射到一个维稠密空间。</p>
<figure>
<img src="./3-1.png" alt="图嵌入" />
<figcaption aria-hidden="true">图嵌入</figcaption>
</figure>
<h3 id="图分解方法">图分解方法</h3>
<p>图分解方法（GF），其解码器为： <span
class="math display"><em>f</em><sub>Decoder
</sub> = <em>z</em><sub><em>i</em></sub><sup><em>T</em></sup><em>z</em><sub><em>j</em></sub>,</span></p>
<p>表示节点 <span
class="math inline"><em>v</em><sub><em>i</em></sub></span> 和 <span
class="math inline"><em>v</em><sub><em>j</em></sub></span>
在嵌入空间的属性向量 <span
class="math inline"><em>z</em><sub><em>i</em></sub></span> 和 <span
class="math inline"><em>z</em><sub><em>j</em></sub></span>
的内积，度量嵌入向量 <span
class="math inline"><em>z</em><sub><em>i</em></sub></span> 和 <span
class="math inline"><em>z</em><sub><em>j</em></sub></span>
之间的相似性。选择二范数作为 GF 方法的损失函数: <span
class="math display">ℒ = ∑<sub>(<em>i</em>, <em>j</em>) ∈ <em>E</em></sub>∥<strong>z</strong><sub><em>i</em></sub><sup><em>T</em></sup><strong>z</strong><sub><em>j</em></sub> − <strong>A</strong>[<em>i</em>, <em>j</em>]∥<sub>2</sub><sup>2</sup>,</span></p>
<p>损失函数度量解码器输出值和相似性测度 <span
class="math inline"><strong>A</strong>[<em>i</em>, <em>j</em>]</span>
之间的差异。</p>
<h3 id="grarep方法">GraRep方法</h3>
<p>GraRep方法在 GF 方法的基础上考虑了次邻接、第三邻接等信息。例如<span
class="math inline"><strong>A</strong><sup>2</sup>[<em>i</em>, <em>j</em>], <strong>A</strong><sup>3</sup>[<em>i</em>, <em>j</em>]⋯<strong>A</strong><sup><em>k</em></sup>[<em>i</em>, <em>j</em>]</span>表示图的二阶邻接、三阶邻接、<span
class="math inline"><em>k</em></span>阶邻接，表示距离为<span
class="math inline">2、3、<em>k</em></span>的节点连接情况。</p>
<h3 id="hope方法">HOPE方法</h3>
<p>高阶邻接保留嵌入算法（HOPE）是一种能够有向图的不对称传递性的网络嵌入算法。</p>
<p>高阶邻近性源自不对称传递性, 采用机器学习中最优化算法最小化损失函数:
<span class="math display">$$\begin{align}
\min \left\|\boldsymbol{S}-\boldsymbol{U}^{\boldsymbol{s}}
\boldsymbol{U}^{t^{\top}}\right\|^2
\end{align}$$</span></p>
<p>公式中, <span class="math inline"><strong>S</strong></span>
是网络高阶相似性测度指标值, <span
class="math inline"><strong>U</strong><sup><em>s</em></sup></span> 和
<span class="math inline"><strong>U</strong><sup><em>t</em></sup></span>
是每个网络节点嵌入向量组成的嵌入属性矩阵。</p>
<h2 id="基于随机游走的图嵌入算法">基于随机游走的图嵌入算法</h2>
<p>矩阵嵌入的算法局限在于矩阵的运算和复杂度较高，针对大型网络表现出一定局限性。为了提升效率以及高效采样，采用随机游走的思路。</p>
<h3 id="deepwalk算法">DeepWalk算法</h3>
<p>原始文献<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1403.6652">DeepWalk: Online
Learning of Social Representations</a></p>
<p>DeepWalk方法通过随机游走在网络上采样，获得网络节点和连边构成的序列数据，构造不同网络节点之间的相似性特征或者领域结构特征。</p>
<figure>
<img src="./3-3.png" alt="DeepWalk" />
<figcaption aria-hidden="true">DeepWalk</figcaption>
</figure>
<figure>
<img src="./3-4.png" alt="DeepWalk" />
<figcaption aria-hidden="true">DeepWalk</figcaption>
</figure>
<p>先初始化参数矩阵<span class="math inline"><em>Φ</em></span>为<span
class="math inline">|<em>V</em>|×<em>d</em></span>的矩阵。然后随机采样，减少网络之间的关联性。</p>
<p>从网络节点<span
class="math inline"><em>v</em><sub><em>i</em></sub></span>开始，采样长度为<span
class="math inline"><em>t</em></span>的样本，表示为： <span
class="math display">$$\begin{align}
W_{v_i} =&amp; \{ v_i,v_{i+1},v_{i+2}\cdots v_{i+t}\} \\
=&amp; \text{RandomWalk}(G, v_i, t)
\end{align}$$</span>
基于随机游走进行数据的采样，算法定义节点之间的领域关系。一般而言，如果两个节<span
class="math inline"><em>u</em><sub><em>k</em></sub>, <em>v</em><sub><em>j</em></sub></span>相邻，其同时出现在一条随机游走路径上的概率较高，因此目标函数定义为：
<span class="math display">$$\begin{align}
J(\Phi)=&amp;-\log{P_\Phi(u_k|v_j)} \\
P_\Phi(u_k|v_j) =&amp; \frac{\exp(\Phi_k^T\Phi_j)}{\sum_{u\in
V}\exp(\Phi_u^T\Phi_j)}
\end{align}$$</span></p>
<p>将嵌入词向量之间的关联性转化为概率值P_(u_k|v_j)，计算两节点同时出现在同一序列的概率值。</p>
<h3 id="node2vec算法">Node2Vec算法</h3>
<figure>
<img src="./3-5.png" alt="Node2Vec" />
<figcaption aria-hidden="true">Node2Vec</figcaption>
</figure>
<p>如上图，从节点<span
class="math inline"><em>v</em></span>跳转到候选节点<span
class="math inline"><em>t</em>, <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub></span>的概率为：
<span class="math display">$$\begin{align}
\pi_{vx}=\alpha_{pq}(t,x)w_{vx}
\end{align}$$</span> 其中<span
class="math inline"><em>w</em><sub><em>v</em><em>x</em></sub></span>表示节点<span
class="math inline"><em>v</em></span>与<span
class="math inline"><em>x</em></span>之间的权重，<span
class="math inline"><em>α</em><sub><em>p</em><em>q</em></sub>(<em>t</em><em>x</em>)</span>表示在超参数<span
class="math inline"><em>p</em>, <em>q</em></span>的情形下，从节点<span
class="math inline"><em>v</em></span>跳转到<span
class="math inline"><em>x</em></span>的非归一化概率，如上图所示。</p>
<p>其中包含超参数<span
class="math inline"><em>p</em>, <em>q</em></span>，如上图所示，<span
class="math inline"><em>v</em></span>节点是从<span
class="math inline"><em>t</em></span>转移过来跳回原节点<span
class="math inline"><em>t</em></span>的概率利用超参数<span
class="math inline"><em>p</em></span>设置概率为<span
class="math inline">$\frac{1}{p}$</span>；如果跳的节点是<span
class="math inline"><em>t</em></span>的二阶相邻节点，则利用超参数<span
class="math inline"><em>q</em></span>设置概率为<span
class="math inline">$\frac{1}{q}$</span>；如果同时是<span
class="math inline"><em>v</em>, <em>t</em></span>的超参数，则直接设为<span
class="math inline">1</span>。</p>
<p>由于这种超参数的设置，使得整个网络的采样是有偏的。</p>
<h1 id="图神经网络">图神经网络</h1>
<p>图神经网络（Graph Neural Networks,
GNN）模型的目标是挖掘图特征数据或信息，解构蕴含于复杂图结构和图网络以及相关特征属性的信息。在机器学习领域，该过程抽象的表达为表示学习过程，将高维、复杂信息抽象为低维的特征向量，获得特征向量更适应接下来的任务处理。本质上图神经网络就是图表示学习（Graph
Representation Learning, GPL）的有效方法。</p>
<h2 id="消息传递神经网络">消息传递神经网络</h2>
<p>消息传递神经网络（Message Passing Neural Networks,
MPNN），汇聚邻居节点的信息以更新网络节点的特征向量。主要包含领域信息汇聚函数、信息更新函数和信息池化函数等。本身还是出于消息传递算法的特性，考虑较短的圈，忽略更远节点的影响。</p>
<p>这种直接在图上进行信息处理的方法，也被叫做空间域图卷积神经网络。其中信息的聚合与信息的池化，对应卷积神经网络里面的卷积与池化。</p>
<h3 id="领域信息汇聚函数">领域信息汇聚函数</h3>
<p>网络 <span
class="math inline"><em>G</em> = (<em>V</em>, <em>E</em>)</span>
作为图神经网络模型的输入数据, 用于计算节点的隐含特征表示向量 <span
class="math inline"><strong>x</strong><sub><em>i</em></sub><sup>(<em>k</em>)</sup></span>
。节点 <span class="math inline"><em>i</em></span>
的特征属性表示向量是基于节点 <span class="math inline"><em>i</em></span>
的邻居节点集合 <span class="math inline">𝒩<sub><em>i</em></sub></span>
计算而来，具体计算公式如下： <span class="math display">$$\begin{align}
\boldsymbol{x}_{-i}^{(k)}=\text { Aggregate }_{j \in \mathcal{N}_i}
f^{(k)}\left(\boldsymbol{x}_j^{(k-1)}\right)
\end{align}$$</span></p>
<p>其中， <span
class="math inline"><strong>x</strong><sub>−<em>i</em></sub><sup>(<em>k</em>)</sup></span>
表示聚合了节点 <span class="math inline"><em>i</em></span>
的邻居节点集合 <span class="math inline">𝒩<sub><em>i</em></sub></span>
的属性表示，但是不包括网络节点 <span
class="math inline"><em>i</em></span> 自身的属性信息。</p>
<p>其中上标<span
class="math inline">(<em>k</em>)</span>表示汇聚节点深度，例如<span
class="math inline"><em>k</em> = 1</span>表示计算最近邻。聚合函数
Aggregate
可以有多种形式，除了传统的求和、均值等，也可以是神经网络。另一方面，信息处理函数<span
class="math inline"><em>f</em><sup>(<em>k</em>)</sup></span>也可以嵌套一些深度神经网络模型。</p>
<h3 id="信息更新函数">信息更新函数</h3>
<p>在图神经网络模型中，节点 <span class="math inline"><em>i</em></span>
聚合了邻居信息后，可以更新节点 <span
class="math inline"><em>i</em></span> 自身的特征属性: <span
class="math display"><strong>x</strong><sub><em>i</em></sub><sup>(<em>k</em>)</sup> = Update (<strong>x</strong><sub><em>i</em></sub><sup>(<em>k</em> − 1)</sup>, <strong>x</strong><sub>−<em>i</em></sub><sup>(<em>k</em>)</sup>),</span></p>
<p>其中， <span
class="math inline"><strong>x</strong><sub><em>i</em></sub><sup>(<em>k</em> − 1)</sup></span>
表示第 <span class="math inline"><em>k</em> − 1</span> 次迭代计算中节点
<span class="math inline"><em>i</em></span> 的特征表示。 Update
函数可以有多种形式, 最常见的更新函数 Update 是深度神经网络模型,
也可以是简单的求和函数 (SUM) 或者平均值函数 (MEAN) 等,
或者进行简单的向量拼接。</p>
<h3 id="图信息池化函数">图信息池化函数</h3>
<p>对节点属性进行池化操作: <span
class="math display"><strong>x</strong><sup>(<em>K</em>)</sup> = Pool<sub>∀<em>i</em></sub>(<strong>x</strong><sub><em>i</em></sub><sup>(<em>K</em>)</sup>).</span></p>
<p>最简单的池化操作就是将所有网络节点属性求和： <span
class="math display"><strong>x</strong><sup>(<em>K</em>)</sup> = SUM<sub>∀<em>i</em></sub>(<strong>x</strong><sub><em>i</em></sub><sup>(<em>K</em>)</sup>)</span></p>
<h3 id="应用">应用</h3>
<p>使用MPNN进行分子性质预测的项目<a
target="_blank" rel="noopener" href="https://github.com/chemprop/chemprop/tree/main">chemprop</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AtomMessagePassing</span>(MessagePassing, HyperparametersMixin):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;A :class:`AtomMessagePassing` encodes a batch of molecular graphs by passing messages along</span></span><br><span class="line"><span class="string">    atoms.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It implements the following operation:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        h_v^&#123;(0)&#125; &amp;= \tau \left( \mathbf&#123;W&#125;_i(x_v) \right) \\</span></span><br><span class="line"><span class="string">        m_v^&#123;(t)&#125; &amp;= \sum_&#123;u \in \mathcal&#123;N&#125;(v)&#125; h_u^&#123;(t-1)&#125; \mathbin\Vert e_&#123;uv&#125; \\</span></span><br><span class="line"><span class="string">        h_v^&#123;(t)&#125; &amp;= \tau\left(h_v^&#123;(0)&#125; + \mathbf&#123;W&#125;_h m_v^&#123;(t-1)&#125;\right) \\</span></span><br><span class="line"><span class="string">        m_v^&#123;(T)&#125; &amp;= \sum_&#123;w \in \mathcal&#123;N&#125;(v)&#125; h_w^&#123;(T-1)&#125; \\</span></span><br><span class="line"><span class="string">        h_v^&#123;(T)&#125; &amp;= \tau \left (\mathbf&#123;W&#125;_o \left( x_v \mathbin\Vert m_&#123;v&#125;^&#123;(T)&#125; \right)  \right),</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`\tau` is the activation function; :math:`\mathbf&#123;W&#125;_i`, :math:`\mathbf&#123;W&#125;_h`, and</span></span><br><span class="line"><span class="string">    :math:`\mathbf&#123;W&#125;_o` are learned weight matrices; :math:`e_&#123;vw&#125;` is the feature vector of the</span></span><br><span class="line"><span class="string">    bond between atoms :math:`v` and :math:`w`; :math:`x_v` is the feature vector of atom :math:`v`;</span></span><br><span class="line"><span class="string">    :math:`h_v^&#123;(t)&#125;` is the hidden representation of atom :math:`v` at iteration :math:`t`;</span></span><br><span class="line"><span class="string">    :math:`m_v^&#123;(t)&#125;` is the message received by atom :math:`v` at iteration :math:`t`; and</span></span><br><span class="line"><span class="string">    :math:`t \in \&#123;1, \dots, T\&#125;` is the number of message passing iterations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    d_v : int, default=DEFAULT_ATOM_FDIM</span></span><br><span class="line"><span class="string">        the feature dimension of the vertices</span></span><br><span class="line"><span class="string">    d_e : int, default=DEFAULT_BOND_FDIM</span></span><br><span class="line"><span class="string">        the feature dimension of the edges</span></span><br><span class="line"><span class="string">    d_h : int, default=DEFAULT_HIDDEN_DIM</span></span><br><span class="line"><span class="string">        the hidden dimension during message passing</span></span><br><span class="line"><span class="string">    bias : bool, defuault=False</span></span><br><span class="line"><span class="string">        if `True`, add a bias term to the learned weight matrices</span></span><br><span class="line"><span class="string">    depth : int, default=3</span></span><br><span class="line"><span class="string">        the number of message passing iterations</span></span><br><span class="line"><span class="string">    undirected : bool, default=False</span></span><br><span class="line"><span class="string">        if `True`, pass messages on undirected edges</span></span><br><span class="line"><span class="string">    dropout : float, default=0.0</span></span><br><span class="line"><span class="string">        the dropout probability</span></span><br><span class="line"><span class="string">    activation : str, default=&quot;relu&quot;</span></span><br><span class="line"><span class="string">        the activation function to use</span></span><br><span class="line"><span class="string">    d_vd : int | None, default=None</span></span><br><span class="line"><span class="string">        the dimension of additional vertex descriptors that will be concatenated to the hidden features before readout</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        d_v: <span class="built_in">int</span> = DEFAULT_ATOM_FDIM,</span></span><br><span class="line"><span class="params">        d_e: <span class="built_in">int</span> = DEFAULT_BOND_FDIM,</span></span><br><span class="line"><span class="params">        d_h: <span class="built_in">int</span> = DEFAULT_HIDDEN_DIM,</span></span><br><span class="line"><span class="params">        bias: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        depth: <span class="built_in">int</span> = <span class="number">3</span>,</span></span><br><span class="line"><span class="params">        dropout: <span class="built_in">float</span> = <span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        activation: <span class="built_in">str</span> | Activation = Activation.RELU,</span></span><br><span class="line"><span class="params">        undirected: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        d_vd: <span class="built_in">int</span> | <span class="literal">None</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        V_d_transform: ScaleTransform | <span class="literal">None</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        graph_transform: GraphTransform | <span class="literal">None</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        <span class="comment"># layers_per_message: int = 1,</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.save_hyperparameters()</span><br><span class="line">        <span class="variable language_">self</span>.hparams[<span class="string">&quot;cls&quot;</span>] = <span class="variable language_">self</span>.__class__</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.W_i, <span class="variable language_">self</span>.W_h, <span class="variable language_">self</span>.W_o, <span class="variable language_">self</span>.W_d = <span class="variable language_">self</span>.setup(d_v, d_e, d_h, d_vd, bias)</span><br><span class="line">        <span class="variable language_">self</span>.depth = depth</span><br><span class="line">        <span class="variable language_">self</span>.undirected = undirected</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.tau = get_activation_function(activation)</span><br><span class="line">        <span class="variable language_">self</span>.V_d_transform = V_d_transform <span class="keyword">if</span> V_d_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        <span class="variable language_">self</span>.graph_transform = graph_transform <span class="keyword">if</span> graph_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">output_dim</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.W_d.out_features <span class="keyword">if</span> <span class="variable language_">self</span>.W_d <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="variable language_">self</span>.W_o.out_features</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        d_v: <span class="built_in">int</span> = DEFAULT_ATOM_FDIM,</span></span><br><span class="line"><span class="params">        d_e: <span class="built_in">int</span> = DEFAULT_BOND_FDIM,</span></span><br><span class="line"><span class="params">        d_h: <span class="built_in">int</span> = DEFAULT_HIDDEN_DIM,</span></span><br><span class="line"><span class="params">        d_vd: <span class="built_in">int</span> | <span class="literal">None</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        bias: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="built_in">tuple</span>[nn.Module, nn.Module, nn.Module, nn.Module | <span class="literal">None</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;setup the weight matrices used in the message passing update functions</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        d_v : int</span></span><br><span class="line"><span class="string">            the vertex feature dimension</span></span><br><span class="line"><span class="string">        d_e : int</span></span><br><span class="line"><span class="string">            the edge feature dimension</span></span><br><span class="line"><span class="string">        d_h : int, default=300</span></span><br><span class="line"><span class="string">            the hidden dimension during message passing</span></span><br><span class="line"><span class="string">        d_vd : int | None, default=None</span></span><br><span class="line"><span class="string">            the dimension of additional vertex descriptors that will be concatenated to the hidden</span></span><br><span class="line"><span class="string">            features before readout, if any</span></span><br><span class="line"><span class="string">        bias: bool, default=False</span></span><br><span class="line"><span class="string">            whether to add a learned bias to the matrices</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        W_i, W_h, W_o, W_d : tuple[nn.Module, nn.Module, nn.Module, nn.Module | None]</span></span><br><span class="line"><span class="string">            the input, hidden, output, and descriptor weight matrices, respectively, used in the</span></span><br><span class="line"><span class="string">            message passing update functions. The descriptor weight matrix is `None` if no vertex</span></span><br><span class="line"><span class="string">            dimension is supplied</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        W_i = nn.Linear(d_v, d_h, bias)</span><br><span class="line">        W_h = nn.Linear(d_e + d_h, d_h, bias)</span><br><span class="line">        W_o = nn.Linear(d_v + d_h, d_h)</span><br><span class="line">        W_d = nn.Linear(d_h + d_vd, d_h + d_vd) <span class="keyword">if</span> d_vd <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> W_i, W_h, W_o, W_d</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">initialize</span>(<span class="params">self, bmg: BatchMolGraph</span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;initialize the message passing scheme by calculating initial matrix of hidden features&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.W_i(bmg.V[bmg.edge_index[<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">message</span>(<span class="params">self, H: Tensor, bmg: BatchMolGraph</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Calculate the message matrix&quot;&quot;&quot;</span></span><br><span class="line">        H = torch.cat((H, bmg.E), dim=<span class="number">1</span>)</span><br><span class="line">        index_torch = bmg.edge_index[<span class="number">1</span>].unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, H.shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="built_in">len</span>(bmg.V), H.shape[<span class="number">1</span>], dtype=H.dtype, device=H.device).scatter_reduce_(</span><br><span class="line">            <span class="number">0</span>, index_torch, H, reduce=<span class="string">&quot;sum&quot;</span>, include_self=<span class="literal">False</span></span><br><span class="line">        )[bmg.edge_index[<span class="number">0</span>]]          <span class="comment">#按照指定方式进行归约，本值为aggregate函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, M_t, H_0</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Calcualte the updated hidden for each edge&quot;&quot;&quot;</span></span><br><span class="line">        H_t = <span class="variable language_">self</span>.W_h(M_t)           <span class="comment">#通过神经网络从其它节点处传来的Message</span></span><br><span class="line">        H_t = <span class="variable language_">self</span>.tau(H_0 + H_t)     <span class="comment">#结合该节点，并进行激活函数激活</span></span><br><span class="line">        H_t = <span class="variable language_">self</span>.dropout(H_t)       <span class="comment">#dropout层，增加表达能力</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> H_t</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, bmg: BatchMolGraph, V_d: Tensor | <span class="literal">None</span> = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Encode a batch of molecular graphs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        bmg: BatchMolGraph</span></span><br><span class="line"><span class="string">            a batch of :class:`BatchMolGraph`s to encode</span></span><br><span class="line"><span class="string">        V_d : Tensor | None, default=None</span></span><br><span class="line"><span class="string">            an optional tensor of shape ``V x d_vd`` containing additional descriptors for each atom</span></span><br><span class="line"><span class="string">            in the batch. These will be concatenated to the learned atomic descriptors and</span></span><br><span class="line"><span class="string">            transformed before the readout phase.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        Tensor</span></span><br><span class="line"><span class="string">            a tensor of shape ``V x d_h`` or ``V x (d_h + d_vd)`` containing the encoding of each</span></span><br><span class="line"><span class="string">            molecule in the batch, depending on whether additional atom descriptors were provided</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        bmg = <span class="variable language_">self</span>.graph_transform(bmg)   <span class="comment">#输入图</span></span><br><span class="line">        H_0 = <span class="variable language_">self</span>.initialize(bmg)        <span class="comment">#初始化</span></span><br><span class="line"></span><br><span class="line">        H = <span class="variable language_">self</span>.tau(H_0)                 <span class="comment">#激活函数/池化层</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="variable language_">self</span>.depth):    <span class="comment">#按照设定深度进行采样</span></span><br><span class="line">            M = <span class="variable language_">self</span>.message(H, bmg)      <span class="comment">#消息传递</span></span><br><span class="line">            H = <span class="variable language_">self</span>.update(M, H_0)       <span class="comment">#更新</span></span><br><span class="line"></span><br><span class="line">        index_torch = bmg.edge_index[<span class="number">1</span>].unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, H.shape[<span class="number">1</span>])</span><br><span class="line">        M = torch.zeros(<span class="built_in">len</span>(bmg.V), H.shape[<span class="number">1</span>], dtype=H.dtype, device=H.device).scatter_reduce_(</span><br><span class="line">            <span class="number">0</span>, index_torch, H, reduce=<span class="string">&quot;sum&quot;</span>, include_self=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.finalize(M, bmg.V, V_d)</span><br></pre></td></tr></table></figure>
<h2 id="谱图卷积神经网络">谱图卷积神经网络</h2>
<p><font color='red'>这部分需要独立出来作为完整的一节重构</font></p>
<p>参考资料： * <a
target="_blank" rel="noopener" href="http://www.cs.yale.edu/homes/spielman/sgta/SpectTut.pdf">Spectral
Graph Theory and its Applications</a> * <a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/554955092">读书报告 | 谱图理论 Ch1:
Introduction</a> * <a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/290755442">图卷积网络原来是这么回事（一）——拉普拉斯矩阵初探</a>
* <a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/292935670">图卷积网络原来是这么回事（二）——图傅里叶变换及案例分析</a>
* <a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/297613044">图卷积网络原来是这么回事（三）——图滤波器与图卷积的设计</a>
* <a target="_blank" rel="noopener" href="https://mathweb.ucsd.edu/~fan/research/revised.html">SPECTRAL
GRAPH THEORY</a> * <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6203">Spectral
Networks and Locally Connected Networks on Graphs</a></p>
<p>谱图理论（Spectral Graph
Theory）是分析图的拉普拉斯矩阵特征值和特征向量及其对应图性质的理论。</p>
<p>考虑一个邻接矩阵，</p>
<figure>
<img src="./4-2-1.png" alt="邻接矩阵例子" />
<figcaption aria-hidden="true">邻接矩阵例子</figcaption>
</figure>
<p>可以写为 <span class="math display">$$
A=\left(\begin{array}{llll}
0 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 0
\end{array}\right)
$$</span> 不难发现矩阵<span
class="math inline"><em>A</em></span>是厄米矩阵<span
class="math inline"><em>A</em> = <em>A</em><sup>†</sup></span>，因此其具有共轭正交的本征矢，<span
class="math inline">∥<em>x</em>∥ = 1</span> 并且 <span
class="math inline"><em>x</em><sup><em>T</em></sup><em>A</em><em>x</em> = <em>λ</em></span>。将其记为：
<span class="math display">$$\begin{align}
A\mathbf{X} =&amp; \lambda \mathbf{X} \\
y=&amp;A x \\
y(i)= &amp; \sum_{j:(i, j) \in E} x(j) \\
x^T A x=&amp;\sum_{(i, j) \in E} 2x(i) x(j) \label{4-2-1} \\
\end{align}$$</span> 公式<span
class="math inline">$\eqref{4-2-1}$</span>中乘以2的原因在于<span
class="math inline">(<em>i</em>, <em>j</em>) ∈ <em>E</em></span>连接的边只计算一次，但是对称矩阵中计算两次。</p>
<p>接下来使用拉普拉斯矩阵衡量本征矢的平整性，这是因为拉普拉斯矩阵是图上的平方损失。
<span class="math display">$$\begin{align}
\boldsymbol{L}_G \stackrel{\text { def }}{=}&amp;
\boldsymbol{D}_G-\boldsymbol{M}_G \\
\boldsymbol{x}^T \boldsymbol{L}_G \boldsymbol{x}=&amp;\sum_{(a, b) \in
E} w_{a, b}(\boldsymbol{x}(a)-\boldsymbol{x}(b))^2
\end{align}$$</span> 如果<span
class="math inline"><strong>x</strong></span>没有很大的跳跃，值将会小。同时另一方面可以看出，拉普拉斯矩阵是半正定的。接下来假定拉普拉斯矩阵<span
class="math inline"><strong>L</strong><sub><em>G</em></sub></span>的本征值<span
class="math inline"><em>λ</em></span>按照从小到大排列 <span
class="math inline">0 = <em>λ</em><sub>1</sub> &lt; <em>λ</em><sub><em>k</em> − 1</sub> &lt; <em>λ</em><sub><em>k</em></sub></span>，代表从低频到高频的本征值信息。</p>
<p>为什么用“频”？接下来用一个实验说明，上面考虑的是包含4个节点的直链，接下来考虑10个节点直链，从左到右依次编号。计算其拉普拉斯矩阵的本征值与本征态并按照约定从小到大排序。其中<span
class="math inline"><em>λ</em> = 0</span>，其对应的本征态<span
class="math inline"><em>v</em><sub>0</sub></span>为一个常数。</p>
<figure>
<img src="./4-2-2.png" alt="说明频率问题" />
<figcaption aria-hidden="true">说明频率问题</figcaption>
</figure>
<p>上图中为横坐标为本征态的位置，因为有10个节点，本征态的长度为10，纵坐标表示每一个位置上代表的值的大小。其中<span
class="math inline"><em>v</em>2, <em>v</em>3, <em>v</em>4</span>分别对应<span
class="math inline"><em>λ</em><sub>1</sub>, <em>λ</em><sub>2</sub>, <em>λ</em><sub>3</sub></span>的本征态。可以看出<span
class="math inline"><em>v</em>2</span>较缓慢的变化，<span
class="math inline"><em>v</em>3, <em>v</em>4</span>的变化幅度增大。</p>
<figure>
<img src="./4-2-3.png" alt="v10频率" />
<figcaption aria-hidden="true">v10频率</figcaption>
</figure>
<p>上图是<span
class="math inline"><em>v</em>10</span>的变化情况，可以看出其变化幅度十分剧烈。</p>
<p>可以发现，以上表现出来的模式很相一根弦的振动模式，可以理解为这直链图是一根弦的离散化表示，而它的拉普拉斯矩阵是拉普拉斯算子的离散化。<font color='red'>有空补一下转化过程。</font>将把低频率的特征值与连通性联系起来。相比之下，最高频率的特征值在每个顶点处正负交替出现。高频率的特征向量可能与图着色问题和寻找独立集的问题相关。<font color='red'>如何与优化问题联系起来的，需要详细说明。</font></p>
<h3 id="图卷积">图卷积</h3>
<p>回到最开始的原因，为什么要在图结构上进行卷积操作，这里引用<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6203">最早提出图卷积方法文献</a>中给出的动机（该文章作者中有卷积神经网络的提出者之一Yann
LeCun）。</p>
<blockquote>
<p>Convolutional Neural Networks are extremely efficient architectures
in image and audio recognition tasks, thanks to their ability to exploit
the local translational invariance of signal classes over their
domain.</p>
</blockquote>
<p>为什么平时的卷积方式不能直接用在图上呢？</p>
<blockquote>
<p>Convolutional Neural Networks (CNNs) have been extremely succesful in
machine learning problems where the coordinates of the underlying data
representation have a grid structure (in 1, 2 and 3 dimensions), and the
data to be studied in those coordinates has translational
equivariance/invariance with respect to this grid.</p>
<p>In many contexts, however, one may be faced with data defined over
coordinates which lack some, or all, of the above geometrical
properties. Graphs offer a natural framework to generalize the
low-dimensional grid structure, and by extension the notion of
convolution.</p>
</blockquote>
<p>解决方法是什么呢？</p>
<blockquote>
<p>The other construction, which we call spectral construction, draws on
the properties of convolutions in the Fourier domain. This equivalence
is given through the graph Laplacian, an operator which provides an
harmonic analysis on the graphs.</p>
</blockquote>
<p>拉普拉斯矩阵是一个厄米矩阵，其本征值与频率有关，那么其本征态处于希尔伯特动量空间（频率空间，物理中称为动量空间、倒空间），给图加入了基矢与结构化数据。</p>
<p>虽然确定方向，但还需要解决两个问题： 1. 卷积真正的内核是什么？ 2.
如何构建在动量空间搭建卷积操作？</p>
<p>卷积, 在泛函中, 指使用一个算符(函数),
与另一个函数运算产生第三个函数。若对连续函数 <span
class="math inline"><em>f</em>(<em>x</em>), <em>g</em>(<em>x</em>)</span>
使用该运算, 可以使用积分公式: <span
class="math display">∫<sub>−∞</sub><sup>+∞</sup><em>f</em>(<em>τ</em>)<em>g</em>(<em>x</em> − <em>τ</em>)<em>d</em><em>τ</em></span></p>
<p>在卷积神经网络中, 我们见到的是离散二维卷积： <span
class="math display">$$
f[x, y] * g[x, y]=\sum_{i=-\infty}^{+\infty} \sum_{j=-\infty}^{+\infty}
f[i, j] \cdot g[x-i, y-j]
$$</span></p>
<p>上面神经网络的卷积通常是对欧式空间数据的操作,
例如图片数据。这样的二维离散卷积可以看作是在固定形状的矩阵 <span
class="math inline"><em>g</em>[<em>x</em>, <em>y</em>]<sub><em>M</em> × <em>N</em></sub></span>
上使用固定的卷积核 <span
class="math inline"><em>f</em>[<em>x</em>, <em>y</em>]<sub><em>a</em> × <em>b</em></sub></span>
进行对应位置相乘再求和(积分)。再联想CNN中的卷积操作，由此可见卷积的核心在于找到卷积核定义方式（也是需要学习参数的地方），给定一种移动方式（这个在CNN中是超参）。</p>
<p>接下来搭建动量空间的卷积操作。首先将图数据投影到动量空间。定义动量空间的基矢量：</p>
<p><span class="math display">$$
\begin{equation}
L=U \Lambda U^T=\left[\begin{array}{cccc}
\mid &amp; \mid &amp; \ldots &amp; \mid \\
\boldsymbol{v}_1 &amp; \boldsymbol{v}_2 &amp; \ldots &amp;
\boldsymbol{v}_n \\
\mid &amp; \mid &amp; \ldots &amp; \mid
\end{array}\right]\left[\begin{array}{cccc}
\lambda_1 &amp; &amp; &amp; \\
&amp; \lambda_2 &amp; &amp; \\
&amp; &amp; \ldots &amp; \\
&amp; &amp; &amp; \lambda_n
\end{array}\right]
\left[
\begin{array}{ccc}
- &amp; \boldsymbol{v}_1^T &amp; - \\
- &amp; \boldsymbol{v}_2^T &amp; - \\
- &amp; \ldots &amp; - \\
- &amp; \boldsymbol{v}_n^T &amp; -
\end{array}
\right] \label{laplace}
\end{equation}
$$</span></p>
<p><font color='yellow'>这里笔者犯了一个很蠢的错误，把邻接矩阵当作图数据。但是邻接矩阵是用于刻画相互作用关系的，每一个节点如何取值才是图数据。</font>定义图数据<span
class="math inline"><strong>x</strong> = [<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>⋯<em>x</em><sub><em>n</em></sub>]</span>，每一个数据<span
class="math inline"><em>x</em><sub><em>i</em></sub></span>为在图上<span
class="math inline"><em>i</em></span>节点的值。对于频率为<span
class="math inline"><em>λ</em><sub><em>j</em></sub></span>的基矢投影得到：</p>
<p><span class="math display">$$\begin{align}
F(\lambda_j) = \langle\boldsymbol{x}, \boldsymbol{v_j}\rangle =
\sum_{j=1}^n x_j v_i(j)
\end{align}$$</span></p>
<p>其中<span
class="math inline"><em>v</em><sub><em>i</em></sub>(<em>j</em>)</span>表示第<span
class="math inline"><em>i</em></span>个特征向量的第<span
class="math inline"><em>j</em></span>个分量，自然这个投影过程就是图傅里叶变换（Graph
Fourier Transform，GFT） <span class="math display">$$\begin{align}
\tilde{x}_k=\left\langle\boldsymbol{x}, \boldsymbol{v}_k\right\rangle,
k=1,2, \ldots, n
\end{align}$$</span></p>
<p>用矩阵形式可以计算出所有的傅里叶系数: <span
class="math display">$$\begin{align}
\tilde{\boldsymbol{x}}=U^T \boldsymbol{x}=\left[\begin{array}{ccc}
- &amp; \boldsymbol{v}_1^T &amp; - \\
- &amp; \boldsymbol{v}_2^T &amp; - \\
- &amp; \ldots &amp; - \\
- &amp; \boldsymbol{v}_n^T &amp; -
\end{array}\right] \quad \boldsymbol{x}, \quad \tilde{\boldsymbol{x}}
\in \boldsymbol{R}^n
\end{align}$$</span></p>
<p>由于拉普拉斯矩阵的特征向量 <span
class="math inline"><em>U</em></span> 是一组 <span
class="math inline"><em>n</em></span> 维空间中的完备的正交基, 对上式左乘
<span class="math inline"><em>U</em></span>, 则有 <span
class="math display">$$\begin{align}
\boldsymbol{x}= &amp; U \tilde{\boldsymbol{x}}=\left[\begin{array}{cccc}
\mid &amp; \mid &amp; \ldots &amp; \mid \\
\boldsymbol{v}_1 &amp; \boldsymbol{v}_2 &amp; \ldots &amp;
\boldsymbol{v}_n \\
\mid &amp; \mid &amp; \ldots &amp; \mid
\end{array}\right]\left[\begin{array}{c}
\tilde{x}_1 \\
\tilde{x}_2 \\
\ldots \\
\tilde{x}_n
\end{array}\right] \\
=&amp;\tilde{x}_1 \boldsymbol{v}_1+\tilde{x}_2
\boldsymbol{v}_2+\ldots+\tilde{x}_n \boldsymbol{v}_n \\
=&amp; \sum_{k=1}^n \tilde{x}_k \boldsymbol{v}_k
\end{align}$$</span></p>
<p>上式是一种矩阵形式的逆图傅里叶变换 (IGFT)。</p>
<p>接下来考虑在动量空间的卷积操作。给定图 <span
class="math inline"><em>G</em></span> 上的两组图信号 <span
class="math inline"><strong>x</strong><sub>1</sub></span> 和 <span
class="math inline"><strong>x</strong><sub>2</sub></span>,
图卷积运算的定义如下: <span class="math display">$$\begin{align}
\boldsymbol{x}_1 *
\boldsymbol{x}_2=\operatorname{IGFT}\left(\operatorname{GFT}\left(\boldsymbol{x}_1\right)
\odot \operatorname{GFT}\left(\boldsymbol{x}_2\right)\right)
\end{align}$$</span></p>
<p>其中, <span class="math inline">⊙</span>
表示两个向量的逐元素相乘。这里的定义与离散信号处理中的卷积定义是一样的，时域中的卷积运算等价于频域中的乘法运算。</p>
<p>继续对上式进行推导: <span class="math display">$$\begin{align}
\boldsymbol{x}_1 * \boldsymbol{x}_2=&amp; U\left[\left(U^T
\boldsymbol{x}_1\right) \odot\left(U^T \boldsymbol{x}_2\right)\right] \\
=&amp; U\left[\tilde{\boldsymbol{x}}_1 \odot\left(U^T
\boldsymbol{x}_2\right)\right] \\
=&amp; U \operatorname{diag}\left(\tilde{\boldsymbol{x}}_1\right)
\mathrm{U}^{\mathrm{T}} \boldsymbol{x}_2
\end{align}$$</span></p>
<p>令<span class="math inline">$H_{\tilde{\boldsymbol{x}}_1}=U
\operatorname{diag}\left(\tilde{\boldsymbol{x}}_1\right)
\mathrm{U}^{\mathrm{T}} \boldsymbol{x}_2$</span>
为卷积核，图信号的卷积操作为: <span class="math display">$$\begin{align}
\boldsymbol{x}_1 * \boldsymbol{x}_2=H_{\tilde{\boldsymbol{x}}_1}
\boldsymbol{x}_2
\end{align}$$</span></p>
<p><span
class="math inline">H = U<em>Λ</em><sub>h</sub>U<sup>T</sup></span>
的核心在于中间对角化的本征值 <span
class="math inline"><em>Λ</em><sub><em>h</em></sub></span>，那么对应于CNN的卷积核，自然会想到<span
class="math inline">H</span>进行参数化,
本质是通过训练的方式得出一组<span
class="math inline"><em>Λ</em><sub><em>h</em></sub></span>，来让图卷积自动地调整和取舍不同本征态的信息，从而提取出有用的图特征。</p>
<p>故设计如下的图卷积层: <span class="math display">$$\begin{align}
Y=&amp;\sigma\left(U\left[\begin{array}{cccc}
\theta_1 &amp; &amp; &amp; \\
&amp; \theta_2 &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; \theta_n
\end{array}\right] \quad U^T X\right) \label{theta}\\
=&amp;\sigma\left(U \operatorname{diag}(\theta) \mathrm{U}^{\mathrm{T}}
\mathrm{X}\right)\\
=&amp;\sigma(\Theta \mathrm{X})
\end{align}$$</span></p>
<p>其中, <span
class="math inline"><em>X</em> ∈ <em>R</em><sup><em>n</em> × <em>f</em></sup></span>
是输入的图信号矩阵, <span
class="math inline"><em>θ</em> = [<em>θ</em><sub>1</sub>, <em>θ</em><sub>2</sub>, …<em>θ</em><sub><em>n</em></sub>]</span>
是需要学习的参数, <span class="math inline"><em>Θ</em></span>
是需要学习的图卷积核, <span class="math inline"><em>σ</em>(⋅)</span>
是激活函数, 而 <span class="math inline"><em>Y</em></span>
是输出的图信号矩阵。</p>
<p>回顾以上思路，首先面对图类型数据由于没有结构化的表示方法，因此将其转入动量空间中进行表示（此操作称为傅里叶变化）；接下来利用基矢量的正交归一性，定义动量空间的卷积操作；然后参数化构造卷积核；最后再将其动量空间转化为图数据，增加激活函数。</p>
<p>但是这样的操作也存在一些缺点，卷积层的参数个数与节点数相等。在上亿节点数规模的图训练任务重，这样的网络极易发生过拟合问题，因为Yann
以及后续的大部分成果都指出：图的有效信息往往蕴含在低频段，没有必要为每一个频段训练一个参数。<font color='red'>有待进一步调研</font></p>
<h3 id="k阶截断多项式算子">K阶截断多项式算子</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/0912.3848">Wavelets on Graphs via
Spectral Graph Theory</a></li>
</ul>
<p>为了解决参数过多过拟合的问题，那就通过截断将参数规模下降，在数学上称为小波变换（Wavelet
Transform）。</p>
<p>观察<span class="math inline">$\eqref{laplace}$</span>和<span
class="math inline">$\eqref{theta}$</span>，其中<span
class="math inline"><em>λ</em></span>是由拉普拉斯矩阵得出，而<span
class="math inline"><em>θ</em></span>是卷积学习参数，可以通过用<span
class="math inline"><em>λ</em></span>的<span
class="math inline"><em>K</em></span>阶表示<span
class="math inline"><em>θ</em></span>来减少学习参数： <span
class="math display">$$\begin{align}
\Lambda(\boldsymbol{h})=&amp;\sum_{i=0}^K h_i \lambda_k^i \\
=&amp;\left[\begin{array}{cccc}
\sum_{i=0}^K h_i \lambda_1^i &amp; &amp; &amp; \\
&amp; \sum_{i=0}^K h_i \lambda_2^i &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; \sum_{i=0}^K h_i \lambda_N^i
\end{array}\right]
\end{align}$$</span></p>
<p>这样通过学习参数<span
class="math inline"><strong>h</strong></span>减少学习的参数量。该变换在空域同样具有含义，指的是，对于一个节点仅仅考虑第<span
class="math inline"><em>K</em></span>阶近邻的节点。</p>
<h3 id="切比雪夫多项式滤波算子">切比雪夫多项式滤波算子</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.09375">Convolutional Neural
Networks on Graphs with Fast Localized Spectral Filtering</a></li>
</ul>
<p>在<span
class="math inline"><em>K</em></span>阶截断多项式算子中，将本征值作为一种基进行展开，然后学习<span
class="math inline"><em>K</em></span>个参数<span
class="math inline"><em>h</em></span>得到最后的卷积核。那么本征值的选取只要是负数且不同的即可，并没有必要准确计算出来。因此，使用切比雪夫多项式结果代替本征值。</p>
<p>切比雪夫多项式来源于 <span class="math inline"><em>n</em></span>
倍角公式, 其每一项可以通过迭代的方式求出来。其第 <span
class="math inline"><em>k</em></span> 项满足: <span
class="math display">$$\begin{align}
T_k(x)=&amp;2 x T_{k-1}(x)-T_{k-2}(x)\\
T_0=&amp;1 \quad T_1=x
\end{align}$$</span> 由于切比雪夫多项式<span
class="math inline"><em>x</em> ∈ [−1, 1]</span>，因此对拉普拉斯矩阵进行变换（<span
class="math inline"><em>λ</em><sub>max</sub></span>为最大的本征值）：
<span class="math display">$$\begin{align}
\bar{\Lambda}=&amp;\frac{2 \Lambda}{\lambda_{\max }}-I \\
\bar{L}=&amp;\frac{2 L}{\lambda_{\max }}-I
\end{align}$$</span></p>
<p>于是, 我们用截断的 <span class="math inline"><em>K</em></span>
阶切比雪夫多项式来表示图卷积操作:</p>
<p><span class="math display">$$\begin{align}
\Lambda(\boldsymbol{h})=&amp;\sum_{i=0}^K h_i T_i(\bar{\Lambda})
\end{align}$$</span></p>
<h3 id="图卷积神经网络">图卷积神经网络</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.02907">Semi-Supervised
Classification with Graph Convolutional Networks</a></li>
</ul>
<p>图卷积神经网络（Graph Convolutional Network, GCN）</p>
<p>在实际运用中，如果<span
class="math inline"><em>K</em></span>值取得过高，则对于包含度数较大的节点的图来说，一个卷积层的感受野很有可能直接覆盖了几乎整张图。这样的话，即使堆叠几层卷积层，后续的卷积层的感受野仍然是整张图，重复执行全局平均操作，最终导致输出的图信号过平滑。</p>
<p>为此进一步近似，令<span
class="math inline"><em>K</em> = 1, <em>λ</em><sub>max</sub> = 2</span>，得到一阶切比雪夫多项式近似：</p>
<p><span class="math display">$$\begin{align}
\Lambda(\boldsymbol{h})=&amp;\sum_{i=0}^2 h_i T_i(\bar{\Lambda}) \\
=&amp; h_0 T_0(\bar{\Lambda}) + h_1 T_1(\bar{\Lambda}) \\
=&amp; h_0 \boldsymbol{I} + h_1 \bar{\Lambda} \\
=&amp; h_0 \boldsymbol{I} + h_1
(\frac{2\Lambda}{\lambda_\max}-\boldsymbol{I}) \\
\end{align}$$</span></p>
<p>利用<span
class="math inline">$\eqref{normal_laplace}$</span>，可以将输出转化为：
<span class="math display">$$\begin{align}
\boldsymbol{y}=&amp;h_0 \boldsymbol{x}+h_1\left(L-I_n\right)
\boldsymbol{x} \\
=&amp;h_0 \boldsymbol{x}-h_1 D^{-\frac{1}{2}} W D^{-\frac{1}{2}}
\boldsymbol{x}
\end{align}$$</span> 设 <span
class="math inline"><em>h</em><sub>0</sub> = −<em>h</em><sub>1</sub> = <em>h</em></span>,
则 <span class="math display">$$\begin{align}
\boldsymbol{y}=h\left(I_n+D^{-\frac{1}{2}} W D^{-\frac{1}{2}}\right)
\boldsymbol{x}=H \boldsymbol{x}
\end{align}$$</span></p>
<p>因为 <span class="math inline">$I_n+D^{-\frac{1}{2}} W
D^{-\frac{1}{2}}=2 I_n-\bar{L}$</span> 的特征值范围是 <span
class="math inline">[0, 2]</span>,
连续堆叠这样的卷积层相当于引入了频率响应函数 <span
class="math inline">(2 − <em>λ̄</em><sub><em>i</em></sub>)<sup><em>K</em></sup></span>,
会容易过度放大 <span
class="math inline"><em>λ̄</em><sub><em>i</em></sub> &lt; 1</span>
频段的信号, 进而可能会引发某些参数梯度爆炸而另外一些参数梯度消失。</p>
<p><font color='red'>未完待续</font></p>
<h1 id="图网络几何等变性">图网络几何等变性</h1>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2202.07230">Geometrically Equivariant
Graph Neural Networks: A Survey</a></li>
</ul>
<p>这篇综述给出研究几何等变性的动机十分精确： &gt;
在物理学和化学等领域中，许多问题需要处理形式为几何图形的数据 [Bronstein
et al.,
2021]。与一般的图形数据不同，几何图形不仅为每个节点分配一个特征，还分配一个几何向量。例如，可以将分子/蛋白质视为一个几何图形，其中原子的3D位置坐标是几何向量；或者在一个一般的多体物理系统中，粒子的3D状态（位置、速度或自旋）是几何向量。值得注意的是，几何图形展现出平移、旋转和/或反射的对称性。这是因为控制原子（或粒子）动态的物理定律无论我们如何将分子（或一般的物理系统）从一个地方平移或旋转到另一个地方都是相同的。在处理这类数据时，将对称性的归纳偏差融入到模型设计中是至关重要的，这激发了对具有几何等变性的图神经网络（GNNs）的研究。</p>
<p>然后这篇综述描述了在消息传递下的等变性问题。</p>
<p>令<span class="math inline">𝒳</span>和<span
class="math inline">𝒴</span>分别为输入和输出向量空间，它们都赋予了一组变换<span
class="math inline"><em>G</em> : <em>G</em> × 𝒳 → 𝒳</span>和<span
class="math inline"><em>G</em> × 𝒴 → 𝒴</span>。如果函数<span
class="math inline"><em>ϕ</em> : 𝒳 → 𝒴</span>在我们对输入应用任何变换时，输出也通过相同的变换或某种可预测的行为发生变化，则称该函数相对于<span
class="math inline"><em>G</em></span>是<strong>等变</strong>的。具体来说，有：</p>
<p><strong>定义 1（等变性）</strong> 函数<span
class="math inline"><em>ϕ</em> : 𝒳 ↦ 𝒴</span>是<span
class="math inline"><em>G</em></span>-等变的，如果它与<span
class="math inline"><em>G</em></span>中的任何变换交换， <span
class="math display">$$\begin{align}
\phi\left(\rho_{\mathcal{X}}(g) x\right)=\rho_{\mathcal{Y}}(g) \phi(x),
\quad \forall g \in G \label{transformation}
\end{align}$$</span> 其中<span
class="math inline"><em>ρ</em><sub>𝒳</sub></span>和<span
class="math inline"><em>ρ</em><sub>𝒴</sub></span>分别是输入空间和输出空间中的群表示。特别是，如果<span
class="math inline"><em>ρ</em><sub>𝒴</sub></span>是恒等的，则称<span
class="math inline"><em>ϕ</em></span>是<strong>不变</strong>的。</p>
<p><strong>定义 2（群）</strong> 群<span
class="math inline"><em>G</em></span>是一组变换，具有满足以下属性的二元操作“<span
class="math inline">⋅</span>“在关联复合下是封闭的，存在一个单位元，且<span
class="math inline"><em>G</em></span>中的每个元素必须有一个逆元。</p>
<p>根据群的定义，我们在这里提供一些例子： - <span
class="math inline">O(<em>n</em>)</span>是一个<span
class="math inline"><em>n</em></span>维正交群，由旋转和反射组成。 -
<span
class="math inline">SO(<em>n</em>)</span>是一个特殊正交群，仅由旋转组成。
- <span class="math inline">E(<em>n</em>)</span>是一个<span
class="math inline"><em>n</em></span>维欧几里得群，由旋转、反射和平移组成。
- <span
class="math inline">SE(<em>n</em>)</span>是一个特殊欧几里得群，由旋转和平移组成。
-
李群是其元素构成可微流形的群。实际上，上面所有的群都是李群的具体例子。</p>
<p><strong>群表示</strong> 一个群的表示是一个可逆的线性映射<span
class="math inline"><em>ρ</em>(<em>g</em>) : <em>G</em> ↦ 𝒱</span>，它接受群元素<span
class="math inline"><em>g</em> ∈ <em>G</em></span>作为输入，并在向量空间<span
class="math inline">𝒱</span>上作用。同时，它是线性的：<span
class="math inline"><em>ρ</em>(<em>g</em>)<em>ρ</em>(<em>h</em>) = <em>ρ</em>(<em>g</em> ⋅ <em>h</em>), ∀<em>g</em>, <em>h</em> ∈ <em>G</em></span>。例如，对于<span
class="math inline">O(<em>n</em>)</span>的矩阵表示是正交矩阵<span
class="math inline"><strong>O</strong> ∈ ℝ<sup><em>n</em></sup></span>，满足<span
class="math inline"><strong>O</strong><sup>⊤</sup><strong>O</strong> = <strong>I</strong></span>。在<span
class="math inline">O(3)</span>上实例化 <span
class="math inline">$\eqref{transformation}$</span> 变为<span
class="math inline"><em>ϕ</em>(<strong>O</strong><em>x</em>) = <strong>O</strong><em>ϕ</em>(<em>x</em>)</span>，如果输入和输出空间共享相同的表示。对于平移等价性，我们有<span
class="math inline"><em>ϕ</em>(<em>x</em> − <strong>t</strong>) = <em>ϕ</em>(<em>x</em>) − <strong>t</strong></span>，其中<span
class="math inline"><em>t</em> ∈ ℝ<sup><em>n</em></sup></span>。</p>
<figure>
<img src="./5-1.png"
alt="An illustration of the geometrically equivariant" />
<figcaption aria-hidden="true">An illustration of the geometrically
equivariant</figcaption>
</figure>
<p>上图是在消息传递图网络上，展示旋转等变性。左边是通常的消息传递过程，首先进行消息传递，然后进行聚合得出最后的结果。有边的图最开始是由左图进行旋转得到，然后同样经过消息传递与聚合操作，得出的结果与左图进行旋转结果相同。由此说明了消息传递的图神经网络的几何等变性。</p>
<p>在消息传递GNN中，实现等变性，主要有以下三种思路： *
使用群论中的不可约表示，例如对于<span
class="math inline">SO(3)</span>群，使用 Wigner-D matrices 进行表示。 *
通过李群和李代数，对之前参见的操作进行修改。例如将卷积修改为李卷积(LieConv)。
*
通过标量化，将几何向量首先被转换为不变的标量，然后通过几个多层感知机（MLPs）来控制大小，最后在原始方向上相加以获得等变性。</p>
<h1 id="hypergraph">Hypergraph</h1>
<p>超图（Hypergraph）是<strong>一种高维的图形结构</strong>，用于表示对象之间的复杂关系。</p>
<p>超图的核心概念在于它包含的“超边”，这是与普通图的边不同的概念。在普通图中，一条边连接两个顶点，而在超图中，一条<strong>超边可以连接多个顶点</strong>（包括两个以上）。这种结构使得超图能够表示更为复杂的关系，比如社交网络中一个人可能同时属于多个社交圈，或者在生态系统中一个物种可能在多个栖息地中出现。</p>
<p>超图由两部分组成：一组顶点集合 <span class="math inline">𝒱</span>
和一组超边集合 <span class="math inline">ℰ</span>
，超图可以表示为这些集合的组合 <span
class="math inline">ℋ = (𝒱, ℰ)</span> 。在超图中，如果一个顶点 <span
class="math inline"><em>v</em></span> 出现在一个超边 <span
class="math inline"><em>e</em></span> 中，那么我们说顶点 <span
class="math inline"><em>v</em></span> 与超边 <span
class="math inline"><em>e</em></span> 相关联。</p>
<h2 id="special-hypergraph-neural-networks">Special Hypergraph Neural
Networks</h2>
<ul>
<li><a
target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1609/aaai.v33i01.33013558">Hypergraph
Neural Networks</a> <a
target="_blank" rel="noopener" href="https://github.com/iMoonLab/HGNN">代码仓库Github</a> <a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/653917458">文章解读</a></li>
<li><a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/6287378">Learning
with Hypergraphs: Clustering, Classification, and Embedding</a> <img
src="./6-1.png" alt="Hypergraph" /></li>
</ul>
<p>针对超图这种复杂的结构，作者提出了 hypergraph neural networks (HGNN)
框架。</p>
<figure>
<img src="./6-2.png"
alt="The comparison between graph and hypergraph" />
<figcaption aria-hidden="true">The comparison between graph and
hypergraph</figcaption>
</figure>
<p>由上图可见，左边的<span
class="math inline"><em>W</em></span>为邻接矩阵，右边表示含有超边的图。该图的每一行代表一个节点，每一列表示一条超边，例如<span
class="math inline"><em>e</em><sub>1</sub></span>这条超边与<span
class="math inline"><em>n</em><sub>2</sub>, <em>n</em><sub>4</sub>, <em>n</em><sub>8</sub></span>相连，在矩阵上只有这三行为<span
class="math inline">1</span>。这种数据结构可以将任意的超边包含在内。<font color='red'>这种方式十分有效，但是之后如何进行嵌入呢？这样看只能在空域运算；在谱域，没有邻接矩阵，如何得到拉普拉斯矩阵？</font></p>
<p>首先将超图定义为<span
class="math inline">𝒢 = (𝒱，ℰ, <strong>W</strong>)</span>，其中顶点集合为<span
class="math inline">𝒱</span>，超边集为<span
class="math inline">ℰ</span>，每一个超边给予一个权重<span
class="math inline"><strong>W</strong></span>。超图<span
class="math inline">𝒢</span>可以通过<span
class="math inline">|𝒱|×|ℰ|</span>的矩阵<span
class="math inline"><strong>H</strong></span>定义为：</p>
<p><span class="math display">$$\begin{align}
h(v, e)= \begin{cases}1, &amp; \text { if } v \in e \\ 0, &amp; \text {
if } v \notin e,\end{cases}
\end{align}$$</span></p>
<p>接下来考虑节点的分类任务，参考<a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/6287378">Learning
with Hypergraphs: Clustering, Classification, and
Embedding</a>，利用正则化框架： <span class="math display">$$
\begin{align}
\arg \min _f\left\{\mathcal{R}_{\text{e m p}}(f)+\Omega(f)\right\},
\end{align}
$$</span></p>
<p>其中 <span class="math inline"><em>Ω</em>(<em>f</em>)</span>
是正则化项，<span class="math inline">ℛ<sub>emp</sub>(<em>f</em>)</span>
是经验损失，<span class="math inline"><em>f</em>(⋅)</span>
表示分类函数。</p>
<p><span class="math display">$$
\begin{align}
\Omega(f) = \frac{1}{2} \sum_{e \in \mathcal{E}} \sum_{\substack{u, v
\in \mathcal{V}}}
\frac{w(e)\, h(u, e)\, h(v, e)}{\delta(e)}
\left( \frac{f(u)}{\sqrt{d(u)}} - \frac{f(v)}{\sqrt{d(v)}} \right)^2
\end{align}
$$</span></p>
<p>令<span
class="math inline"><em>θ</em> = <strong>D</strong><sub><em>v</em></sub><sup>−1/2</sup><strong>H</strong><strong>W</strong><strong>D</strong><sub><em>e</em></sub><sup>−1</sup><strong>H</strong><sup>⊤</sup><strong>D</strong><sub><em>v</em></sub><sup>−1/2</sup></span>
和 <span
class="math inline"><strong>Δ</strong> = <strong>I</strong> − <strong>Θ</strong></span>，那么正则化项<span
class="math inline"><em>Ω</em>(<em>f</em>)</span> 可以写为： <span
class="math display"><em>Ω</em>(<em>f</em>) = <em>f</em><sup>⊤</sup><strong>Δ</strong>,</span>
其中<span class="math inline"><strong>Δ</strong></span>
是版正定的通常被叫为超图上的拉普拉斯矩阵（hypergraph Laplacian）。</p>
<p>剩下的操作和之前的一致，找到其本征态定义为傅里叶变换矩阵，然后利用本征值作为滤波函数，引入学习参数。为了简化，将滤波函数用切比雪夫多项式表示。最后完成特征提取。</p>
<h2 id="hypergraph-embedding-based-on-random-walk">Hypergraph Embedding
Based on Random Walk</h2>
<h2 id="random-walks-on-random-hypergraph">Random Walks on Random
Hypergraph</h2>
<h2
id="random-walks-and-laplacians-on-hypergraphs-when-do-they-match">Random
walks and Laplacians on hypergraphs: When do they match?</h2>
<h2 id="message-passing-on-hypergraphs">Message-Passing on
Hypergraphs</h2>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Graphs/" rel="tag"># Graphs</a>
              <a href="/tags/Complex-Network/" rel="tag"># Complex Network</a>
              <a href="/tags/Laplacian-Matrix/" rel="tag"># Laplacian Matrix</a>
              <a href="/tags/Hypergraph-Neural-Networks/" rel="tag"># Hypergraph Neural Networks</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/04/15/Math/Ising_formulations_of_many_NP_problems/Ising_formulations_of_many_NP_problems/" rel="prev" title="Ising formulations of many NP problems">
      <i class="fa fa-chevron-left"></i> Ising formulations of many NP problems
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/04/17/DL/Survey_MCTS/Survey_MCTS/" rel="next" title="A Survey of Monte Carlo Tree Search Methods">
      A Survey of Monte Carlo Tree Search Methods <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#why-graphs"><span class="nav-number">1.</span> <span class="nav-text">Why Graphs</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="nav-number">1.1.</span> <span class="nav-text">数据结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#what-types-of-problems-have-graph-structured-data"><span class="nav-number">1.2.</span> <span class="nav-text">What types of
problems have graph structured data?</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#graphs-with-complex-network"><span class="nav-number">2.</span> <span class="nav-text">Graphs with Complex Network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-number">2.1.</span> <span class="nav-text">图的定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E7%9A%84%E6%80%A7%E8%B4%A8"><span class="nav-number">2.2.</span> <span class="nav-text">图的性质</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%82%BB%E6%8E%A5%E8%8A%82%E7%82%B9neighbors"><span class="nav-number">2.2.1.</span> <span class="nav-text">邻接节点（neighbors）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%9A%84%E5%BA%A6degree"><span class="nav-number">2.2.2.</span> <span class="nav-text">图的度（degree）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%8C%E8%B5%B0walk%E5%92%8C%E8%B7%AF%E5%BE%84path"><span class="nav-number">2.2.3.</span> <span class="nav-text">行走（walk）和路径（path）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B7%9D%E7%A6%BBdistance%E5%92%8C%E7%9B%B4%E5%BE%84diameter"><span class="nav-number">2.2.4.</span> <span class="nav-text">距离（distance）和直径（diameter）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%90%E5%9B%BEsubgraph%E8%BF%9E%E9%80%9A%E5%88%86%E9%87%8Fconnected-component%E8%BF%9E%E9%80%9A%E5%9B%BEconected-graph"><span class="nav-number">2.2.5.</span> <span class="nav-text">子图（subgraph）、连通分量（connected
component）、连通图（conected graph）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB%E7%B3%BB%E6%95%B0clustering-coefficient"><span class="nav-number">2.2.6.</span> <span class="nav-text">聚类系数（Clustering
Coefficient）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A5%E8%BF%91%E4%B8%AD%E5%BF%83%E5%BA%A6closeness-centrality"><span class="nav-number">2.2.7.</span> <span class="nav-text">接近中心度（closeness
centrality）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E7%9A%84%E8%BF%9E%E6%8E%A5%E8%A1%A8%E7%A4%BA"><span class="nav-number">2.3.</span> <span class="nav-text">图的连接表示</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%82%BB%E6%8E%A5%E7%9F%A9%E9%98%B5"><span class="nav-number">2.3.1.</span> <span class="nav-text">邻接矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E8%81%94%E7%9F%A9%E9%98%B5"><span class="nav-number">2.3.2.</span> <span class="nav-text">关联矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%9F%A9%E9%98%B5"><span class="nav-number">2.3.3.</span> <span class="nav-text">拉普拉斯矩阵</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.4.</span> <span class="nav-text">图的类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%9A%84%E6%8B%93%E6%89%91%E7%BB%93%E6%9E%84"><span class="nav-number">2.4.1.</span> <span class="nav-text">图的拓扑结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8C%E8%B4%A8%E5%9B%BEhomogeneous-graph%E5%92%8C%E5%BC%82%E8%B4%A8%E5%9B%BEheterogeneous-graph"><span class="nav-number">2.4.2.</span> <span class="nav-text">同质图（Homogeneous
Graph）和异质图（Heterogeneous Graph）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E5%88%86%E5%9B%BE-bipartite-graph"><span class="nav-number">2.4.3.</span> <span class="nav-text">二分图 （bipartite graph）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8A%82%E7%82%B9%E6%8C%87%E6%A0%87"><span class="nav-number">2.5.</span> <span class="nav-text">节点指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%8A%82%E7%82%B9%E7%9A%84%E5%BC%BA%E5%BA%A6"><span class="nav-number">2.5.1.</span> <span class="nav-text">节点的强度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%9A%E7%B0%87%E7%B3%BB%E6%95%B0"><span class="nav-number">2.5.2.</span> <span class="nav-text">聚簇系数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E8%BF%9E%E8%BE%B9%E6%8C%87%E6%A0%87"><span class="nav-number">2.6.</span> <span class="nav-text">网络连边指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E6%A8%A1%E4%BD%93%E7%BB%93%E6%9E%84"><span class="nav-number">2.7.</span> <span class="nav-text">网络模体结构</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#graph-representation-learning"><span class="nav-number">3.</span> <span class="nav-text">Graph Representation
Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E8%A1%A8%E7%A4%BA%E4%B8%80%E8%88%AC%E6%A1%86%E6%9E%B6"><span class="nav-number">3.1.</span> <span class="nav-text">图表示一般框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-number">3.1.1.</span> <span class="nav-text">编码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-number">3.1.2.</span> <span class="nav-text">解码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96"><span class="nav-number">3.1.3.</span> <span class="nav-text">模型优化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E7%9A%84%E5%9B%BE%E5%B5%8C%E5%85%A5"><span class="nav-number">3.2.</span> <span class="nav-text">基于矩阵分解的图嵌入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%88%86%E8%A7%A3%E6%96%B9%E6%B3%95"><span class="nav-number">3.2.1.</span> <span class="nav-text">图分解方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#grarep%E6%96%B9%E6%B3%95"><span class="nav-number">3.2.2.</span> <span class="nav-text">GraRep方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hope%E6%96%B9%E6%B3%95"><span class="nav-number">3.2.3.</span> <span class="nav-text">HOPE方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0%E7%9A%84%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95"><span class="nav-number">3.3.</span> <span class="nav-text">基于随机游走的图嵌入算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#deepwalk%E7%AE%97%E6%B3%95"><span class="nav-number">3.3.1.</span> <span class="nav-text">DeepWalk算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#node2vec%E7%AE%97%E6%B3%95"><span class="nav-number">3.3.2.</span> <span class="nav-text">Node2Vec算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.</span> <span class="nav-text">图神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.1.</span> <span class="nav-text">消息传递神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%86%E5%9F%9F%E4%BF%A1%E6%81%AF%E6%B1%87%E8%81%9A%E5%87%BD%E6%95%B0"><span class="nav-number">4.1.1.</span> <span class="nav-text">领域信息汇聚函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E6%9B%B4%E6%96%B0%E5%87%BD%E6%95%B0"><span class="nav-number">4.1.2.</span> <span class="nav-text">信息更新函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E4%BF%A1%E6%81%AF%E6%B1%A0%E5%8C%96%E5%87%BD%E6%95%B0"><span class="nav-number">4.1.3.</span> <span class="nav-text">图信息池化函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%94%E7%94%A8"><span class="nav-number">4.1.4.</span> <span class="nav-text">应用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B0%B1%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.2.</span> <span class="nav-text">谱图卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%8D%B7%E7%A7%AF"><span class="nav-number">4.2.1.</span> <span class="nav-text">图卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#k%E9%98%B6%E6%88%AA%E6%96%AD%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%AE%97%E5%AD%90"><span class="nav-number">4.2.2.</span> <span class="nav-text">K阶截断多项式算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%87%E6%AF%94%E9%9B%AA%E5%A4%AB%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%BB%A4%E6%B3%A2%E7%AE%97%E5%AD%90"><span class="nav-number">4.2.3.</span> <span class="nav-text">切比雪夫多项式滤波算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.2.4.</span> <span class="nav-text">图卷积神经网络</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%BE%E7%BD%91%E7%BB%9C%E5%87%A0%E4%BD%95%E7%AD%89%E5%8F%98%E6%80%A7"><span class="nav-number">5.</span> <span class="nav-text">图网络几何等变性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#hypergraph"><span class="nav-number">6.</span> <span class="nav-text">Hypergraph</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#special-hypergraph-neural-networks"><span class="nav-number">6.1.</span> <span class="nav-text">Special Hypergraph Neural
Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hypergraph-embedding-based-on-random-walk"><span class="nav-number">6.2.</span> <span class="nav-text">Hypergraph Embedding
Based on Random Walk</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#random-walks-on-random-hypergraph"><span class="nav-number">6.3.</span> <span class="nav-text">Random Walks on Random
Hypergraph</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#random-walks-and-laplacians-on-hypergraphs-when-do-they-match"><span class="nav-number">6.4.</span> <span class="nav-text">Random
walks and Laplacians on hypergraphs: When do they match?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#message-passing-on-hypergraphs"><span class="nav-number">6.5.</span> <span class="nav-text">Message-Passing on
Hypergraphs</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ren Yixiong</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">67</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ren Yixiong</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">332k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">10:04</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
